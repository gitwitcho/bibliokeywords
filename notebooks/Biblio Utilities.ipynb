{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPPy5CXbjA0A"
   },
   "source": [
    "# Utilities for bibliometric analysis\n",
    "\n",
    "### Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 804,
     "status": "ok",
     "timestamp": 1679669593682,
     "user": {
      "displayName": "Gilbert Peffer",
      "userId": "05735818633312149901"
     },
     "user_tz": -60
    },
    "id": "4NAFY2kTkOQ9"
   },
   "outputs": [],
   "source": [
    "# Import libraries and modules\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import logging\n",
    "import re\n",
    "\n",
    "from IPython.display import display\n",
    "from itables import init_notebook_mode, show"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AKwLdk8n95r4"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Change the logging level to logging.INFO in the cells\n",
    "# for additional informative output.\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Activate Interactive Tables for better dataframe visualisations: https://mwouts.github.io/itables/quick_start.html\n",
    "init_notebook_mode(all_interactive = False)\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 549,
     "status": "ok",
     "timestamp": 1679677875441,
     "user": {
      "displayName": "Gilbert Peffer",
      "userId": "05735818633312149901"
     },
     "user_tz": -60
    },
    "id": "b_w10No6i50A"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /Users/gilbert/Analyses/bibliometrics/projects/mabs_repsol\n"
     ]
    }
   ],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "project = 'mabs_repsol'  # the project folder\n",
    "\n",
    "# ----------------------------\n",
    "\n",
    "\n",
    "# Create folder structure if the project is new and set root directory of the project\n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    os.chdir('../')\n",
    "\n",
    "root_dir = os.getcwd() + '/projects/' + project\n",
    "\n",
    "print(f'Working directory: {root_dir}')\n",
    "\n",
    "if not os.path.exists(root_dir):\n",
    "    os.makedirs(root_dir)\n",
    "    os.makedirs(root_dir + '/data')\n",
    "    os.makedirs(root_dir + '/data/processed')\n",
    "    os.makedirs(root_dir + '/data/raw')\n",
    "    os.makedirs(root_dir + '/models')\n",
    "    os.makedirs(root_dir + '/results')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Clean bibliography dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: lens_ml_sim_heart_all.csv, Size: 2113 rows\n",
      "Number of publications in the input dataframe: 2113\n",
      "Number of duplicate title-author pairs that were removed: 40\n",
      "Number of remaining duplicate titles that were removed: 28\n",
      "Removed 0 titles that were empty strings\n",
      "Removed 1 titles that were NaN\n",
      "Removed 13 records where the title contained \"conference\", \"workshop\", or \"proceeding\"\n",
      "Replaced 22 abtracts that were NaN with an empty string\n",
      "Removing all strings of the form <some text>\n",
      "Removing the word \"abstract\" at the start of the title\n",
      "Removing the word \"abstract\" and \"objective\" at the start of the abstract\n",
      "Removing common terms like \"background\", \"conclusions\",... from the abstract\n",
      "Replacing colons and semi-colons in the abstract with a white space\n",
      "Removing excess white space\n",
      "Removed 2 titles that were empty strings\n",
      "Number of remaining duplicate titles that were removed in 2nd sweep: 83\n",
      "Remaining special characters: {\"'\", 'ψ', 'π', 'γ', ')', ':', '-', '%', 'μ', '.', '’', 'Δ', 'χ', 'β', ',', 'ε', 'θ', '$', ' ', 'υ', 'Ω', '\"', 'α', 'σ', '(', 'ρ'}\n",
      "Number of anonymous authors: 10\n",
      "Number of entries in cleaned dataframe: 1946\n",
      "Saving file TEST_lens_ml_sim_heart_all.csv ...\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "biblio_source = 'lens'  # 'scopus' or 'lens'\n",
    "search_label = 'lens_ml_sim_heart_all'   # can be useful to know from which search the publication came from when merging datasets\n",
    "\n",
    "data_dir = '/data/raw_lens_ml_sim_heart/'\n",
    "results_dir = '/data/processed/'\n",
    "\n",
    "biblio_csv_files = []\n",
    "# biblio_csv_files = ['lens_ml_fos_medicine_2023.csv']  # comment out to read all csv files in data_dir\n",
    "\n",
    "write_csv = True\n",
    "file_cleaned_csv = 'lens_ml_sim_heart_all'\n",
    "\n",
    "timestamping = False\n",
    "\n",
    "# ----------------------------\n",
    "\n",
    "# Get list of CSV files in directory\n",
    "if len(biblio_csv_files) == 0:\n",
    "    biblio_csv_files = [f for f in os.listdir(root_dir + data_dir) if f.endswith('.csv')]\n",
    "\n",
    "all_f_df = []\n",
    "\n",
    "# Read all CSV files into a single DataFrame\n",
    "for f in biblio_csv_files:\n",
    "    f_df = pd.read_csv(os.path.join(root_dir + data_dir, f), on_bad_lines = 'skip')\n",
    "    print(f'File: {os.path.basename(f)}, Size: {len(f_df)} rows')\n",
    "    all_f_df.append(f_df)\n",
    "\n",
    "biblio_df = pd.concat(all_f_df, ignore_index = True)\n",
    "\n",
    "print(f'Number of publications in the input dataframe: {len(biblio_df)}')\n",
    "\n",
    "if biblio_source == 'scopus':\n",
    "\n",
    "    # Rename and filter the columns of the dataframe\n",
    "    biblio_df.rename(columns = {'Authors':'authors', \n",
    "                                'Author(s) ID':'author_ids', \n",
    "                                'Authors with affiliations':'affiliations', \n",
    "                                'Title':'title',\n",
    "                                'Abstract':'abstract',\n",
    "                                'Year':'year',\n",
    "                                'Source title':'source',\n",
    "                                'Cited by':'cited',\n",
    "                                'Author Keywords':'kws_author',\n",
    "                                'Index Keywords':'kws_index',\n",
    "                                'Document Type':'doc_type'}, inplace = True)\n",
    "\n",
    "    biblio_df = biblio_df[['authors', 'author_ids', 'affiliations', \n",
    "                           'title', 'abstract', 'year', 'source', \n",
    "                           'cited', 'kws_author', 'kws_index', 'doc_type']]\n",
    "    \n",
    "    biblio_df['search_label'] = search_label\n",
    "\n",
    "elif biblio_source == 'lens':\n",
    "    \n",
    "    # Drop columns and change remaining column names\n",
    "    biblio_df = biblio_df.loc[:,['Author/s', 'Title', 'Abstract', 'Keywords', 'MeSH Terms', \n",
    "                                 'Fields of Study', 'Publication Year', 'Source Title', \n",
    "                                 'Citing Works Count', 'Date Published']]\n",
    "    biblio_df = biblio_df.rename(columns = {'Author/s':'authors', \n",
    "                                            'Title':'title', \n",
    "                                            'Abstract':'abstract', \n",
    "                                            'Keywords':'lens_kws', \n",
    "                                            'MeSH Terms':'mesh', \n",
    "                                            'Fields of Study':'fos', \n",
    "                                            'Publication Year':'year',\n",
    "                                            'Source Title':'source', \n",
    "                                            'Citing Works Count':'cited'})\n",
    "\n",
    "    # Type of year should be integer\n",
    "    biblio_df['year'] = pd.to_numeric(biblio_df['year'], errors = 'coerce')\n",
    "    biblio_df['year'] = biblio_df['year'].fillna(0)\n",
    "    biblio_df['year'] = biblio_df['year'].astype(int)\n",
    "\n",
    "    biblio_df['search_label'] = search_label\n",
    "    \n",
    "else:\n",
    "    raise Exception(f'The bibliographic source {biblio_source} does nto exist')\n",
    "\n",
    "# Remove title-author duplicates\n",
    "count_dupl = biblio_df.duplicated(subset = ['title', 'authors']).sum()\n",
    "biblio_df.drop_duplicates(subset = ['title', 'authors'], inplace = True)\n",
    "print(f'Number of duplicate title-author pairs that were removed: {count_dupl}')\n",
    "\n",
    "# Identify remaining duplicate titles\n",
    "count_dupl_df = biblio_df[biblio_df['title'].str.lower().duplicated(keep = 'first')]\n",
    "\n",
    "# Remove remaining title duplicates\n",
    "biblio_df = biblio_df[~biblio_df['title'].str.lower().duplicated(keep = 'first')]\n",
    "print(f'Number of remaining duplicate titles that were removed: {count_dupl_df.shape[0]}')\n",
    "\n",
    "# Identify duplicate abstracts\n",
    "mask = biblio_df.duplicated(subset = 'abstract', keep = False)\n",
    "duplicates = biblio_df[mask]\n",
    "num_unique_duplicates = duplicates['abstract'].nunique()\n",
    "print(f'Number of unique abstracts that have duplicates: {num_unique_duplicates}')\n",
    "# duplicates['abstract'] = duplicates['abstract'].apply(lambda x: ' '.join(x.split()[:5]) if isinstance(x, str) and x else x)\n",
    "duplicate_counts = duplicates['abstract'].apply(lambda x: ' '.join(x.split()[:5]) if isinstance(x, str) and x else x).value_counts()\n",
    "# print(f'Number of duplicate abstracts for each unique abstract:\\n{duplicate_counts}')\n",
    "\n",
    "# Remove the abstract duplicates\n",
    "if biblio_source == 'lens': # Lens can have many duplicate abstracts where the title slightly differs\n",
    "    biblio_df = biblio_df.sort_values(by = 'Date Published', ascending = False)\n",
    "    biblio_df = biblio_df.drop_duplicates(subset = 'abstract', keep = 'first')\n",
    "else:\n",
    "    biblio_df = biblio_df[~biblio_df['abstract'].str.lower().duplicated(keep = 'first')]\n",
    "\n",
    "print(f'Number of duplicate abstracts that were removed: {duplicate_counts.sum() - num_unique_duplicates}')\n",
    "\n",
    "# Remove empty titles\n",
    "count_titles_empty = len(biblio_df[biblio_df['title'] == ''])\n",
    "biblio_df = biblio_df[biblio_df['title'] != '']\n",
    "print(f'Removed {count_titles_empty} titles that were empty strings')\n",
    "\n",
    "# Remove all titles that are NaN\n",
    "count_titles_nan = biblio_df['title'].isna().sum()\n",
    "biblio_df = biblio_df.dropna(subset = ['title'])\n",
    "print(f'Removed {count_titles_nan} titles that were NaN')\n",
    "\n",
    "# Convert the titles to lower case except for the first word\n",
    "def title_to_lc(s):\n",
    "    words = s.split()\n",
    "    words = [words[0]] + [w.lower() if w[0].isupper() and len(w) > 1 and w[1].isalpha() and not w[1].isupper() else w for w in words[1:]]\n",
    "    return ' '.join(words)\n",
    "\n",
    "biblio_df['title'] = biblio_df['title'].apply(title_to_lc)\n",
    "\n",
    "# Remove all titles that contain 'conference', 'workshop', or 'proceedings'\n",
    "count_procs = len(biblio_df[biblio_df['title'].str.contains('proceedings|conference|workshop', case = False)])\n",
    "biblio_df = biblio_df[~biblio_df['title'].str.contains('proceedings|conference|workshop', case = False)]\n",
    "print(f'Removed {count_procs} records where the title contained \"conference\", \"workshop\", or \"proceeding\"')\n",
    "\n",
    "# Replace all abstracts that are NaN with empty strings\n",
    "count_abs_nan = biblio_df['abstract'].isna().sum()\n",
    "biblio_df['abstract'] = biblio_df['abstract'].fillna('')\n",
    "print(f'Replaced {count_abs_nan} abtracts that were NaN with an empty string')\n",
    "\n",
    "# Remove all text between '<' and '>' characters\n",
    "print(f'Removing all strings of the form <some text>')\n",
    "biblio_df['title'] = biblio_df['title'].str.replace(r'<.*?>', '', regex = True)\n",
    "biblio_df['abstract'] = biblio_df['abstract'].str.replace(r'<.*?>', '', regex = True)\n",
    "\n",
    "# Remove all non-alphabetic characters except for '.', '-', ',', ':', ')', '(', '$', '%',  whitespace and Greek letters\n",
    "biblio_df['title'] = biblio_df['title'].apply(lambda x: re.sub(r'[^a-zA-Z0-9α-ωΑ-Ω\\s.,:’()$%\\'\\\"\\-]+', ' ', x))\n",
    "biblio_df['abstract'] = biblio_df['abstract'].apply(lambda x: re.sub(r'[^a-zA-Z0-9α-ωΑ-Ω\\s.,:’()$%\\'\\\"\\-]+', ' ', x))\n",
    "\n",
    "# Replace any special whitespace character with a normal whitespace\n",
    "biblio_df['title'] = biblio_df['title'].str.replace(r'\\u2002|\\u2003|\\u2005|\\u2009|\\u200a|\\u202f|\\xa0', ' ', regex = True)\n",
    "biblio_df['abstract'] = biblio_df['abstract'].str.replace(r'\\u2002|\\u2003|\\u2005|\\u2009|\\u200a|\\u202f|\\xa0', ' ', regex = True)\n",
    "\n",
    "# Replace all newline and tab characters with a white space\n",
    "biblio_df['title'] = biblio_df['title'].str.replace(r'\\n|\\t', ' ', regex = True)\n",
    "biblio_df['abstract'] = biblio_df['abstract'].str.replace(r'\\n|\\t', ' ', regex = True)\n",
    "\n",
    "# Remove the word 'abstract' at the start of any title\n",
    "print(f'Removing the word \"abstract\" at the start of the title')\n",
    "biblio_df['title'] = biblio_df['title'].str.replace(r'^(?i)abstract\\s*', '', regex = True)\n",
    "\n",
    "# Remove the word 'abstract' and 'objective' at the start of any abstract\n",
    "print(f'Removing the word \"abstract\" and \"objective\" at the start of the abstract')\n",
    "biblio_df['abstract'] = biblio_df['abstract'].str.replace(r'^(?i)abstract\\s*', '', regex = True)\n",
    "biblio_df['abstract'] = biblio_df['abstract'].str.replace(r'^(?i)objective(s)?\\s*', '', regex = True)\n",
    "\n",
    "# Remove the following common terms from the abstract independently of the case\n",
    "print(f'Removing common terms like \"background\", \"conclusions\",... from the abstract')\n",
    "remove_strings = ['background', 'objective', 'results', 'conclusions', 'introduction']\n",
    "pattern = \"|\".join(remove_strings)\n",
    "biblio_df['abstract'] = biblio_df['abstract'].apply(lambda x: re.sub(pattern, '', x, flags = re.IGNORECASE))\n",
    "\n",
    "# Replace colons and semi-colons in the abstract with a white space\n",
    "print(f'Replacing colons and semi-colons in the abstract with a white space')\n",
    "biblio_df['abstract'] = biblio_df['abstract'].str.replace('(;|:)', ' ', regex = True)\n",
    "\n",
    "# Remove words from the beginning of the title that are combinations of at least one number and zero or more special charcters\n",
    "biblio_df['title'] = biblio_df['title'].apply(lambda x: re.sub(r'^[\\W\\d]+(?=\\s)', '', x))\n",
    "\n",
    "# Remove any words starting with '.' or '-' and any single letter except 'a' from the beginning of the title and abstract\n",
    "biblio_df['title'] = biblio_df['title'].str.replace(r'^[-.]+\\s*\\w+\\s*|[-.]+(?!\\w)|(\\s|^)[^Aa\\s+](\\s+|$)', '', regex = True)\n",
    "#biblio_df['abstract'] = biblio_df['abstract'].str.replace(r'^[-.]+\\s*\\w+\\s*|[-.]+(?!\\w)|(\\s|^)[^Aa\\s+](\\s+|$)', '', regex = True)\n",
    "biblio_df['abstract'] = biblio_df['abstract'].str.replace(r'^[-.]+\\s*\\w+\\s*|(\\s|^)[^Aa\\s+](\\s+|$)', '', regex = True)\n",
    "\n",
    "# Remove excess whitespace\n",
    "print(f'Removing excess white space')\n",
    "biblio_df['title'] = biblio_df['title'].str.replace(r'\\s+', ' ', regex = True).str.strip()\n",
    "biblio_df['abstract'] = biblio_df['abstract'].str.replace(r'\\s+', ' ', regex = True).str.strip()\n",
    "\n",
    "# Remove any remaining empty titles\n",
    "count_titles = biblio_df.shape[0]\n",
    "biblio_df = biblio_df[biblio_df['title'].str.strip().astype(bool)]\n",
    "print(f'Removed {count_titles - biblio_df.shape[0]} titles that were empty strings')\n",
    "\n",
    "# Identify remaining duplicate titles (2nd sweep)\n",
    "count_dupl_df = biblio_df[biblio_df['title'].str.lower().duplicated(keep = 'first')]\n",
    "\n",
    "# Remove any remaining title duplicates (2nd sweep)\n",
    "biblio_df = biblio_df[~biblio_df['title'].str.lower().duplicated(keep = 'first')]\n",
    "print(f'Number of remaining duplicate titles that were removed in 2nd sweep: {count_dupl_df.shape[0]}')\n",
    "\n",
    "\n",
    "# Sort the dataset before creating the ids\n",
    "biblio_df = biblio_df.sort_values(by = ['year', 'title'], ascending = [False, True])\n",
    "\n",
    "# Generate the record IDs\n",
    "counter = 0\n",
    "\n",
    "def generate_id(row):\n",
    "    global counter\n",
    "\n",
    "    if (row['authors'] != \"\") and isinstance(row['authors'], str):\n",
    "        if \"no author name\" in row['authors'].lower():\n",
    "            author = 'Anonymous'\n",
    "        else:\n",
    "            if biblio_source == 'scopus':\n",
    "                author = row['authors'].split(',')[0].split(' ')[0]\n",
    "            elif biblio_source == 'lens':\n",
    "                author = row['authors'].split(';')[0].split(' ')[-1]\n",
    "            else:\n",
    "                raise Exception(f'bibliographic source {biblio_source} not recognised')\n",
    "    else:\n",
    "        author = 'Anonymous'\n",
    "\n",
    "    id = str(counter).zfill(6) + '_' + author + '_' + str(row['year'])\n",
    "    counter += 1\n",
    "\n",
    "    return id\n",
    "\n",
    "biblio_df['id'] = biblio_df.apply(generate_id, axis = 1)\n",
    "\n",
    "# Check that there are no special characters left other than '.', '-', ' ', and greek letters\n",
    "non_alphanumeric_characters = set()\n",
    "\n",
    "for text in biblio_df['title']:\n",
    "    non_alphanumeric_characters.update(set(re.findall('[^a-zA-Z0-9]', text)))\n",
    "\n",
    "for text in biblio_df['abstract']:\n",
    "    non_alphanumeric_characters.update(set(re.findall('[^a-zA-Z0-9]', text)))\n",
    "\n",
    "if biblio_source == 'scopus':\n",
    "\n",
    "    # Merge index and author keywords\n",
    "    def unique_keywords(row):\n",
    "        keywords = set()\n",
    "        cols = [col for col in ['kws_author', 'kws_index'] if not pd.isna(row[col])]\n",
    "        \n",
    "        for col in cols:\n",
    "            for keyword in row[col].split(';'):\n",
    "                keywords.add(keyword.lower())\n",
    "\n",
    "        return ','.join(sorted(keywords))\n",
    "\n",
    "    biblio_df['kws'] = biblio_df.apply(unique_keywords, axis = 1)\n",
    "    biblio_df = biblio_df.drop(columns = ['kws_author', 'kws_index'])\n",
    "\n",
    "    # Add a column that indicates whether this is a literature review\n",
    "    biblio_df['lit_review'] = biblio_df['doc_type'].apply(lambda x: 1 if x == 'Review' else 0)\n",
    "    biblio_df = biblio_df.drop(columns = ['doc_type'])\n",
    "\n",
    "    # Reorder columns before saving to CSV\n",
    "    biblio_df = biblio_df.reindex(columns = ['id', 'title', 'year', 'abstract', 'kws', 'source', \n",
    "                                            'lit_review', 'cited', 'authors', 'affiliations', \n",
    "                                            'author_ids', 'search_label'])\n",
    "\n",
    "elif biblio_source == 'lens':\n",
    "\n",
    "    # Merge keywords and MeSH terms\n",
    "    def unique_keywords(row):\n",
    "        keywords = set()\n",
    "        cols = [col for col in ['lens_kws', 'mesh'] if not pd.isna(row[col])]\n",
    "        \n",
    "        for col in cols:\n",
    "            for keyword in row[col].split(';'):\n",
    "                keywords.add(keyword.lower())\n",
    "\n",
    "        return ','.join(sorted(keywords))\n",
    "\n",
    "    biblio_df['kws'] = biblio_df.apply(unique_keywords, axis = 1)\n",
    "    biblio_df = biblio_df.drop(columns = ['lens_kws', 'mesh'])\n",
    "\n",
    "    # Add a column that indicates whether this is a literature review\n",
    "    biblio_df['lit_review'] = biblio_df['title'].apply(lambda x: 1 if 'review' in x.lower() or 'survey' in x.lower() else 0)\n",
    "\n",
    "    # Reorder columns before saving to CSV\n",
    "    biblio_df = biblio_df.reindex(columns = ['id', 'title', 'year', 'abstract', 'kws', \n",
    "                                             'fos', 'source', 'lit_review', 'cited', \n",
    "                                             'authors', 'search_label'])\n",
    "    \n",
    "print(f'Remaining special characters: {non_alphanumeric_characters}')\n",
    "print(f\"Number of anonymous authors: {len(biblio_df[biblio_df['id'].str.contains('Anonymous')])}\")\n",
    "print(f'Number of entries in cleaned dataframe: {len(biblio_df)}')\n",
    "\n",
    "# Create timestamp\n",
    "if timestamping:\n",
    "    timestamp = '_' + datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "else:\n",
    "    timestamp = ''\n",
    "\n",
    "# Write dataframe to CSV\n",
    "if write_csv:\n",
    "    print(f'Saving file {file_cleaned_csv + timestamp}.csv ...')\n",
    "    biblio_df.to_csv(root_dir + results_dir + file_cleaned_csv + timestamp + '.csv', index = False)\n",
    "\n",
    "print(f'DONE!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Scopus & Lens: Merge Scopus and Lens datasets\n",
    "**Prerequisite**: create the input datasets using the utilities \"Scopus/Lens: Generate a merged ScopusLens file with a reduced set of columns\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: scopus_ml_sim_heart_all.csv, Size: 1644 rows\n",
      "File: TEST_lens_ml_sim_heart_all.csv, Size: 1946 rows\n",
      "Number of entries in the Scopus dataset: 1644\n",
      "Number of entries in the Lens dataset: 1946\n",
      "Number of duplicate titles: 150\n",
      "Number of duplicate ids: 1\n",
      "Number of remaining duplicate titles: 0\n",
      "Number of publications in the Scopus & Lens dataset: 3439\n",
      "Saving file TEST_scopus_lens_ml_sim_heart_all.csv ...\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "data_dir = '/data/processed/'\n",
    "results_dir = '/data/processed/'\n",
    "\n",
    "scopus_csv_files = ['scopus_ml_sim_heart_all.csv']\n",
    "lens_csv_files = ['TEST_lens_ml_sim_heart_all.csv']\n",
    "\n",
    "write_csv = True\n",
    "merged_csv_file = 'TEST_scopus_lens_ml_sim_heart_all'\n",
    "\n",
    "lens_source_filter = []   # only publications with the sources listed here are included; if empty, all are included\n",
    "add_fos_to_kws = False\n",
    "timestamping = False\n",
    "\n",
    "# ----------------------------\n",
    "\n",
    "# Read Scopus files\n",
    "all_f_df = []\n",
    "\n",
    "for f in scopus_csv_files:\n",
    "    f_df = pd.read_csv(os.path.join(root_dir + data_dir, f), on_bad_lines = 'skip')\n",
    "    print(f'File: {os.path.basename(f)}, Size: {len(f_df)} rows')\n",
    "    all_f_df.append(f_df)\n",
    "\n",
    "scopus_df = pd.concat(all_f_df, ignore_index = True)\n",
    "\n",
    "# Read Lens files\n",
    "all_f_df = []\n",
    "\n",
    "for f in lens_csv_files:\n",
    "    f_df = pd.read_csv(os.path.join(root_dir + data_dir, f))\n",
    "    print(f'File: {os.path.basename(f)}, Size: {len(f_df)} rows')\n",
    "    all_f_df.append(f_df)\n",
    "\n",
    "lens_df = pd.concat(all_f_df, ignore_index = True)\n",
    "\n",
    "print(f'Number of entries in the Scopus dataset: {len(scopus_df)}')\n",
    "print(f'Number of entries in the Lens dataset: {len(lens_df)}')\n",
    "\n",
    "# Apply the lens_source_filter\n",
    "if lens_source_filter:\n",
    "    lens_df = lens_df[lens_df['source'].apply(lambda x: isinstance(x, str) and any(s.lower() in x.lower() for s in lens_source_filter))]\n",
    "    print(f'Number of entries in the Lens dataset after applying the filter: {len(lens_df)}')\n",
    "\n",
    "# Shrink columns\n",
    "scopus_df = scopus_df.loc[:, ['id', 'title', 'year', 'abstract', 'kws', 'source', 'lit_review', 'cited', 'authors', 'search_label']]\n",
    "lens_df = lens_df.loc[:, ['id', 'title', 'year', 'abstract', 'kws', 'fos', 'source', 'lit_review', 'cited', 'authors', 'search_label']]\n",
    "\n",
    "# Add lower case title columns for the join operation\n",
    "scopus_df['title_lc'] = scopus_df['title'].apply(lambda x: x.lower())\n",
    "lens_df['title_lc'] = lens_df['title'].apply(lambda x: x.lower())\n",
    "\n",
    "# Merge the two dataframes, keeping all the rows\n",
    "merged_df = pd.merge(scopus_df, lens_df, left_on = 'title_lc', right_on = 'title_lc', suffixes = ('_sco', '_len'), how = 'outer')\n",
    "\n",
    "print(f'Number of duplicate titles: {len(scopus_df) + len(lens_df) - len(merged_df)}')\n",
    "\n",
    "# Scopus articles take precedence over lens articles so that the id, abstract, year, \n",
    "# and source is that of the Scopus article. We first create the new columns and further\n",
    "# below check whether Scopus data are available for a given publication.\n",
    "merged_df['title'] = merged_df['title_sco']\n",
    "merged_df['id'] = merged_df['id_sco']\n",
    "merged_df['year'] = merged_df['year_sco'].fillna(0).astype(int)\n",
    "merged_df['abstract'] = merged_df['abstract_sco']\n",
    "merged_df['source'] = merged_df['source_sco']\n",
    "merged_df['authors'] = merged_df['authors_sco']\n",
    "\n",
    "# Use the values from Lens if the Scopus data are not available for a given title \n",
    "def len_if_scopus_nan(row):\n",
    "    new_title = row['title']\n",
    "    new_id = row['id']\n",
    "    new_year = row['year']\n",
    "    new_abstract = row['abstract']\n",
    "    new_source = row['source']\n",
    "    new_authors = row['authors']\n",
    "    \n",
    "    if pd.isnull(row['id_sco']) or pd.isna(row['id_sco']):\n",
    "        new_title = row['title_len']\n",
    "        new_id = row['id_len']\n",
    "        new_year = int(row['year_len'])\n",
    "        new_abstract = row['abstract_len']\n",
    "        new_source = row['source_len']\n",
    "        new_authors = row['authors_len']\n",
    "\n",
    "    new_row = pd.Series({'title': new_title, 'id': new_id, 'year': new_year, 'abstract': new_abstract, 'source': new_source, 'authors': new_authors})\n",
    "    \n",
    "    return new_row\n",
    "\n",
    "# Merge the Scopus and Lens dataframes\n",
    "merged_df[['title', 'id', 'year', 'abstract', 'source', 'authors']] = merged_df.apply(len_if_scopus_nan, axis = 1)\n",
    "merged_df.drop(['title_sco', 'title_len', 'id_sco', 'id_len', 'year_sco', 'year_len', \n",
    "                'abstract_sco', 'abstract_len', 'source_sco', \n",
    "                'source_len', 'authors_sco', 'authors_len'], axis = 1, inplace = True)\n",
    "\n",
    "# Merge the keywords and fields of study\n",
    "def merge_kws_fos(row):\n",
    "    kws_merged = []\n",
    "\n",
    "    if not pd.isnull(row['kws_sco']):\n",
    "        kws_merged += row['kws_sco'].split(',')\n",
    "    \n",
    "    if not pd.isnull(row['kws_len']):\n",
    "        kws_merged += row['kws_len'].split(';')\n",
    "\n",
    "    if not pd.isnull(row['fos']) and add_fos_to_kws:\n",
    "        kws_merged += row['fos'].split(';')\n",
    "    \n",
    "    kws_merged = [x.lower().strip() for x in kws_merged]\n",
    "    kws_merged = list(set(kws_merged))\n",
    "    kws_merged = ','.join(kws_merged)\n",
    "    \n",
    "    return kws_merged\n",
    "\n",
    "merged_df['kws'] = merged_df.apply(merge_kws_fos, axis = 1)\n",
    "merged_df.drop(['kws_sco', 'kws_len'], axis = 1, inplace = True)\n",
    "\n",
    "if add_fos_to_kws:\n",
    "\n",
    "    merged_df['kws'] = merged_df.apply(merge_kws_fos, axis = 1)\n",
    "    merged_df.drop(['fos'], axis = 1, inplace = True)\n",
    "\n",
    "# Merge the cited columns\n",
    "def merge_cited(row):\n",
    "    cited = 0\n",
    "    \n",
    "    if not pd.isnull(row['cited_sco']):\n",
    "        cited += int(row['cited_sco'])\n",
    "    \n",
    "    if not pd.isnull(row['cited_len']):\n",
    "        cited += int(row['cited_len'])\n",
    "\n",
    "    return cited\n",
    "\n",
    "merged_df['cited'] = merged_df.apply(merge_cited, axis = 1)\n",
    "merged_df.drop(['cited_sco', 'cited_len'], axis = 1, inplace = True)\n",
    "\n",
    "# Merge the literature review columns\n",
    "def merge_lit(row):\n",
    "    is_review = 0\n",
    "    \n",
    "    if not pd.isnull(row['lit_review_sco']):\n",
    "        is_review += int(row['lit_review_sco'])\n",
    "    \n",
    "    if not pd.isnull(row['lit_review_len']):\n",
    "        is_review += int(row['lit_review_len'])\n",
    "\n",
    "    if is_review == 2:\n",
    "        is_review = 1\n",
    "\n",
    "    return is_review\n",
    "\n",
    "merged_df['lit_review'] = merged_df.apply(merge_lit, axis = 1)\n",
    "merged_df.drop(['lit_review_sco', 'lit_review_len'], axis = 1, inplace = True)\n",
    "\n",
    "# Merge the search labels\n",
    "def merge_search_label(row):\n",
    "    sl = ''\n",
    "\n",
    "    if not pd.isnull(row['search_label_sco']):\n",
    "        sl = row['search_label_sco']\n",
    "\n",
    "        if not pd.isnull(row['search_label_len']):        \n",
    "            sl += ', ' + row['search_label_len']\n",
    "\n",
    "    elif not pd.isnull(row['search_label_len']):\n",
    "        sl = row['search_label_len']\n",
    "\n",
    "    return sl\n",
    "\n",
    "merged_df['search_label'] = merged_df.apply(merge_search_label, axis = 1)\n",
    "merged_df.drop(['search_label_sco', 'search_label_len'], axis = 1, inplace = True)\n",
    "\n",
    "# For duplicate ids, keep only the row corresponding to the first occurrence\n",
    "print(f\"Number of duplicate ids: {merged_df.duplicated(subset = 'id', keep = 'first').sum()}\")\n",
    "merged_df = merged_df.drop_duplicates(subset = 'id', keep = 'first')\n",
    "\n",
    "# For any remaining duplicate titles, keep only the row corresponding to the first occurrence\n",
    "print(f\"Number of remaining duplicate titles: {merged_df.duplicated(subset = 'title', keep = 'first').sum()}\")\n",
    "merged_df = merged_df.drop_duplicates(subset = 'title', keep = 'first')\n",
    "\n",
    "# Sort the dataset before re-indexing\n",
    "merged_df = merged_df.sort_values(by = ['year', 'title'], ascending = [False, True])\n",
    "\n",
    "# Re-index the ids\n",
    "counter = 0\n",
    "\n",
    "def reindex_id(id):\n",
    "    global counter\n",
    "    new_id = \"\"\n",
    "\n",
    "    try:\n",
    "        new_id = str(counter).zfill(6) + id[6:]\n",
    "        counter += 1\n",
    "    except TypeError:\n",
    "        print(id)\n",
    "        \n",
    "    return new_id\n",
    "\n",
    "merged_df['id'] = merged_df['id'].apply(reindex_id)\n",
    "merged_df.reset_index(drop = True, inplace = True)\n",
    "\n",
    "# Reorder columns\n",
    "if 'fos' in merged_df.columns:\n",
    "    merged_df = merged_df.reindex(columns = ['id', 'title', 'year', 'abstract', 'kws', \n",
    "                                            'fos', 'source', 'lit_review', 'cited', \n",
    "                                            'authors', 'search_label'])\n",
    "else:\n",
    "    merged_df = merged_df.reindex(columns = ['id', 'title', 'year', 'abstract', 'kws', \n",
    "                                             'source', 'lit_review', 'cited', 'authors'\n",
    "                                             'search_label'])\n",
    "\n",
    "print(f'Number of publications in the Scopus & Lens dataset: {len(merged_df)}')\n",
    "\n",
    "# Create timestamp\n",
    "if timestamping:\n",
    "    timestamp = '_' + datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "else:\n",
    "    timestamp = ''\n",
    "\n",
    "# Write dataframe to CSV\n",
    "if write_csv:\n",
    "    print(f'Saving file {merged_csv_file + timestamp}.csv ...')\n",
    "    merged_df.to_csv(root_dir + results_dir + merged_csv_file + timestamp + '.csv', index = False)\n",
    "\n",
    "print(f'DONE!')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract review publications from bibliographic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "data_dir = '/data/processed/'\n",
    "results_dir = '/data/processed/'\n",
    "\n",
    "biblio_csv_files = ['lens_ml_in_engineering_all.csv']\n",
    "\n",
    "write_csv = True\n",
    "review_pubs_csv = 'lens_ml_in_engineering_reviews_all'\n",
    "\n",
    "timestamping = False\n",
    "\n",
    "# ----------------------------\n",
    "\n",
    "# Get list of CSV files in directory\n",
    "if len(biblio_csv_files) == 0:\n",
    "    biblio_csv_files = [f for f in os.listdir(root_dir + data_dir) if f.endswith('.csv')]\n",
    "\n",
    "all_f_df = []\n",
    "\n",
    "# Read all CSV files into a single DataFrame\n",
    "for f in biblio_csv_files:\n",
    "    f_df = pd.read_csv(os.path.join(root_dir + data_dir, f), on_bad_lines = 'skip')\n",
    "    print(f'File: {os.path.basename(f)}, Size: {len(f_df)} rows')\n",
    "    all_f_df.append(f_df)\n",
    "\n",
    "biblio_df = pd.concat(all_f_df, ignore_index = True)\n",
    "print(f'Total number of publications in the dataset: {len(biblio_df)}')\n",
    "\n",
    "biblio_df = biblio_df[biblio_df['lit_review'] == 1]\n",
    "print(f'Number of review publications in the dataset: {len(biblio_df)}')\n",
    "\n",
    "# Create timestamp\n",
    "if timestamping:\n",
    "    timestamp = '_' + datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "else:\n",
    "    timestamp = ''\n",
    "\n",
    "# Write dataframe to CSV\n",
    "if write_csv:\n",
    "    print(f'Saving file {review_pubs_csv + timestamp}.csv ...')\n",
    "    biblio_df.to_csv(root_dir + results_dir + review_pubs_csv + timestamp + '.csv', index = False)\n",
    "\n",
    "print(f'DONE!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stitch together the original bibliography dataset with the computed topic information from BERTopic\n",
    "Titles are assumed to have no suffix like 039836_Zheng_2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: scopus_lens_ml_sim_engineering_all_st.csv, Size: 72967 rows\n",
      "File: bertopic_doc_info_ml_sim_engineering_abstracts.csv, Size: 74543 rows\n",
      "The merged file has 73557 publications\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>abstract</th>\n",
       "      <th>kws</th>\n",
       "      <th>fos</th>\n",
       "      <th>source</th>\n",
       "      <th>lit_review</th>\n",
       "      <th>cited</th>\n",
       "      <th>authors</th>\n",
       "      <th>search_label</th>\n",
       "      <th>search_title</th>\n",
       "      <th>search_abs</th>\n",
       "      <th>tp_num</th>\n",
       "      <th>tp_name</th>\n",
       "      <th>top_n_words</th>\n",
       "      <th>prob</th>\n",
       "      <th>representative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000_Wang_2023</td>\n",
       "      <td>2v2 air combat confrontation strategy based on...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Aircraft cluster air combat scenario is a long...</td>\n",
       "      <td>energy distributions,air combat,long sequences...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lecture Notes in Electrical Engineering</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Wang J., Zhu L., Yang H., Ji Y., Wang X.</td>\n",
       "      <td>scopus_ml_sim_subj_engineering</td>\n",
       "      <td>['reinforcement learning']</td>\n",
       "      <td>['simulation', 'reinforcement learning', 'rnn'...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_of_the_to_and</td>\n",
       "      <td>of - the - to - and - in - for - is - model - ...</td>\n",
       "      <td>0.821538</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000001_Preethi_2023</td>\n",
       "      <td>3D echocardiogram reconstruction employing a f...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Three dimensional 3D echocardiogram enables ca...</td>\n",
       "      <td>3d ann patch matching,image reconstruction,fli...</td>\n",
       "      <td>Voxel; Computer science; Artificial intelligen...</td>\n",
       "      <td>Computer Systems Science and Engineering</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Preethi C., Mohamed Sathik M., Shajun Nisha S.</td>\n",
       "      <td>scopus_ml_sim_subj_engineering, lens_ml_sim_su...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['ann', 'simulation']</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_of_the_to_and</td>\n",
       "      <td>of - the - to - and - in - for - is - model - ...</td>\n",
       "      <td>0.868854</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000002_Li_2023</td>\n",
       "      <td>3D ground penetrating radar cavity identificat...</td>\n",
       "      <td>2023</td>\n",
       "      <td>3D ground penetrating radar GPR is the main me...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ground-penetrating radar; Radar; Identificatio...</td>\n",
       "      <td>Measurement Science and Technology</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Fanruo Li; Feng Yang; Xu Qiao; Wentai Xing; Ch...</td>\n",
       "      <td>lens_ml_sim_subj_engineering</td>\n",
       "      <td>['transfer learning']</td>\n",
       "      <td>['simulation', 'transfer learning']</td>\n",
       "      <td>346</td>\n",
       "      <td>346_gpr_penetrating_radar_bscan</td>\n",
       "      <td>gpr - penetrating - radar - bscan - undergroun...</td>\n",
       "      <td>0.398205</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000003_Mehrpooya_2023</td>\n",
       "      <td>3D inverse synthetic aperture radar image qual...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Generalisation of one-dimensional dictionary l...</td>\n",
       "      <td>multidimensional data,generalisation,inverse s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IET Radar, Sonar and Navigation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mehrpooya A., Karbasi S.M., Nazari M., Abbasi ...</td>\n",
       "      <td>scopus_ml_sim_subj_engineering</td>\n",
       "      <td>[]</td>\n",
       "      <td>['simulation']</td>\n",
       "      <td>508</td>\n",
       "      <td>508_cs_compressive_sensing_compressed</td>\n",
       "      <td>cs - compressive - sensing - compressed - reco...</td>\n",
       "      <td>0.036927</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000004_Park_2023</td>\n",
       "      <td>3D off grid localization for adjacent cavitati...</td>\n",
       "      <td>2023</td>\n",
       "      <td>The propeller tip vortex cavitation TVC locali...</td>\n",
       "      <td>off-grids,bayesian networks,noise source,bayes...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sensors</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Park M., Memon S.A., Kim G., Choo Y.</td>\n",
       "      <td>scopus_ml_sim_subj_engineering</td>\n",
       "      <td>[]</td>\n",
       "      <td>['simulation']</td>\n",
       "      <td>16</td>\n",
       "      <td>16_aerodynamic_drag_wing_airfoil</td>\n",
       "      <td>aerodynamic - drag - wing - airfoil - lift - p...</td>\n",
       "      <td>0.017674</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                              title  year                                           abstract                                                kws                                                fos                                    source  lit_review  cited                                            authors                                       search_label                search_title                                         search_abs  tp_num                                tp_name                                        top_n_words      prob representative\n",
       "0       000000_Wang_2023  2v2 air combat confrontation strategy based on...  2023  Aircraft cluster air combat scenario is a long...  energy distributions,air combat,long sequences...                                                NaN   Lecture Notes in Electrical Engineering           0      0           Wang J., Zhu L., Yang H., Ji Y., Wang X.                     scopus_ml_sim_subj_engineering  ['reinforcement learning']  ['simulation', 'reinforcement learning', 'rnn'...      -1                       -1_of_the_to_and  of - the - to - and - in - for - is - model - ...  0.821538          False\n",
       "1    000001_Preethi_2023  3D echocardiogram reconstruction employing a f...  2023  Three dimensional 3D echocardiogram enables ca...  3d ann patch matching,image reconstruction,fli...  Voxel; Computer science; Artificial intelligen...  Computer Systems Science and Engineering           0      0     Preethi C., Mohamed Sathik M., Shajun Nisha S.  scopus_ml_sim_subj_engineering, lens_ml_sim_su...                          []                              ['ann', 'simulation']      -1                       -1_of_the_to_and  of - the - to - and - in - for - is - model - ...  0.868854          False\n",
       "2         000002_Li_2023  3D ground penetrating radar cavity identificat...  2023  3D ground penetrating radar GPR is the main me...                                                NaN  Ground-penetrating radar; Radar; Identificatio...        Measurement Science and Technology           0      0  Fanruo Li; Feng Yang; Xu Qiao; Wentai Xing; Ch...                       lens_ml_sim_subj_engineering       ['transfer learning']                ['simulation', 'transfer learning']     346        346_gpr_penetrating_radar_bscan  gpr - penetrating - radar - bscan - undergroun...  0.398205          False\n",
       "3  000003_Mehrpooya_2023  3D inverse synthetic aperture radar image qual...  2023  Generalisation of one-dimensional dictionary l...  multidimensional data,generalisation,inverse s...                                                NaN           IET Radar, Sonar and Navigation           0      0  Mehrpooya A., Karbasi S.M., Nazari M., Abbasi ...                     scopus_ml_sim_subj_engineering                          []                                     ['simulation']     508  508_cs_compressive_sensing_compressed  cs - compressive - sensing - compressed - reco...  0.036927          False\n",
       "4       000004_Park_2023  3D off grid localization for adjacent cavitati...  2023  The propeller tip vortex cavitation TVC locali...  off-grids,bayesian networks,noise source,bayes...                                                NaN                                   Sensors           0      0               Park M., Memon S.A., Kim G., Choo Y.                     scopus_ml_sim_subj_engineering                          []                                     ['simulation']      16       16_aerodynamic_drag_wing_airfoil  aerodynamic - drag - wing - airfoil - lift - p...  0.017674          False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving file scopus_lens_ml_sim_engineering_all_st_plus_abstract_topics.csv with 73557 publications...\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "match_col = 'abstract'  # 'title' or 'abstract' topic information provided?\n",
    "has_abs_suffix = False  # is the publication ID appended to the abstract?\n",
    "\n",
    "data_dir = '/results/'\n",
    "results_dir = '/results/'\n",
    "\n",
    "read_biblio_csv = 'scopus_lens_ml_sim_engineering_all_st.csv'     # the original processed bibliographic dataset\n",
    "read_bertopic_csv = 'bertopic_doc_info_ml_sim_engineering_abstracts.csv' # the title by topic probabilities\n",
    "\n",
    "write_csv = True\n",
    "write_xlsx = False\n",
    "write_biblio_bertopic_file = 'scopus_lens_ml_sim_engineering_all_st_plus_abstract_topics'\n",
    "\n",
    "timestamping = False\n",
    "\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "# ----------------------------\n",
    "\n",
    "# Read CSV files\n",
    "biblio_df = pd.read_csv(root_dir + data_dir + read_biblio_csv)\n",
    "print(f'File: {read_biblio_csv}, Size: {len(biblio_df)} rows')\n",
    "\n",
    "bertopic_df = pd.read_csv(root_dir + data_dir + read_bertopic_csv)\n",
    "print(f'File: {read_bertopic_csv}, Size: {len(bertopic_df)} rows')\n",
    "\n",
    "biblio_plus_topic_df = pd.DataFrame()\n",
    "\n",
    "# Merge biblio_df and bertopic_merged_df\n",
    "if match_col == 'title':\n",
    "    biblio_plus_topic_df = pd.merge(biblio_df, bertopic_df, on = 'title', how = 'left')\n",
    "    print(f'The merged file has {len(biblio_plus_topic_df)} publications')\n",
    "elif match_col == 'abstract':\n",
    "    if has_abs_suffix:\n",
    "        bertopic_df['id'] = bertopic_df['abstract'].str.split().str[-1]\n",
    "        biblio_plus_topic_df = pd.merge(biblio_df, bertopic_df, on = 'id', how = 'left')\n",
    "        biblio_plus_topic_df = biblio_plus_topic_df.drop('abstract_y', axis = 1).rename(columns = {'abstract_x': 'abstract'})\n",
    "    else:\n",
    "        biblio_plus_topic_df = pd.merge(biblio_df, bertopic_df, on = 'abstract', how = 'left')\n",
    "\n",
    "    print(f'The merged file has {len(biblio_plus_topic_df)} publications')\n",
    "\n",
    "    # if len(biblio_plus_topic_df) != len(biblio_df):\n",
    "    #     raise Exception(f'biblio_plus_topic_df has {len(biblio_plus_topic_df)} rows, but it needs to have the same number of rows \\\n",
    "    #                     than biblio_df, which are {len(biblio_df)}')\n",
    "else:\n",
    "    raise Exception(f'Match column {match_col} not recognised')\n",
    "\n",
    "biblio_plus_topic_df['tp_num'] = biblio_plus_topic_df['tp_num'].fillna(0).astype(int)\n",
    "    \n",
    "if logger.getEffectiveLevel() == logging.INFO:\n",
    "    pd.set_option('display.max_colwidth', 0)\n",
    "    display(biblio_plus_topic_df[['title', 'tp_name']].head())\n",
    "    pd.reset_option('display.max_colwidth')\n",
    "    display(biblio_plus_topic_df.head())\n",
    "    \n",
    "display(biblio_plus_topic_df.head())\n",
    "\n",
    "# Create timestamp\n",
    "if timestamping:\n",
    "    timestamp = '_' + datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "else:\n",
    "    timestamp = ''\n",
    "\n",
    "# Write results to file\n",
    "if write_csv:\n",
    "    print(f'Saving file {write_biblio_bertopic_file}.csv with {len(biblio_plus_topic_df)} publications...')\n",
    "    biblio_plus_topic_df.to_csv(root_dir + results_dir + write_biblio_bertopic_file + timestamp + '.csv', index = False)\n",
    "\n",
    "if write_xlsx:\n",
    "    print(f'Saving file {write_biblio_bertopic_file}.xlsx with {len(biblio_plus_topic_df)} publications...')\n",
    "    biblio_plus_topic_df.to_excel(root_dir + results_dir + write_biblio_bertopic_file + timestamp + '.xlsx', index = False)\n",
    "\n",
    "print(f'DONE!')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SuWGQ55Gb6z_",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Generate dataset with titles only\n",
    "This creates a CSV file with two columns (id and title) for topic modeling.\n",
    "\n",
    "#### TODO\n",
    "- This utility and that for the abstracts isn't needed since I can filter this dierctly in the BERTopic notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59347,
     "status": "ok",
     "timestamp": 1679677938131,
     "user": {
      "displayName": "Gilbert Peffer",
      "userId": "05735818633312149901"
     },
     "user_tz": -60
    },
    "id": "H6Xvvuc-kiHj",
    "outputId": "6d8ea60b-9512-40cd-c23c-a70b40675684"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: scopus_lens_ml_heart_all.csv, Size: 47168 rows\n",
      "Number of entries in dataset: 47168\n",
      "Saving file scopus_lens_ml_heart_all_title.csv ...\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "data_dir = '/results/processed/'\n",
    "results_dir = '/results/'\n",
    "\n",
    "read_files_csv = ['scopus_lens_ml_heart_all.csv']\n",
    "\n",
    "write_csv = True\n",
    "write_title_file_csv = 'scopus_lens_ml_heart_all_title'\n",
    "\n",
    "timestamping = False\n",
    "\n",
    "# ----------------------------\n",
    "\n",
    "# Get list of CSV files in directory\n",
    "if len(read_files_csv) == 0:\n",
    "    read_files_csv = [f for f in os.listdir(root_dir + data_dir) if f.endswith('.csv')]\n",
    "\n",
    "all_f_df = []\n",
    "\n",
    "# Read all CSV files into a single DataFrame\n",
    "for f in read_files_csv:\n",
    "    f_df = pd.read_csv(os.path.join(root_dir + data_dir, f))\n",
    "    print(f'File: {os.path.basename(f)}, Size: {len(f_df)} rows')\n",
    "    all_f_df.append(f_df)\n",
    "\n",
    "biblio_df = pd.concat(all_f_df, ignore_index = True)\n",
    "biblio_df = biblio_df.loc[:, ['id', 'title']]\n",
    "\n",
    "print(f'Number of entries in dataset: {len(biblio_df)}')\n",
    "\n",
    "# Create timestamp\n",
    "if timestamping:\n",
    "    timestamp = '_' + datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "else:\n",
    "    timestamp = ''\n",
    "\n",
    "# Write dataframe to CSV\n",
    "if write_csv:\n",
    "    print(f'Saving file {write_title_file_csv + timestamp}.csv ...')\n",
    "    biblio_df.to_csv(root_dir + results_dir + write_title_file_csv + timestamp + '.csv', index = False)\n",
    "\n",
    "print(f'DONE!')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SuWGQ55Gb6z_",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Generate dataset with abstracts only\n",
    "This creates a CSV file with two columns (id and abstract) for topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 59347,
     "status": "ok",
     "timestamp": 1679677938131,
     "user": {
      "displayName": "Gilbert Peffer",
      "userId": "05735818633312149901"
     },
     "user_tz": -60
    },
    "id": "H6Xvvuc-kiHj",
    "outputId": "6d8ea60b-9512-40cd-c23c-a70b40675684"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: scopus_lens_ml_heart_all.csv, Size: 47168 rows\n",
      "Number of entries in dataset: 47168\n",
      "Saving file scopus_lens_ml_heart_all_abstract.csv ...\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "data_dir = '/results/processed/'\n",
    "results_dir = '/results/'\n",
    "\n",
    "read_file_csv = ['scopus_lens_ml_heart_all.csv']\n",
    "\n",
    "write_csv = True\n",
    "abs_file = 'scopus_lens_ml_heart_all_abstract'\n",
    "\n",
    "timestamping = False\n",
    "\n",
    "# ----------------------------\n",
    "\n",
    "# Get list of CSV files in directory\n",
    "if len(read_file_csv) == 0:\n",
    "    read_file_csv = [f for f in os.listdir(root_dir + data_dir) if f.endswith('.csv')]\n",
    "\n",
    "all_f_df = []\n",
    "\n",
    "# Read all CSV files into a single DataFrame\n",
    "for f in read_file_csv:\n",
    "    f_df = pd.read_csv(os.path.join(root_dir + data_dir, f))\n",
    "    print(f'File: {os.path.basename(f)}, Size: {len(f_df)} rows')\n",
    "    all_f_df.append(f_df)\n",
    "\n",
    "biblio_df = pd.concat(all_f_df, ignore_index = True)\n",
    "biblio_df = biblio_df.loc[:, ['id', 'abstract']]\n",
    "\n",
    "print(f'Number of entries in dataset: {len(biblio_df)}')\n",
    "\n",
    "# Create timestamp\n",
    "if timestamping:\n",
    "    timestamp = '_' + datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "else:\n",
    "    timestamp = ''\n",
    "\n",
    "# Write dataframe to CSV\n",
    "if write_csv:\n",
    "    print(f'Saving file {abs_file + timestamp}.csv ...')\n",
    "    biblio_df.to_csv(root_dir + results_dir + abs_file + timestamp + '.csv', index = False)\n",
    "\n",
    "print(f'DONE!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNiwhSX3vJ3/mygpjbsQ+sL",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
