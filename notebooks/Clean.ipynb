{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**TODO**\n","- Add the possibility to switch off the logger output.\n","- Print some basic stats at the end of the code.\n","- Group the Scopus, Dimensions, and Lens operations so that they can be selected by an if statement in case the user doesn't specify all three (Scopus, Lens, Dimensions) directories."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import sys\n","\n","from pathlib import Path\n","\n","# Add the src directory to the Python path\n","# if os.path.abspath('../src') not in sys.path:\n","#     sys.path.insert(0, os.path.abspath('../src'))\n","\n","src_path = Path(\"..\") / \"src\"\n","if src_path.resolve() not in sys.path:\n","    sys.path.insert(0, str(src_path.resolve()))\n","\n","from utilities import *\n","from config import *\n","from clean import *"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Input parameters\n","# -----------------------\n","biblio_project_dir = 'systemic_risk'    # directory for the data and models of your bibliographic project\n","scopus_input_dir = 'raw/scopus'         # directory with Scopus data; leave empty if you don't have Scopus data\n","lens_input_dir = 'raw/lens'             # directory with Lens data; leave empty if you don't have Lens data\n","dims_input_dir = 'raw/dimensions'       # directory with Dimensions data; leave empty if you don't have Diemnsions data\n","\n","output_dir = 'results'                  # directory where you want to save the merged and cleaned bibliographic dataset\n","output_file = f'biblio_sr_1000.csv'     # filename of the bibliographic dataset; leave empty if you don't want to save the data\n","\n","n_rows = 0                              # the maxium number of rows read for each dataset; set to '0' if you want to read all the data\n","# -----------------------"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-05-20 19:09:42,570 - Biblio - Reading 3 CSV files...\n","2023-05-20 19:09:42,629 - Biblio - File: scopus_systemic_risk_1999_2014.csv, Size: 1621 rows\n","2023-05-20 19:09:42,688 - Biblio - File: scopus_systemic_risk_2015_2019.csv, Size: 1790 rows\n","2023-05-20 19:09:42,751 - Biblio - File: scopus_systemic_risk_2020_2023.csv, Size: 1909 rows\n","2023-05-20 19:09:42,798 - Biblio - Total number of publications in the dataframe: 5320\n","2023-05-20 19:09:42,800 - Biblio - Reading 1 CSV files...\n","2023-05-20 19:09:43,094 - Biblio - File: lens_systemic_risk_all.csv, Size: 13813 rows\n","2023-05-20 19:09:43,148 - Biblio - Total number of publications in the dataframe: 13813\n","2023-05-20 19:09:43,150 - Biblio - Reading 1 CSV files...\n","2023-05-20 19:09:43,375 - Biblio - File: dims_systemic_risk_all.csv, Size: 8520 rows\n","2023-05-20 19:09:43,466 - Biblio - Total number of publications in the dataframe: 8520\n","2023-05-20 19:09:54,644 - Biblio - Number of publications in the input biblio_df: 27653\n"]},{"name":"stdout","output_type":"stream","text":["Removed 7 titles that were empty strings\n","Removed 0 titles that were NaN\n","Removed 104 records where the title contained \"conference\", \"workshop\", or \"proceeding\"\n","Removed additional 125 titles that were empty strings\n","Replaced 3093 abtracts that were NaN with an empty string\n"]},{"name":"stderr","output_type":"stream","text":["2023-05-20 19:10:01,591 - Biblio - Number of publications before removing duplicate titles: 27417\n"]},{"name":"stdout","output_type":"stream","text":["Duplicate group: #0 \r"]},{"ename":"AttributeError","evalue":"Can only use .str accessor with string values!","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m biblio_df \u001b[39m=\u001b[39m merge_biblio_dfs(scopus_df, lens_df, dims_df)\n\u001b[1;32m     35\u001b[0m \u001b[39m# 5. Clean the title and abstract, remove duplicate titles, and merge values from\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39m#    different bibliographic datasets\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m biblio_df \u001b[39m=\u001b[39m clean_biblio_df(biblio_df_ \u001b[39m=\u001b[39;49m biblio_df)\n\u001b[1;32m     39\u001b[0m \u001b[39m# 6. Optionally save the results\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39mif\u001b[39;00m output_file:\n","File \u001b[0;32m~/Analyses/bibliotopics/src/clean.py:1087\u001b[0m, in \u001b[0;36mclean_biblio_df\u001b[0;34m(biblio_df_)\u001b[0m\n\u001b[1;32m   1080\u001b[0m biblio_df[\u001b[39m'\u001b[39m\u001b[39mabstract\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m biblio_df[\u001b[39m'\u001b[39m\u001b[39mabstract\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mreplace(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms+\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m, regex \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m   1083\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m \u001b[39m    Removing duplicate publications\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1087\u001b[0m biblio_df \u001b[39m=\u001b[39m remove_title_duplicates(biblio_df)\n\u001b[1;32m   1090\u001b[0m \u001b[39m# Convert year to integer and replace nan with zeros (in Lens, year is a float)\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39myear\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m biblio_df\u001b[39m.\u001b[39mcolumns \u001b[39mand\u001b[39;00m biblio_df[\u001b[39m'\u001b[39m\u001b[39myear\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m \u001b[39mfloat\u001b[39m:\n","File \u001b[0;32m~/Analyses/bibliotopics/src/clean.py:806\u001b[0m, in \u001b[0;36mremove_title_duplicates\u001b[0;34m(biblio_df_)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[39m# Merge the values in the keywords (kws) column\u001b[39;00m\n\u001b[1;32m    805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mkws\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m group\u001b[39m.\u001b[39mcolumns:\n\u001b[0;32m--> 806\u001b[0m     unique_kws \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39;49m\u001b[39mkws\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(empty_strings_to_nan)\u001b[39m.\u001b[39;49mdropna()\u001b[39m.\u001b[39;49mstr\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m;\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mexplode()\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39munique()\n\u001b[1;32m    807\u001b[0m     group[\u001b[39m'\u001b[39m\u001b[39mkws\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m; \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(np\u001b[39m.\u001b[39msort(unique_kws[unique_kws \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[1;32m    808\u001b[0m     \u001b[39m# biblio_df.update(group[['kws']])\u001b[39;00m\n","File \u001b[0;32m~/anaconda3/envs/ml-plus-env/lib/python3.11/site-packages/pandas/core/generic.py:5902\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5895\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   5896\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[1;32m   5897\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[1;32m   5898\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[1;32m   5899\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5900\u001b[0m ):\n\u001b[1;32m   5901\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[0;32m-> 5902\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n","File \u001b[0;32m~/anaconda3/envs/ml-plus-env/lib/python3.11/site-packages/pandas/core/accessor.py:182\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[39m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessor\n\u001b[0;32m--> 182\u001b[0m accessor_obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_accessor(obj)\n\u001b[1;32m    183\u001b[0m \u001b[39m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[39m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39m# NDFrame\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name, accessor_obj)\n","File \u001b[0;32m~/anaconda3/envs/ml-plus-env/lib/python3.11/site-packages/pandas/core/strings/accessor.py:181\u001b[0m, in \u001b[0;36mStringMethods.__init__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, data) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marrays\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstring_\u001b[39;00m \u001b[39mimport\u001b[39;00m StringDtype\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_dtype \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate(data)\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_categorical \u001b[39m=\u001b[39m is_categorical_dtype(data\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    183\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_string \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(data\u001b[39m.\u001b[39mdtype, StringDtype)\n","File \u001b[0;32m~/anaconda3/envs/ml-plus-env/lib/python3.11/site-packages/pandas/core/strings/accessor.py:235\u001b[0m, in \u001b[0;36mStringMethods._validate\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    232\u001b[0m inferred_dtype \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39minfer_dtype(values, skipna\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    234\u001b[0m \u001b[39mif\u001b[39;00m inferred_dtype \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m allowed_types:\n\u001b[0;32m--> 235\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCan only use .str accessor with string values!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    236\u001b[0m \u001b[39mreturn\u001b[39;00m inferred_dtype\n","\u001b[0;31mAttributeError\u001b[0m: Can only use .str accessor with string values!"]}],"source":["# 1. Read the bibliographic datasets\n","scopus_df = read_biblio_csv_files_to_df(biblio_project_dir = biblio_project_dir, \n","                                        input_dir = scopus_input_dir,\n","                                        biblio_source = BiblioSource.SCOPUS,\n","                                        n_rows = n_rows)\n","\n","lens_df = read_biblio_csv_files_to_df(biblio_project_dir = biblio_project_dir, \n","                                      input_dir = lens_input_dir,\n","                                      biblio_source = BiblioSource.LENS,\n","                                      n_rows = n_rows)\n","\n","dims_df = read_biblio_csv_files_to_df(biblio_project_dir = biblio_project_dir, \n","                                      input_dir = dims_input_dir,\n","                                      biblio_source = BiblioSource.DIMS,\n","                                      n_rows = n_rows)\n","\n","# 2. Select and rename columns from the dataset\n","scopus_df = modify_cols_biblio_df(biblio_df_ = scopus_df, \n","                                  reshape_base = Reshape.SCOPUS_ALL)\n","\n","lens_df = modify_cols_biblio_df(biblio_df_ = lens_df, \n","                                reshape_base = Reshape.LENS_ALL)\n","\n","dims_df = modify_cols_biblio_df(biblio_df_ = dims_df, \n","                                reshape_base = Reshape.DIMS_ALL)\n","\n","# 3. Normalise key variables in the dataset (bib_src, links, keywords, authors, author-affils)\n","scopus_df = normalise_biblio_entities(biblio_df_ = scopus_df)\n","lens_df = normalise_biblio_entities(biblio_df_ = lens_df)\n","dims_df = normalise_biblio_entities(biblio_df_ = dims_df)\n","\n","# 4. Merge the datasets from Scopus, Lens, and Dimensions\n","biblio_df = merge_biblio_dfs(scopus_df, lens_df, dims_df)\n","\n","# 5. Clean the title and abstract, remove duplicate titles, and merge values from\n","#    different bibliographic datasets\n","biblio_df = clean_biblio_df(biblio_df_ = biblio_df)\n","\n","# 6. Optionally save the results\n","if output_file:\n","    write_df(biblio_df = biblio_df,\n","            biblio_project_dir = biblio_project_dir,\n","            output_dir = output_dir,\n","            output_file = output_file)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def split_string(string):\n","    splits = []\n","    stack = []  # Stack to keep track of open parentheses\n","    start = 0   # Starting index of the current substring\n","    \n","    for i, char in enumerate(string):\n","        if char == '(':\n","            stack.append(i)  # Add the index of opening parentheses to the stack\n","        elif char == ')' and stack:\n","            stack.pop()  # Remove the index of matching opening parentheses from the stack\n","            if not stack and i < len(string) - 1 and string[i + 1] == ';':\n","                # If stack is empty and ';' is followed by the substring, split the string\n","                splits.append(string[start:i + 1])\n","                start = i + 2\n","\n","    splits.append(string[start:])  # Append the remaining part of the string\n","    return splits\n","\n","string = \"Haas, Armin (Institute for Advanced Sustainability Studies, (IASS), Potsdam, Brandenburg, Germany; Global Climate Forum (GCF), Berlin, Germany); Laubichler, Manfred (Arizona State University, Phoenix, AZ, USA); Applegate, Joffa (Arizona State University, Phoenix, AZ, USA); Steudle, Gesine (Global Climate Forum (GCF), Berlin, Germany); Jaeger, Carlo C. (Global Climate Forum (GCF), Berlin, Germany; Beijing Normal University, Beijing, People's Republic of China)\"\n","\n","result = split_string(string)\n","print(result)\n"]}],"metadata":{"kernelspec":{"display_name":"ml-plus-env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
