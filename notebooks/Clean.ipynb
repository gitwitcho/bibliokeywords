{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning bibliographic datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "if os.path.abspath('../src') not in sys.path:\n",
    "    sys.path.insert(0, os.path.abspath('../src'))\n",
    "\n",
    "from pathlib import Path\n",
    "from utilities import *\n",
    "from config import *\n",
    "from clean import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "- `biblio_project_dir`: Name of the folder containing the bibliographic project data and models (`bibliometrics/data/{biblio_project_dir}` and `bibliometrics/models/{biblio_project_dir}`).\n",
    "- `scopus_data_dir,...`: Names of the folders inside `biblio_project_dir` where the input files are stored. All files in those folder will be read.\n",
    "- `output_dir`: Name of the folder inside `biblio_project_dir` where the file should be written to.\n",
    "- `output_file`: Name of the output file. The file extension can be .xlsx or .csv.\n",
    "- `n_rows`: The maxium number of rows to be read from the bibliographic dataset. Set this to `0` to read the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_project_dir = 'systemic_risk'\n",
    "scopus_data_dir = 'raw/scopus'\n",
    "lens_data_dir = 'raw/lens'\n",
    "dims_data_dir = 'raw/dimensions'\n",
    "output_dir = 'results'\n",
    "\n",
    "n_rows = 100\n",
    "\n",
    "scopus_output_file = f'scopus_sr_{n_rows}.xlsx'\n",
    "lens_output_file = f'lens_sr_{n_rows}.xlsx'\n",
    "dims_output_file = f'dims_sr_{n_rows}.xlsx'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the bibliogaphic datasets\n",
    "Read the Scopus, Lens, and Dimensions CSV files and store them in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-18 10:10:17,577 - Biblio - Reading 3 CSV files...\n",
      "2023-05-18 10:10:17,649 - Biblio - File: scopus_systemic_risk_1999_2014.csv, Size: 100 rows\n",
      "2023-05-18 10:10:17,660 - Biblio - File: scopus_systemic_risk_2015_2019.csv, Size: 100 rows\n",
      "2023-05-18 10:10:17,677 - Biblio - File: scopus_systemic_risk_2020_2023.csv, Size: 100 rows\n",
      "2023-05-18 10:10:17,698 - Biblio - Total number of publications in the dataframe: 100\n",
      "2023-05-18 10:10:17,703 - Biblio - Reading 1 CSV files...\n",
      "2023-05-18 10:10:17,721 - Biblio - File: lens_systemic_risk_all.csv, Size: 100 rows\n",
      "2023-05-18 10:10:17,727 - Biblio - Total number of publications in the dataframe: 100\n",
      "2023-05-18 10:10:17,735 - Biblio - Reading 1 CSV files...\n",
      "2023-05-18 10:10:17,748 - Biblio - File: dims_systemic_risk_all.csv, Size: 100 rows\n",
      "2023-05-18 10:10:17,757 - Biblio - Total number of publications in the dataframe: 100\n"
     ]
    }
   ],
   "source": [
    "scopus_df = read_biblio_csv_files_to_df(biblio_project_dir_name = biblio_project_dir, \n",
    "                                        input_dir = scopus_data_dir,\n",
    "                                        biblio_source = BiblioSource.SCOPUS,\n",
    "                                        n_rows = n_rows)\n",
    "\n",
    "lens_df = read_biblio_csv_files_to_df(biblio_project_dir_name = biblio_project_dir, \n",
    "                                      input_dir = lens_data_dir,\n",
    "                                      biblio_source = BiblioSource.LENS,\n",
    "                                      n_rows = n_rows)\n",
    "\n",
    "dims_df = read_biblio_csv_files_to_df(biblio_project_dir_name = biblio_project_dir, \n",
    "                                      input_dir = dims_data_dir,\n",
    "                                      biblio_source = BiblioSource.DIMS,\n",
    "                                      n_rows = n_rows)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select and rename columns\n",
    "Rename and retain a set of columns from the full bibliographic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scopus_df = modify_cols_biblio_df(biblio_df_ = scopus_df,\n",
    "                                  reshape_base = Reshape.SCOPUS_COMPACT)\n",
    "\n",
    "lens_df = modify_cols_biblio_df(biblio_df_ = lens_df,\n",
    "                                reshape_base = Reshape.LENS_COMPACT)\n",
    "\n",
    "dims_df = modify_cols_biblio_df(biblio_df_ = dims_df,\n",
    "                                reshape_base = Reshape.DIMS_COMPACT)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalise the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scopus_df = normalise_biblio_entities(biblio_df_ = scopus_df,\n",
    "                                      biblio_source = BiblioSource.SCOPUS)\n",
    "\n",
    "lens_df = normalise_biblio_entities(biblio_df_ = lens_df,\n",
    "                                    biblio_source = BiblioSource.LENS)\n",
    "\n",
    "dims_df = normalise_biblio_entities(biblio_df_ = dims_df,\n",
    "                                    biblio_source = BiblioSource.DIMS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "biblio_df = merge_biblio_dfs(scopus_df, lens_df, dims_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the dataset\n",
    "Clean the bibliographic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-18 09:55:20,690 - Biblio - Number of publications in the input biblio_df: 300\n",
      "2023-05-18 09:55:20,786 - Biblio - Number of publications before removing duplicate titles: 299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 titles that were empty strings\n",
      "Removed 0 titles that were NaN\n",
      "Removed 1 records where the title contained \"conference\", \"workshop\", or \"proceeding\"\n",
      "Removed additional 0 titles that were empty strings\n",
      "Replaced 14 abtracts that were NaN with an empty string\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can only use .str accessor with string values!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m biblio_df \u001b[39m=\u001b[39m clean_biblio_df(biblio_df_ \u001b[39m=\u001b[39;49m biblio_df)\n",
      "File \u001b[0;32m~/Analyses/bibliotopics/src/clean.py:925\u001b[0m, in \u001b[0;36mclean_biblio_df\u001b[0;34m(biblio_df_)\u001b[0m\n\u001b[1;32m    918\u001b[0m biblio_df[\u001b[39m'\u001b[39m\u001b[39mabstract\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m biblio_df[\u001b[39m'\u001b[39m\u001b[39mabstract\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mreplace(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39ms+\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m, regex \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m    921\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    922\u001b[0m \u001b[39m    Removing duplicate publications\u001b[39;00m\n\u001b[1;32m    923\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 925\u001b[0m biblio_df \u001b[39m=\u001b[39m remove_title_duplicates(biblio_df)\n\u001b[1;32m    928\u001b[0m \u001b[39m# Convert year to integer and replace nan with zeros (in Lens, year is a float)\u001b[39;00m\n\u001b[1;32m    929\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39myear\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m biblio_df\u001b[39m.\u001b[39mcolumns \u001b[39mand\u001b[39;00m biblio_df[\u001b[39m'\u001b[39m\u001b[39myear\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m \u001b[39mfloat\u001b[39m:\n",
      "File \u001b[0;32m~/Analyses/bibliotopics/src/clean.py:657\u001b[0m, in \u001b[0;36mremove_title_duplicates\u001b[0;34m(biblio_df_)\u001b[0m\n\u001b[1;32m    653\u001b[0m     group[\u001b[39m'\u001b[39m\u001b[39msources\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m; \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(unique_src)\n\u001b[1;32m    654\u001b[0m     \u001b[39m# biblio_df.loc[group.index, 'sources'] = group['source'] => original code\u001b[39;00m\n\u001b[1;32m    655\u001b[0m     \u001b[39m# biblio_df.loc[group.index.values, 'sources'] = group['source'] => first attempt\u001b[39;00m\n\u001b[1;32m    656\u001b[0m     biblio_df\u001b[39m.\u001b[39mloc[biblio_df\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39misin(group\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39mvalues), \u001b[39m'\u001b[39m\u001b[39msources\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \\\n\u001b[0;32m--> 657\u001b[0m         biblio_df\u001b[39m.\u001b[39;49mloc[biblio_df\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49misin(group\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mvalues), \u001b[39m'\u001b[39;49m\u001b[39msources\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mstr\u001b[39m.\u001b[39mcat(group[\u001b[39m'\u001b[39m\u001b[39msource\u001b[39m\u001b[39m'\u001b[39m], sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m; \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    660\u001b[0m \u001b[39mif\u001b[39;00m(idx \u001b[39m%\u001b[39m \u001b[39m1\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m    661\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDuplicate group: #\u001b[39m\u001b[39m{\u001b[39;00midx\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m, end \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\r\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml-plus-env/lib/python3.11/site-packages/pandas/core/generic.py:5902\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5895\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   5896\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[1;32m   5897\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[1;32m   5898\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[1;32m   5899\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5900\u001b[0m ):\n\u001b[1;32m   5901\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[0;32m-> 5902\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml-plus-env/lib/python3.11/site-packages/pandas/core/accessor.py:182\u001b[0m, in \u001b[0;36mCachedAccessor.__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[39mif\u001b[39;00m obj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[39m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessor\n\u001b[0;32m--> 182\u001b[0m accessor_obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_accessor(obj)\n\u001b[1;32m    183\u001b[0m \u001b[39m# Replace the property with the accessor object. Inspired by:\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[39m# https://www.pydanny.com/cached-property.html\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39m# We need to use object.__setattr__ because we overwrite __setattr__ on\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39m# NDFrame\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__setattr__\u001b[39m(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name, accessor_obj)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml-plus-env/lib/python3.11/site-packages/pandas/core/strings/accessor.py:181\u001b[0m, in \u001b[0;36mStringMethods.__init__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, data) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marrays\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstring_\u001b[39;00m \u001b[39mimport\u001b[39;00m StringDtype\n\u001b[0;32m--> 181\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inferred_dtype \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate(data)\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_categorical \u001b[39m=\u001b[39m is_categorical_dtype(data\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    183\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_string \u001b[39m=\u001b[39m \u001b[39misinstance\u001b[39m(data\u001b[39m.\u001b[39mdtype, StringDtype)\n",
      "File \u001b[0;32m~/anaconda3/envs/ml-plus-env/lib/python3.11/site-packages/pandas/core/strings/accessor.py:235\u001b[0m, in \u001b[0;36mStringMethods._validate\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    232\u001b[0m inferred_dtype \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39minfer_dtype(values, skipna\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    234\u001b[0m \u001b[39mif\u001b[39;00m inferred_dtype \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m allowed_types:\n\u001b[0;32m--> 235\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCan only use .str accessor with string values!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    236\u001b[0m \u001b[39mreturn\u001b[39;00m inferred_dtype\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can only use .str accessor with string values!"
     ]
    }
   ],
   "source": [
    "biblio_df = clean_biblio_df(biblio_df_ = biblio_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the bibliographic dataset to an Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "write_df(biblio_df = biblio_df,\n",
    "         biblio_project_dir_name = biblio_project_dir,\n",
    "         output_dir = output_dir,\n",
    "         output_file = 'biblio_output.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-plus-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
