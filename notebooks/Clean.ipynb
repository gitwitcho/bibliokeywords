{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["**TODO**\n","- Add the possibility to switch off the logger output.\n","- Print some basic stats at the end of the code.\n","- Group the Scopus, Dimensions, and Lens operations so that they can be selected by an if statement in case the user doesn't specify all three (Scopus, Lens, Dimensions) directories."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import sys\n","\n","from pathlib import Path\n","\n","# Add the src directory to the Python path\n","# if os.path.abspath('../src') not in sys.path:\n","#     sys.path.insert(0, os.path.abspath('../src'))\n","\n","src_path = Path(\"..\") / \"src\"\n","if src_path.resolve() not in sys.path:\n","    sys.path.insert(0, str(src_path.resolve()))\n","\n","from utilities import *\n","from config import *\n","from clean import *"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Input parameters\n","# -----------------------\n","biblio_project_dir = 'systemic_risk'    # directory for the data and models of your bibliographic project\n","scopus_input_dir = 'raw/scopus'         # directory with Scopus data; leave empty if you don't have Scopus data\n","lens_input_dir = 'raw/lens'             # directory with Lens data; leave empty if you don't have Lens data\n","dims_input_dir = 'raw/dimensions'       # directory with Dimensions data; leave empty if you don't have Diemnsions data\n","\n","output_dir = 'results'                  # directory where you want to save the merged and cleaned bibliographic dataset\n","output_file = f'biblio_sr_1000.csv'     # filename of the bibliographic dataset; leave empty if you don't want to save the data\n","\n","n_rows = 1000                           # the maxium number of rows read for each dataset; set to '0' if you want to read all the data\n","# -----------------------"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-05-19 20:16:04,699 - Biblio - Reading 3 CSV files...\n","2023-05-19 20:16:04,755 - Biblio - File: scopus_systemic_risk_1999_2014.csv, Size: 1000 rows\n","2023-05-19 20:16:04,801 - Biblio - File: scopus_systemic_risk_2015_2019.csv, Size: 1000 rows\n","2023-05-19 20:16:04,846 - Biblio - File: scopus_systemic_risk_2020_2023.csv, Size: 1000 rows\n","2023-05-19 20:16:04,866 - Biblio - Total number of publications in the dataframe: 1000\n","2023-05-19 20:16:04,867 - Biblio - Reading 1 CSV files...\n","2023-05-19 20:16:04,903 - Biblio - File: lens_systemic_risk_all.csv, Size: 1000 rows\n","2023-05-19 20:16:04,905 - Biblio - Total number of publications in the dataframe: 1000\n","2023-05-19 20:16:04,907 - Biblio - Reading 1 CSV files...\n","2023-05-19 20:16:04,962 - Biblio - File: dims_systemic_risk_all.csv, Size: 1000 rows\n","2023-05-19 20:16:04,964 - Biblio - Total number of publications in the dataframe: 1000\n","2023-05-19 20:17:07,079 - Biblio - Number of publications in the input biblio_df: 3000\n"]},{"name":"stdout","output_type":"stream","text":["Removed 0 titles that were empty strings\n","Removed 0 titles that were NaN\n","Removed 8 records where the title contained \"conference\", \"workshop\", or \"proceeding\"\n","Removed additional 10 titles that were empty strings\n","Replaced 188 abtracts that were NaN with an empty string\n"]},{"name":"stderr","output_type":"stream","text":["2023-05-19 20:17:07,961 - Biblio - Number of publications before removing duplicate titles: 2982\n"]},{"name":"stdout","output_type":"stream","text":["Duplicate group: #2600 \r"]},{"name":"stderr","output_type":"stream","text":["2023-05-19 20:17:15,146 - Biblio - Number of publications after removing duplicate titles: 2668\n","2023-05-19 20:17:15,286 - Biblio - Writing biblio_df to file biblio_sr_1000.csv...\n"]}],"source":["# 1. Read the bibliographic datasets\n","scopus_df = read_biblio_csv_files_to_df(biblio_project_dir = biblio_project_dir, \n","                                        input_dir = scopus_input_dir,\n","                                        biblio_source = BiblioSource.SCOPUS,\n","                                        n_rows = n_rows)\n","\n","lens_df = read_biblio_csv_files_to_df(biblio_project_dir = biblio_project_dir, \n","                                      input_dir = lens_input_dir,\n","                                      biblio_source = BiblioSource.LENS,\n","                                      n_rows = n_rows)\n","\n","dims_df = read_biblio_csv_files_to_df(biblio_project_dir = biblio_project_dir, \n","                                      input_dir = dims_input_dir,\n","                                      biblio_source = BiblioSource.DIMS,\n","                                      n_rows = n_rows)\n","\n","# 2. Select and rename columns from the dataset\n","scopus_df = modify_cols_biblio_df(biblio_df_ = scopus_df, \n","                                  reshape_base = Reshape.SCOPUS_ALL)\n","\n","lens_df = modify_cols_biblio_df(biblio_df_ = lens_df, \n","                                reshape_base = Reshape.LENS_ALL)\n","\n","dims_df = modify_cols_biblio_df(biblio_df_ = dims_df, \n","                                reshape_base = Reshape.DIMS_ALL)\n","\n","# 3. Normalise key variables in the dataset (bib_src, links, keywords, authors, author-affils)\n","scopus_df = normalise_biblio_entities(biblio_df_ = scopus_df)\n","lens_df = normalise_biblio_entities(biblio_df_ = lens_df)\n","dims_df = normalise_biblio_entities(biblio_df_ = dims_df)\n","\n","# 4. Merge the datasets from Scopus, Lens, and Dimensions\n","biblio_df = merge_biblio_dfs(scopus_df, lens_df, dims_df)\n","\n","# 5. Clean the title and abstract, remove duplicate titles, and merge values from\n","#    different bibliographic datasets\n","biblio_df = clean_biblio_df(biblio_df_ = biblio_df)\n","\n","\n","\n","# 6. Optionally save the results\n","if output_file:\n","    write_df(biblio_df = biblio_df,\n","            biblio_project_dir = biblio_project_dir,\n","            output_dir = output_dir,\n","            output_file = output_file)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def split_string(string):\n","    splits = []\n","    stack = []  # Stack to keep track of open parentheses\n","    start = 0   # Starting index of the current substring\n","    \n","    for i, char in enumerate(string):\n","        if char == '(':\n","            stack.append(i)  # Add the index of opening parentheses to the stack\n","        elif char == ')' and stack:\n","            stack.pop()  # Remove the index of matching opening parentheses from the stack\n","            if not stack and i < len(string) - 1 and string[i + 1] == ';':\n","                # If stack is empty and ';' is followed by the substring, split the string\n","                splits.append(string[start:i + 1])\n","                start = i + 2\n","\n","    splits.append(string[start:])  # Append the remaining part of the string\n","    return splits\n","\n","string = \"Haas, Armin (Institute for Advanced Sustainability Studies, (IASS), Potsdam, Brandenburg, Germany; Global Climate Forum (GCF), Berlin, Germany); Laubichler, Manfred (Arizona State University, Phoenix, AZ, USA); Applegate, Joffa (Arizona State University, Phoenix, AZ, USA); Steudle, Gesine (Global Climate Forum (GCF), Berlin, Germany); Jaeger, Carlo C. (Global Climate Forum (GCF), Berlin, Germany; Beijing Normal University, Beijing, People's Republic of China)\"\n","\n","result = split_string(string)\n","print(result)\n"]}],"metadata":{"kernelspec":{"display_name":"ml-plus-env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
