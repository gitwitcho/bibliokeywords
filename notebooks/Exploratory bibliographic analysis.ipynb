{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1otrTS1k_xsU"
   },
   "source": [
    "# Exploratory analysis of large bibliographic datasets\n",
    "\n",
    "Large bibliographic datasets can be difficult to explore at a lower level. This Jupyter notebook provides **several tools**:\n",
    "* Filtered (stemmed) keywords and associated keywords (HTML).\n",
    "* Differentiated Scopus search and bibliographic keyword filtering and highlighting (HTML, and Excel).\n",
    "* Researcher, department, and country by number of publications in the full or filtered results.\n",
    "* Topic model applied to the Scopus keywords.\n",
    "\n",
    "Currently, this requires **Scopus bibliographic files in CSV** format as an input.\n",
    "\n",
    "### Before you get started\n",
    "* Open this notebook from the Github repository by clicking on the small 'Open in Colab' button in Github. This will open the notebook in your browser.\n",
    "* On your Gogole Drive, create a folder 'Biblio Analysis' inside 'My Drive/Colab Notebooks'. Create two subfolders 'Input' and 'Output' inside of 'Biblio Analysis'.\n",
    "* Save the notebook to 'Biblio Analysis' on your Google Drive by clicking on the menu 'File > Save a copy in Drive' and navigating to 'Biblio Analysis'. You can change the folder names and locations, but you will need to manually change the paths in the 'Folders and file paths' section below.\n",
    "* Download the bibliographic files from Scopus as CSV, Make sure to check the following categories in the Export Document Settings in Scopus:\n",
    "  * Citation information\n",
    "  * Bibliographical information\n",
    "  * Abstract & keywords\n",
    "* Upload the Scopus files you want to analyse to the 'Input' folder. They will be bulk-read by the script (unless you specify a single file name in the script parameters).\n",
    "\n",
    "#### TODO\n",
    "- Add a module to analyse the topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_rwVRODHjeI"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Hur9xC_y9agr"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pycountry\n",
    "import datetime\n",
    "import ast\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.styles import Font, colors, Alignment\n",
    "from openpyxl.cell.text import InlineFont\n",
    "from openpyxl.cell.rich_text import TextBlock, CellRichText\n",
    "from keybert import KeyBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AKwLdk8n95r4"
   },
   "outputs": [],
   "source": [
    "# Change the logging level to logging.INFO in the cells\n",
    "# for additional informative output.\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZRB1g7EN9eU"
   },
   "source": [
    "### Folders and file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZYc2-hvIOA1q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /Users/gilbert/Analyses/bibliometrics/projects/ml_in_engineering\n"
     ]
    }
   ],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "project = 'ml_in_engineering'  # the project folder\n",
    "\n",
    "# ----------------------------\n",
    "\n",
    "\n",
    "# Create folder structure if the project is new and set root directory of the project\n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    os.chdir('../')\n",
    "\n",
    "root_dir = os.getcwd() + '/projects/' + project\n",
    "\n",
    "print(f'Working directory: {root_dir}')\n",
    "\n",
    "if not os.path.exists(root_dir):\n",
    "    os.makedirs(root_dir)\n",
    "    os.makedirs(root_dir + '/data')\n",
    "    os.makedirs(root_dir + '/data/processed')\n",
    "    os.makedirs(root_dir + '/data/raw_scopus')\n",
    "    os.makedirs(root_dir + '/data/raw_lens')\n",
    "    os.makedirs(root_dir + '/models')\n",
    "    os.makedirs(root_dir + '/results')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "EcoG9c1de1VT"
   },
   "source": [
    "### Read bibliographic files\n",
    "Reads previously pre-processed Scopus and Lens files. For raw Scopus or Lens files, it retains a subset of the columns and changes column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VssuZ6JEBjK0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading bibliographic files...\n",
      "File: scopus_lens_ml_sim_engineering_all_st.csv, Size: 72967 rows\n",
      "\n",
      "Number of publications: 72967\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>year</th>\n",
       "      <th>abstract</th>\n",
       "      <th>kws</th>\n",
       "      <th>fos</th>\n",
       "      <th>source</th>\n",
       "      <th>lit_review</th>\n",
       "      <th>cited</th>\n",
       "      <th>authors</th>\n",
       "      <th>search_label</th>\n",
       "      <th>search_title</th>\n",
       "      <th>search_abs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000_Wang_2023</td>\n",
       "      <td>2v2 air combat confrontation strategy based on...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Aircraft cluster air combat scenario is a long...</td>\n",
       "      <td>energy distributions,air combat,long sequences...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lecture Notes in Electrical Engineering</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Wang J., Zhu L., Yang H., Ji Y., Wang X.</td>\n",
       "      <td>scopus_ml_sim_subj_engineering</td>\n",
       "      <td>[reinforcement learning]</td>\n",
       "      <td>[simulation, reinforcement learning, rnn, mult...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000001_Preethi_2023</td>\n",
       "      <td>3D echocardiogram reconstruction employing a f...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Three dimensional 3D echocardiogram enables ca...</td>\n",
       "      <td>3d ann patch matching,image reconstruction,fli...</td>\n",
       "      <td>Voxel; Computer science; Artificial intelligen...</td>\n",
       "      <td>Computer Systems Science and Engineering</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Preethi C., Mohamed Sathik M., Shajun Nisha S.</td>\n",
       "      <td>scopus_ml_sim_subj_engineering, lens_ml_sim_su...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ann, simulation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000002_Li_2023</td>\n",
       "      <td>3D ground penetrating radar cavity identificat...</td>\n",
       "      <td>2023</td>\n",
       "      <td>3D ground penetrating radar GPR is the main me...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ground-penetrating radar; Radar; Identificatio...</td>\n",
       "      <td>Measurement Science and Technology</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Fanruo Li; Feng Yang; Xu Qiao; Wentai Xing; Ch...</td>\n",
       "      <td>lens_ml_sim_subj_engineering</td>\n",
       "      <td>[transfer learning]</td>\n",
       "      <td>[simulation, transfer learning]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000003_Mehrpooya_2023</td>\n",
       "      <td>3D inverse synthetic aperture radar image qual...</td>\n",
       "      <td>2023</td>\n",
       "      <td>Generalisation of one-dimensional dictionary l...</td>\n",
       "      <td>multidimensional data,generalisation,inverse s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IET Radar, Sonar and Navigation</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Mehrpooya A., Karbasi S.M., Nazari M., Abbasi ...</td>\n",
       "      <td>scopus_ml_sim_subj_engineering</td>\n",
       "      <td>[]</td>\n",
       "      <td>[simulation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000004_Park_2023</td>\n",
       "      <td>3D off grid localization for adjacent cavitati...</td>\n",
       "      <td>2023</td>\n",
       "      <td>The propeller tip vortex cavitation TVC locali...</td>\n",
       "      <td>off-grids,bayesian networks,noise source,bayes...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sensors</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Park M., Memon S.A., Kim G., Choo Y.</td>\n",
       "      <td>scopus_ml_sim_subj_engineering</td>\n",
       "      <td>[]</td>\n",
       "      <td>[simulation]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                              title  \\\n",
       "0       000000_Wang_2023  2v2 air combat confrontation strategy based on...   \n",
       "1    000001_Preethi_2023  3D echocardiogram reconstruction employing a f...   \n",
       "2         000002_Li_2023  3D ground penetrating radar cavity identificat...   \n",
       "3  000003_Mehrpooya_2023  3D inverse synthetic aperture radar image qual...   \n",
       "4       000004_Park_2023  3D off grid localization for adjacent cavitati...   \n",
       "\n",
       "   year                                           abstract  \\\n",
       "0  2023  Aircraft cluster air combat scenario is a long...   \n",
       "1  2023  Three dimensional 3D echocardiogram enables ca...   \n",
       "2  2023  3D ground penetrating radar GPR is the main me...   \n",
       "3  2023  Generalisation of one-dimensional dictionary l...   \n",
       "4  2023  The propeller tip vortex cavitation TVC locali...   \n",
       "\n",
       "                                                 kws  \\\n",
       "0  energy distributions,air combat,long sequences...   \n",
       "1  3d ann patch matching,image reconstruction,fli...   \n",
       "2                                                NaN   \n",
       "3  multidimensional data,generalisation,inverse s...   \n",
       "4  off-grids,bayesian networks,noise source,bayes...   \n",
       "\n",
       "                                                 fos  \\\n",
       "0                                                NaN   \n",
       "1  Voxel; Computer science; Artificial intelligen...   \n",
       "2  Ground-penetrating radar; Radar; Identificatio...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                     source  lit_review  cited  \\\n",
       "0   Lecture Notes in Electrical Engineering           0      0   \n",
       "1  Computer Systems Science and Engineering           0      0   \n",
       "2        Measurement Science and Technology           0      0   \n",
       "3           IET Radar, Sonar and Navigation           0      0   \n",
       "4                                   Sensors           0      0   \n",
       "\n",
       "                                             authors  \\\n",
       "0           Wang J., Zhu L., Yang H., Ji Y., Wang X.   \n",
       "1     Preethi C., Mohamed Sathik M., Shajun Nisha S.   \n",
       "2  Fanruo Li; Feng Yang; Xu Qiao; Wentai Xing; Ch...   \n",
       "3  Mehrpooya A., Karbasi S.M., Nazari M., Abbasi ...   \n",
       "4               Park M., Memon S.A., Kim G., Choo Y.   \n",
       "\n",
       "                                        search_label  \\\n",
       "0                     scopus_ml_sim_subj_engineering   \n",
       "1  scopus_ml_sim_subj_engineering, lens_ml_sim_su...   \n",
       "2                       lens_ml_sim_subj_engineering   \n",
       "3                     scopus_ml_sim_subj_engineering   \n",
       "4                     scopus_ml_sim_subj_engineering   \n",
       "\n",
       "               search_title                                         search_abs  \n",
       "0  [reinforcement learning]  [simulation, reinforcement learning, rnn, mult...  \n",
       "1                        []                                  [ann, simulation]  \n",
       "2       [transfer learning]                    [simulation, transfer learning]  \n",
       "3                        []                                       [simulation]  \n",
       "4                        []                                       [simulation]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "biblio_source = 'processed'  # 'scopus' or 'lens' or 'processed' (previously processed and saved in one of the cells below)\n",
    "\n",
    "cutoff = 0   # retain the first 'cutoff' rows of the dataset; set to zero for the full dataset\n",
    "\n",
    "data_dir = '/results/'\n",
    "results_dir = '/results/'\n",
    "\n",
    "biblio_csv_files = []\n",
    "biblio_csv_files = ['scopus_lens_ml_sim_engineering_all_st.csv'] # comment out to read all csv files in data_dir\n",
    "\n",
    "logger.setLevel(logging.INFO)\n",
    "# --------------------------------------\n",
    "\n",
    "# Read all CSV files in the directory, otherwise those provided in the list biblio_csv_files\n",
    "if len(biblio_csv_files) == 0:\n",
    "    biblio_csv_files = [f for f in os.listdir(root_dir + data_dir) if f.endswith('.csv')]\n",
    "\n",
    "all_f_df = []\n",
    "\n",
    "# Read all CSV files into a single DataFrame\n",
    "print(f'Reading bibliographic files...')\n",
    "\n",
    "for f in biblio_csv_files:\n",
    "    f_df = pd.read_csv(os.path.join(root_dir + data_dir, f))\n",
    "    print(f'File: {os.path.basename(f)}, Size: {len(f_df)} rows')\n",
    "    all_f_df.append(f_df)\n",
    "\n",
    "biblio_df = pd.concat(all_f_df, ignore_index = True)\n",
    "\n",
    "if biblio_source == 'scopus':\n",
    "\n",
    "    biblio_df = biblio_df[['author', 'author_id', 'affiliation', 'title', 'abstract', 'year', 'source', 'cited', 'kws_author', 'kws_index']]\n",
    "\n",
    "    print(f'\\nNumber of Scopus publications: {len(biblio_df)}\\n')\n",
    "\n",
    "    if logger.getEffectiveLevel() == logging.INFO:\n",
    "        display(biblio_df.head())\n",
    "\n",
    "elif biblio_source == 'lens':\n",
    "            \n",
    "    biblio_df = biblio_df.loc[:,['id', 'title', 'abstract', 'kws', 'fos', 'year', 'source']] \n",
    "    \n",
    "    print(f'\\nNumber of Lens publications: {len(biblio_df)}\\n')\n",
    "\n",
    "    if logger.getEffectiveLevel() == logging.INFO:\n",
    "        display(biblio_df.head())\n",
    "\n",
    "elif biblio_source == 'processed':  # a file that has been previously processed in one of the cells below\n",
    "    \n",
    "    # Convert the string representation of the search term lists to Python lists\n",
    "    if 'search_title' in biblio_df.columns:\n",
    "        biblio_df['search_title'] = biblio_df['search_title'].apply(ast.literal_eval)\n",
    "        biblio_df['search_abs'] = biblio_df['search_abs'].apply(ast.literal_eval)\n",
    "\n",
    "    print(f'\\nNumber of publications: {len(biblio_df)}\\n')\n",
    "\n",
    "else:\n",
    "    raise Exception(f\"The bibliographic source '{biblio_source}' does not exist. Only 'scopus' and 'lens' are valid\")\n",
    "\n",
    "if cutoff > 0:\n",
    "    biblio_df = biblio_df.head(cutoff)  # this is handy when working with larger dataframes\n",
    "    print(f'The dataframe was cut off at row {cutoff - 1}')\n",
    "\n",
    "if logger.getEffectiveLevel() == logging.INFO:\n",
    "    display(biblio_df.head())\n",
    "\n",
    "print(f'DONE!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Search term matches by title and abstract\n",
    "Create two new columns that hold the search terms matches for titles and abstracts respectively.\n",
    "\n",
    "#### TODO\n",
    "- Remove rows where both search_title and search_abs are empty lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "write_csv = False\n",
    "file_csv_out = ''\n",
    "\n",
    "logger.setLevel(logging.INFO)\n",
    "# --------------------------------\n",
    "\n",
    "\n",
    "'''\n",
    "    Extract keywords from Lens search term\n",
    "'''\n",
    "\n",
    "# Dataset: \n",
    "# - [Anomaly]\n",
    "# biblio_search_term = ''' \n",
    "#                     (( TITLE ( anomal* OR outlier OR novelt* ) OR KEY ( anomal* ) OR TITLE-ABS-KEY ( \"anomaly detection\" OR \"detect anomalies\" OR \"detection of anomalies\" OR \"detection of point anomalies\" OR \"detection of collective anomalies\" OR \"detection of contextual anomalies\" OR \"outlier detection\" OR \"detection of outliers\" OR \"novelty detection\" OR \"detection of novelties\" OR \"event detection\" OR \"deviant discovery\" OR \"change point detection\" ) ) AND TITLE-ABS-KEY ( \"neural network\" OR \"reinforcement learning\" OR \"machine learning\" OR \"deep learning\" OR transformer OR \"BERT\" OR \"GPT\" OR \"adversarial network\" OR \"gan\" OR \"natural language processing\" OR \"word embedding\" OR \"document embedding\" OR \"sentence embedding\" OR \"transfer learning\" OR \"ensemble learning\" OR \"learning algorithm\" OR \"genetic algorithm\" OR \"evolutionary algorithm\" OR \"support vector machine\" OR \"decision tree\" OR \"bayesian network\" OR \"q-learning\" OR \"long short-term memory\" OR \"classification model\" OR \"classification algorithm\" OR \"ann\" OR \"clustering algorithm\" OR \"feature extraction\" OR \"inference engine\" OR \"k nearest neighbour\" OR \"cluster analysis\" OR \"linear regression\" OR \"hidden markov\" OR perceptron OR \"random forest\" OR \"support vector regression\" OR cnn OR rnn OR \"predictive model\" OR \"logistic regression\" OR \"statistical learning\" OR lstm OR \"neural differential\" OR \"neural ordinary\" OR \"neural ODE\" OR \"data-driven model\" OR \"physics-informed\" OR \"physics-constrained\" OR \"physics-embedded\" OR \"physics-inspired\" OR \"physics-aware\" OR \"physics-enhanced\" OR \"hidden physics\" OR \"differentiable physics\" OR \"scientific machine learning\" OR \"physics machine learning\" ) AND TITLE-ABS-KEY ( time OR timeseries OR temporal OR duration OR frequency OR process ) )\n",
    "#                   '''\n",
    "\n",
    "# Dataset: \n",
    "# - [Practicum]\n",
    "# biblio_search_term = '''\n",
    "# TITLE-ABS-KEY ( \"practice placement\"  OR  practicum  OR  internship  OR  \"work placement\"  OR  \"clinical experience\"  OR  \"professional placement\"  OR  \"clinical rotation\"  OR  \"practical training\"  OR  \"practice learning\"  OR  \"experiential learning\" )  AND  ( TITLE-ABS-KEY ( stress  OR  depression  OR  anxiety  OR  burnout  OR  marginalisation  OR  exhaustion  OR  exclusionary  OR  unsupportive  OR  abusive )  OR  TITLE-ABS-KEY ( onboarding  OR  \"on-boarding\"  OR  orientation  OR  induction  OR  initiation  OR  \"ramp-up\"  OR  familiarisation  OR  welcome ) )  => 13,348 results\n",
    "# '''\n",
    "\n",
    "# I removed linear regression and logistic regression from the search term so that publications that contain only these and no other ML-related terms are removed from the dataset\n",
    "# biblio_search_term = '''\n",
    "#     (\"neural network\" OR \"reinforcement learning\" OR \"machine learning\" OR \"deep learning\" OR \"transformer model\" OR \"BERT\" OR \"GPT\" OR \"adversarial network\" OR \"gan\" OR \"natural language processing\" OR \"word embedding\" OR \"document embedding\" OR \"sentence embedding\" OR \"transfer learning\" OR \"ensemble learning\" OR \"learning algorithm\" OR \"genetic algorithm\" OR \"evolutionary algorithm\" OR \"support vector machine\" OR \"decision tree\" OR \"bayesian network\" OR \"q-learning\" OR \"long short-term memory\" OR \"classification model\" OR \"classification algorithm\" OR \"ann\" OR \"clustering algorithm\" OR \"feature extraction\" OR \"anomaly detection\" OR \"inference engine\" OR \"k nearest neighbour\" OR \"cluster analysis\" OR \"hidden markov\" OR \"random forest\" OR \"support vector regression\" OR cnn OR rnn OR \"predictive model\" OR \"statistical learning\" OR lstm OR \"neural differential\" OR \"neural ordinary\" OR \"neural ODE\" OR \"data-driven model\" OR \"physics-informed\" OR \"physics-constrained\" OR \"physics-embedded\" OR \"physics-inspired\" OR \"physics-aware\" OR \"physics-enhanced\" OR \"hidden physics\" OR \"differentiable physics\" OR \"scientific machine learning\" OR \"physics machine learning\")\n",
    "# '''\n",
    "\n",
    "# Dataset: \n",
    "# - ML+SIM in Engineering\n",
    "# - ML+SIM in Engineering Reviews\n",
    "# biblio_search_term = '''\n",
    "# ( ( ( TITLE( simulation  OR  \"numerical method\"  OR  \"numerical model\"  OR  \"navier stokes\"  OR  \"system dynamics\"  OR  \"numerical experiment\"  OR  fem  OR  turbulence  OR  \"numerical analysis\"  OR  \"multiagent\"  OR  \"multi-agent\"  OR  \"surrogate\"  OR  pde  OR  \"partial differential equation\"  OR  \"computational fluid\"  OR  \"computational model*\"  OR  \"computational method*\"  OR  \"computational framework\"  OR  \"computational approach\"  OR  \"computational experiment\"  OR  \"computational mechanic*\"  OR  \"computational technique\"  OR  \"computational study\"  OR  \"computational analysis\"  OR  \"computational science\"  OR  \"computational electro*\"  OR  \"computational material\"  OR  \"computational biomech*\"  OR  \"computational physics\"  OR  \"computational research\"  OR  \"computational engineering\"  OR  \"finite element\"  OR  \"finite difference\"  OR  \"finite volume\"  OR  \"boundary element method\"  OR  \"discrete element method\"  OR  \"meshfree method\"  OR  \"mesh free method\"  OR  \"meshless method\"  OR  \"particle hydrodynamics\"  OR  \"dissipative particle dynamics\"  OR  \"particle method\" ) OR KEY( simulation  OR  \"numerical method\"  OR  \"numerical model\"  OR  \"navier stokes\"  OR  \"system dynamics\"  OR  \"numerical experiment\"  OR  fem  OR  turbulence  OR  \"numerical analysis\"  OR  \"multiagent\"  OR  \"multi-agent\"  OR  \"surrogate\"  OR  pde  OR  \"partial differential equation\"  OR  \"computational fluid\"  OR  \"computational model*\"  OR  \"computational method*\"  OR  \"computational framework\"  OR  \"computational approach\"  OR  \"computational experiment\"  OR  \"computational mechanic*\"  OR  \"computational technique\"  OR  \"computational study\"  OR  \"computational analysis\"  OR  \"computational science\"  OR  \"computational electro*\"  OR  \"computational material\"  OR  \"computational biomech*\"  OR  \"computational physics\"  OR  \"computational research\"  OR  \"computational engineering\"  OR  \"finite element\"  OR  \"finite difference\"  OR  \"finite volume\"  OR  \"boundary element method\"  OR  \"discrete element method\"  OR  \"meshfree method\"  OR  \"mesh free method\"  OR  \"meshless method\"  OR  \"particle hydrodynamics\"  OR  \"dissipative particle dynamics\"  OR  \"particle method\" ) ) AND (TITLE( \"neural network\" OR \"reinforcement learning\" OR \"machine learning\" OR \"deep learning\" OR \"transformer model\" OR \"BERT\" OR \"GPT\" OR \"adversarial network\" OR \"gan\" OR \"natural language processing\" OR \"word embedding\" OR \"document embedding\" OR \"sentence embedding\" OR \"transfer learning\" OR \"ensemble learning\" OR \"learning algorithm\" OR \"genetic algorithm\" OR \"evolutionary algorithm\" OR \"support vector machine\" OR \"decision tree\" OR \"bayesian network\" OR \"q-learning\" OR \"long short-term memory\" OR \"classification model\" OR \"classification algorithm\" OR \"ann\" OR \"clustering algorithm\" OR \"feature extraction\" OR \"anomaly detection\" OR \"inference engine\" OR \"k nearest neighbour\" OR \"cluster analysis\" OR \"linear regression\" OR \"hidden markov\" OR perceptron OR \"random forest\" OR \"support vector regression\" OR cnn OR rnn OR \"predictive model\" OR \"logistic regression\" OR \"statistical learning\" OR lstm ) OR KEY( \"neural network\" OR \"reinforcement learning\" OR \"machine learning\" OR \"deep learning\" OR \"transformer model\" OR \"BERT\" OR \"GPT\" OR \"adversarial network\" OR \"gan\" OR \"natural language processing\" OR \"word embedding\" OR \"document embedding\" OR \"sentence embedding\" OR \"transfer learning\" OR \"ensemble learning\" OR \"learning algorithm\" OR \"genetic algorithm\" OR \"evolutionary algorithm\" OR \"support vector machine\" OR \"decision tree\" OR \"bayesian network\" OR \"q-learning\" OR \"long short-term memory\" OR \"classification model\" OR \"classification algorithm\" OR \"ann\" OR \"clustering algorithm\" OR \"feature extraction\" OR \"anomaly detection\" OR \"inference engine\" OR \"k nearest neighbour\" OR \"cluster analysis\" OR \"linear regression\" OR \"hidden markov\" OR perceptron OR \"random forest\" OR \"support vector regression\" OR cnn OR rnn OR \"predictive model\" OR \"logistic regression\" OR \"statistical learning\" OR lstm ) ) ) OR TITLE-ABS-KEY( \"neural differential\" OR \"neural ordinary\" OR \"neural ODE\" OR \"data-driven model\" OR \"physics-informed\" OR \"physics-constrained\" OR \"physics-embedded\" OR \"physics-inspired\" OR \"physics-aware\" OR \"physics-enhanced\" OR \"hidden physics\" OR \"differentiable physics\" OR \"scientific machine learning\" OR \"physics machine learning\" ) )\n",
    "# '''\n",
    "\n",
    "# Dataset: \n",
    "# - ML in Medicine\n",
    "# - ML in Medicine Reviews\n",
    "# biblio_search_term = '''\n",
    "# (title:(\"neural network\" OR \"reinforcement learning\" OR \"machine learning\" OR \"deep learning\" OR \"transformer model\" OR \"BERT\" OR \"GPT\" OR \"adversarial network\" OR \"gan\" OR \"natural language processing\" OR \"word embedding\" OR \"document embedding\" OR \"sentence embedding\" OR \"transfer learning\" OR \"ensemble learning\" OR \"learning algorithm\" OR \"genetic algorithm\" OR \"evolutionary algorithm\" OR \"support vector machine\" OR \"decision tree\" OR \"q-learning\" OR \"long short-term memory\" OR \"classification model\" OR \"classification algorithm\" OR \"ann\" OR \"clustering algorithm\" OR \"feature extraction\" OR \"anomaly detection\" OR \"inference engine\" OR \"k nearest neighbour\" OR \"cluster analysis\" OR \"hidden markov\" OR perceptron OR \"random forest\" OR \"support vector regression\" OR cnn OR rnn OR \"predictive model\" OR \"statistical learning\" OR lstm) OR abstract:(\"neural network\" OR \"reinforcement learning\" OR \"machine learning\" OR \"deep learning\" OR \"transformer model\" OR \"BERT\" OR \"GPT\" OR \"adversarial network\" OR \"gan\" OR \"natural language processing\" OR \"word embedding\" OR \"document embedding\" OR \"sentence embedding\" OR \"transfer learning\" OR \"ensemble learning\" OR \"learning algorithm\" OR \"genetic algorithm\" OR \"evolutionary algorithm\" OR \"support vector machine\" OR \"decision tree\" OR \"q-learning\" OR \"long short-term memory\" OR \"classification model\" OR \"classification algorithm\" OR \"ann\" OR \"clustering algorithm\" OR \"feature extraction\" OR \"anomaly detection\" OR \"inference engine\" OR \"k nearest neighbour\" OR \"cluster analysis\" OR \"hidden markov\" OR perceptron OR \"random forest\" OR \"support vector regression\" OR cnn OR rnn OR \"predictive model\" OR \"statistical learning\" OR lstm ) OR title:(\"neural differential\" OR \"neural ordinary\" OR \"neural ODE\" OR \"data-driven model\" OR \"physics-informed\" OR \"physics-constrained\" OR \"physics-embedded\" OR \"physics-inspired\" OR \"physics-aware\" OR \"physics-enhanced\" OR \"hidden physics\" OR \"differentiable physics\" OR \"scientific machine learning\" OR \"physics machine learning\" ) OR abstract:(\"neural differential\" OR \"neural ordinary\" OR \"neural ODE\" OR \"data-driven model\" OR \"physics-informed\" OR \"physics-constrained\" OR \"physics-embedded\" OR \"physics-inspired\" OR \"physics-aware\" OR \"physics-enhanced\" OR \"hidden physics\" OR \"differentiable physics\" OR \"scientific machine learning\" OR \"physics machine learning\"))\n",
    "# '''\n",
    "\n",
    "# Dataset: \n",
    "# - SIM for Heart Reviews\n",
    "# biblio_search_term = '''\n",
    "# (title:(*cardi* OR heart OR *ventric* OR coronary OR *atria* OR atrium ) OR keyword:(*cardi* OR heart OR *ventric* OR coronary OR *atria* OR atrium )) AND (title:(simulation  OR  \"numerical method\"  OR  \"numerical model\"  OR  \"navier stokes\"  OR  \"system dynamics\"  OR  \"numerical experiment\"  OR  fem  OR  turbulence  OR  \"numerical analysis\"  OR  \"multiagent\"  OR  \"multi-agent\"  OR  \"surrogate\"  OR  pde  OR  \"partial differential equation\"  OR  \"computational fluid\"  OR  \"computational model*\"  OR  \"computational method*\"  OR  \"computational framework\"  OR  \"computational approach\"  OR  \"computational experiment\"  OR  \"computational mechanic*\"  OR  \"computational technique\"  OR  \"computational study\"  OR  \"computational analysis\"  OR  \"computational science\"  OR  \"computational electro*\"  OR  \"computational material\"  OR  \"computational biomech*\"  OR  \"computational physics\"  OR  \"computational research\"  OR  \"computational engineering\"  OR  \"finite element\"  OR  \"finite difference\"  OR  \"finite volume\"  OR  \"boundary element method\"  OR  \"discrete element method\"  OR  \"meshfree method\"  OR  \"mesh free method\"  OR  \"meshless method\"  OR  \"particle hydrodynamics\"  OR  \"dissipative particle dynamics\"  OR  \"particle method\") OR keyword:(simulation  OR  \"numerical method\"  OR  \"numerical model\"  OR  \"navier stokes\"  OR  \"system dynamics\"  OR  \"numerical experiment\"  OR  fem  OR  turbulence  OR  \"numerical analysis\"  OR  \"multiagent\"  OR  \"multi-agent\"  OR  \"surrogate\"  OR  pde  OR  \"partial differential equation\"  OR  \"computational fluid\"  OR  \"computational model*\"  OR  \"computational method*\"  OR  \"computational framework\"  OR  \"computational approach\"  OR  \"computational experiment\"  OR  \"computational mechanic*\"  OR  \"computational technique\"  OR  \"computational study\"  OR  \"computational analysis\"  OR  \"computational science\"  OR  \"computational electro*\"  OR  \"computational material\"  OR  \"computational biomech*\"  OR  \"computational physics\"  OR  \"computational research\"  OR  \"computational engineering\"  OR  \"finite element\"  OR  \"finite difference\"  OR  \"finite volume\"  OR  \"boundary element method\"  OR  \"discrete element method\"  OR  \"meshfree method\"  OR  \"mesh free method\"  OR  \"meshless method\"  OR  \"particle hydrodynamics\"  OR  \"dissipative particle dynamics\"  OR  \"particle method\") OR keyword:(simulation  OR  \"numerical method\"  OR  \"numerical model\"  OR  \"navier stokes\"  OR  \"system dynamics\"  OR  \"numerical experiment\"  OR  fem  OR  turbulence  OR  \"numerical analysis\"  OR  \"multiagent\"  OR  \"multi-agent\"  OR  \"surrogate\"  OR  pde  OR  \"partial differential equation\"  OR  \"computational fluid\"  OR  \"computational model*\"  OR  \"computational method*\"  OR  \"computational framework\"  OR  \"computational approach\"  OR  \"computational experiment\"  OR  \"computational mechanic*\"  OR  \"computational technique\"  OR  \"computational study\"  OR  \"computational analysis\"  OR  \"computational science\"  OR  \"computational electro*\"  OR  \"computational material\"  OR  \"computational biomech*\"  OR  \"computational physics\"  OR  \"computational research\"  OR  \"computational engineering\"  OR  \"finite element\"  OR  \"finite difference\"  OR  \"finite volume\"  OR  \"boundary element method\"  OR  \"discrete element method\"  OR  \"meshfree method\"  OR  \"mesh free method\"  OR  \"meshless method\"  OR  \"particle hydrodynamics\"  OR  \"dissipative particle dynamics\"  OR  \"particle method\"))\n",
    "# '''\n",
    "\n",
    "# Dataset: \n",
    "# - ML+SIM for Heart\n",
    "# biblio_search_term = '''\n",
    "# (((title:(\"neural network\" OR \"reinforcement learning\" OR \"machine learning\" OR \"deep learning\" OR \"transformer model\" OR \"BERT\" OR \"GPT\" OR \"adversarial network\" OR \"gan\" OR \"natural language processing\" OR \"word embedding\" OR \"document embedding\" OR \"sentence embedding\" OR \"transfer learning\" OR \"ensemble learning\" OR \"learning algorithm\" OR \"genetic algorithm\" OR \"evolutionary algorithm\" OR \"support vector machine\" OR \"decision tree\" OR \"q-learning\" OR \"long short-term memory\" OR \"classification model\" OR \"classification algorithm\" OR \"ann\" OR \"clustering algorithm\" OR \"feature extraction\" OR \"anomaly detection\" OR \"inference engine\" OR \"k nearest neighbour\" OR \"cluster analysis\" OR \"hidden markov\" OR perceptron OR \"random forest\" OR \"support vector regression\" OR cnn OR rnn OR \"predictive model\" OR \"statistical learning\" OR lstm) OR abstract:(\"neural network\" OR \"reinforcement learning\" OR \"machine learning\" OR \"deep learning\" OR \"transformer model\" OR \"BERT\" OR \"GPT\" OR \"adversarial network\" OR \"gan\" OR \"natural language processing\" OR \"word embedding\" OR \"document embedding\" OR \"sentence embedding\" OR \"transfer learning\" OR \"ensemble learning\" OR \"learning algorithm\" OR \"genetic algorithm\" OR \"evolutionary algorithm\" OR \"support vector machine\" OR \"decision tree\" OR \"q-learning\" OR \"long short-term memory\" OR \"classification model\" OR \"classification algorithm\" OR \"ann\" OR \"clustering algorithm\" OR \"feature extraction\" OR \"anomaly detection\" OR \"inference engine\" OR \"k nearest neighbour\" OR \"cluster analysis\" OR \"hidden markov\" OR perceptron OR \"random forest\" OR \"support vector regression\" OR cnn OR rnn OR \"predictive model\" OR \"statistical learning\" OR lstm )) AND (title:(simulation  OR  \"numerical method\"  OR  \"numerical model\"  OR  \"navier stokes\"  OR  \"system dynamics\"  OR  \"numerical experiment\"  OR  fem  OR  turbulence  OR  \"numerical analysis\"  OR  \"multiagent\"  OR  \"multi-agent\"  OR  \"surrogate\"  OR  pde  OR  \"partial differential equation\"  OR  \"computational fluid\"  OR  \"computational model*\"  OR  \"computational method*\"  OR  \"computational framework\"  OR  \"computational approach\"  OR  \"computational experiment\"  OR  \"computational mechanic*\"  OR  \"computational technique\"  OR  \"computational study\"  OR  \"computational analysis\"  OR  \"computational science\"  OR  \"computational electro*\"  OR  \"computational material\"  OR  \"computational biomech*\"  OR  \"computational physics\"  OR  \"computational research\"  OR  \"computational engineering\"  OR  \"finite element\"  OR  \"finite difference\"  OR  \"finite volume\"  OR  \"boundary element method\"  OR  \"discrete element method\"  OR  \"meshfree method\"  OR  \"mesh free method\"  OR  \"meshless method\"  OR  \"particle hydrodynamics\"  OR  \"dissipative particle dynamics\"  OR  \"particle method\") OR abstract:(simulation  OR  \"numerical method\"  OR  \"numerical model\"  OR  \"navier stokes\"  OR  \"system dynamics\"  OR  \"numerical experiment\"  OR  fem  OR  turbulence  OR  \"numerical analysis\"  OR  \"multiagent\"  OR  \"multi-agent\"  OR  \"surrogate\"  OR  pde  OR  \"partial differential equation\"  OR  \"computational fluid\"  OR  \"computational model*\"  OR  \"computational method*\"  OR  \"computational framework\"  OR  \"computational approach\"  OR  \"computational experiment\"  OR  \"computational mechanic*\"  OR  \"computational technique\"  OR  \"computational study\"  OR  \"computational analysis\"  OR  \"computational science\"  OR  \"computational electro*\"  OR  \"computational material\"  OR  \"computational biomech*\"  OR  \"computational physics\"  OR  \"computational research\"  OR  \"computational engineering\"  OR  \"finite element\"  OR  \"finite difference\"  OR  \"finite volume\"  OR  \"boundary element method\"  OR  \"discrete element method\"  OR  \"meshfree method\"  OR  \"mesh free method\"  OR  \"meshless method\"  OR  \"particle hydrodynamics\"  OR  \"dissipative particle dynamics\"  OR  \"particle method\"))) OR (title:(\"neural differential\" OR \"neural ordinary\" OR \"neural ODE\" OR \"data-driven model\" OR \"physics-informed\" OR \"physics-constrained\" OR \"physics-embedded\" OR \"physics-inspired\" OR \"physics-aware\" OR \"physics-enhanced\" OR \"hidden physics\" OR \"differentiable physics\" OR \"scientific machine learning\" OR \"physics machine learning\" ) OR abstract:(\"neural differential\" OR \"neural ordinary\" OR \"neural ODE\" OR \"data-driven model\" OR \"physics-informed\" OR \"physics-constrained\" OR \"physics-embedded\" OR \"physics-inspired\" OR \"physics-aware\" OR \"physics-enhanced\" OR \"hidden physics\" OR \"differentiable physics\" OR \"scientific machine learning\" OR \"physics machine learning\"))) AND (title:(*cardi*  OR  heart  OR  *ventric*  OR  coronary  OR  *atria*  OR  atrium ) OR abstract:(*cardi*  OR  heart  OR  *ventric*  OR  coronary  OR  *atria*  OR  atrium )) \n",
    "# '''\n",
    "\n",
    "# Dataset: \n",
    "# - ML Heart\n",
    "biblio_search_term = '''\n",
    "\"neural network\" OR \"reinforcement learning\" OR \"machine learning\" OR \"deep learning\" OR \"transformer model\" OR \"BERT\" OR \"GPT\" OR \"adversarial network\" OR \"gan\" OR \"natural language processing\" OR \"word embedding\" OR \"document embedding\" OR \"sentence embedding\" OR \"transfer learning\" OR \"ensemble learning\" OR \"learning algorithm\" OR \"genetic algorithm\" OR \"evolutionary algorithm\" OR \"support vector machine\" OR \"decision tree\" OR \"q-learning\" OR \"long short-term memory\" OR \"classification model\" OR \"classification algorithm\" OR \"ann\" OR \"clustering algorithm\" OR \"feature extraction\" OR \"anomaly detection\" OR \"inference engine\" OR \"k nearest neighbour\" OR \"cluster analysis\" OR \"hidden markov\" OR perceptron OR \"random forest\" OR \"support vector regression\" OR cnn OR rnn OR \"predictive model\" OR \"statistical learning\" OR lstm OR *cardi*  OR  heart  OR  *ventric*  OR  coronary  OR  *atria*  OR  atrium \n",
    "'''\n",
    "\n",
    "# Dataset: \n",
    "# - ML+SIM in Engineering\n",
    "# biblio_search_term = '''\n",
    "# ((title:(\"neural network\" OR \"reinforcement learning\" OR \"machine learning\" OR \"deep learning\" OR \"transformer model\" OR \"BERT\" OR \"GPT\" OR \"adversarial network\" OR \"gan\" OR \"natural language processing\" OR \"word embedding\" OR \"document embedding\" OR \"sentence embedding\" OR \"transfer learning\" OR \"ensemble learning\" OR \"learning algorithm\" OR \"genetic algorithm\" OR \"evolutionary algorithm\" OR \"support vector machine\" OR \"decision tree\" OR \"bayesian network\" OR \"q-learning\" OR \"long short-term memory\" OR \"classification model\" OR \"classification algorithm\" OR \"ann\" OR \"clustering algorithm\" OR \"feature extraction\" OR \"anomaly detection\" OR \"inference engine\" OR \"k nearest neighbour\" OR \"cluster analysis\" OR \"linear regression\" OR \"hidden markov\" OR perceptron OR \"random forest\" OR \"support vector regression\" OR cnn OR rnn OR \"predictive model\" OR \"logistic regression\" OR \"statistical learning\" OR lstm) OR abstract:(\"neural network\" OR \"reinforcement learning\" OR \"machine learning\" OR \"deep learning\" OR \"transformer model\" OR \"BERT\" OR \"GPT\" OR \"adversarial network\" OR \"gan\" OR \"natural language processing\" OR \"word embedding\" OR \"document embedding\" OR \"sentence embedding\" OR \"transfer learning\" OR \"ensemble learning\" OR \"learning algorithm\" OR \"genetic algorithm\" OR \"evolutionary algorithm\" OR \"support vector machine\" OR \"decision tree\" OR \"bayesian network\" OR \"q-learning\" OR \"long short-term memory\" OR \"classification model\" OR \"classification algorithm\" OR \"ann\" OR \"clustering algorithm\" OR \"feature extraction\" OR \"anomaly detection\" OR \"inference engine\" OR \"k nearest neighbour\" OR \"cluster analysis\" OR \"linear regression\" OR \"hidden markov\" OR perceptron OR \"random forest\" OR \"support vector regression\" OR cnn OR rnn OR \"predictive model\" OR \"logistic regression\" OR \"statistical learning\" OR lstm )) AND (title:(simulation  OR  \"numerical method\"  OR  \"numerical model\"  OR  \"navier stokes\"  OR  \"system dynamics\"  OR  \"numerical experiment\"  OR  fem  OR  turbulence  OR  \"numerical analysis\"  OR  \"multiagent\"  OR  \"multi-agent\"  OR  \"surrogate\"  OR  pde  OR  \"partial differential equation\"  OR  \"computational fluid\"  OR  \"computational model*\"  OR  \"computational method*\"  OR  \"computational framework\"  OR  \"computational approach\"  OR  \"computational experiment\"  OR  \"computational mechanic*\"  OR  \"computational technique\"  OR  \"computational study\"  OR  \"computational analysis\"  OR  \"computational science\"  OR  \"computational electro*\"  OR  \"computational material\"  OR  \"computational biomech*\"  OR  \"computational physics\"  OR  \"computational research\"  OR  \"computational engineering\"  OR  \"finite element\"  OR  \"finite difference\"  OR  \"finite volume\"  OR  \"boundary element method\"  OR  \"discrete element method\"  OR  \"meshfree method\"  OR  \"mesh free method\"  OR  \"meshless method\"  OR  \"particle hydrodynamics\"  OR  \"dissipative particle dynamics\"  OR  \"particle method\") OR abstract:(simulation  OR  \"numerical method\"  OR  \"numerical model\"  OR  \"navier stokes\"  OR  \"system dynamics\"  OR  \"numerical experiment\"  OR  fem  OR  turbulence  OR  \"numerical analysis\"  OR  \"multiagent\"  OR  \"multi-agent\"  OR  \"surrogate\"  OR  pde  OR  \"partial differential equation\"  OR  \"computational fluid\"  OR  \"computational model*\"  OR  \"computational method*\"  OR  \"computational framework\"  OR  \"computational approach\"  OR  \"computational experiment\"  OR  \"computational mechanic*\"  OR  \"computational technique\"  OR  \"computational study\"  OR  \"computational analysis\"  OR  \"computational science\"  OR  \"computational electro*\"  OR  \"computational material\"  OR  \"computational biomech*\"  OR  \"computational physics\"  OR  \"computational research\"  OR  \"computational engineering\"  OR  \"finite element\"  OR  \"finite difference\"  OR  \"finite volume\"  OR  \"boundary element method\"  OR  \"discrete element method\"  OR  \"meshfree method\"  OR  \"mesh free method\"  OR  \"meshless method\"  OR  \"particle hydrodynamics\"  OR  \"dissipative particle dynamics\"  OR  \"particle method\"))) OR (title:(\"neural differential\" OR \"neural ordinary\" OR \"neural ODE\" OR \"data-driven model\" OR \"physics-informed\" OR \"physics-constrained\" OR \"physics-embedded\" OR \"physics-inspired\" OR \"physics-aware\" OR \"physics-enhanced\" OR \"hidden physics\" OR \"differentiable physics\" OR \"scientific machine learning\" OR \"physics machine learning\" ) OR abstract:(\"neural differential\" OR \"neural ordinary\" OR \"neural ODE\" OR \"data-driven model\" OR \"physics-informed\" OR \"physics-constrained\" OR \"physics-embedded\" OR \"physics-inspired\" OR \"physics-aware\" OR \"physics-enhanced\" OR \"hidden physics\" OR \"differentiable physics\" OR \"scientific machine learning\" OR \"physics machine learning\"))\n",
    "# '''\n",
    "\n",
    "# Dataset: \n",
    "# - ML+SIM in Medicine\n",
    "# biblio_search_term = '''\n",
    "# ((title:(\"neural network\" OR \"reinforcement learning\" OR \"machine learning\" OR \"deep learning\" OR \"transformer model\" OR \"BERT\" OR \"GPT\" OR \"adversarial network\" OR \"gan\" OR \"natural language processing\" OR \"word embedding\" OR \"document embedding\" OR \"sentence embedding\" OR \"transfer learning\" OR \"ensemble learning\" OR \"learning algorithm\" OR \"genetic algorithm\" OR \"evolutionary algorithm\" OR \"support vector machine\" OR \"decision tree\" OR \"q-learning\" OR \"long short-term memory\" OR \"classification model\" OR \"classification algorithm\" OR \"ann\" OR \"clustering algorithm\" OR \"feature extraction\" OR \"anomaly detection\" OR \"inference engine\" OR \"k nearest neighbour\" OR \"cluster analysis\" OR \"hidden markov\" OR perceptron OR \"random forest\" OR \"support vector regression\" OR cnn OR rnn OR \"predictive model\" OR \"statistical learning\" OR lstm) OR abstract:(\"neural network\" OR \"reinforcement learning\" OR \"machine learning\" OR \"deep learning\" OR \"transformer model\" OR \"BERT\" OR \"GPT\" OR \"adversarial network\" OR \"gan\" OR \"natural language processing\" OR \"word embedding\" OR \"document embedding\" OR \"sentence embedding\" OR \"transfer learning\" OR \"ensemble learning\" OR \"learning algorithm\" OR \"genetic algorithm\" OR \"evolutionary algorithm\" OR \"support vector machine\" OR \"decision tree\" OR \"q-learning\" OR \"long short-term memory\" OR \"classification model\" OR \"classification algorithm\" OR \"ann\" OR \"clustering algorithm\" OR \"feature extraction\" OR \"anomaly detection\" OR \"inference engine\" OR \"k nearest neighbour\" OR \"cluster analysis\" OR \"hidden markov\" OR perceptron OR \"random forest\" OR \"support vector regression\" OR cnn OR rnn OR \"predictive model\" OR \"statistical learning\" OR lstm )) AND (title:(simulation  OR  \"numerical method\"  OR  \"numerical model\"  OR  \"navier stokes\"  OR  \"system dynamics\"  OR  \"numerical experiment\"  OR  fem  OR  turbulence  OR  \"numerical analysis\"  OR  \"multiagent\"  OR  \"multi-agent\"  OR  \"surrogate\"  OR  pde  OR  \"partial differential equation\"  OR  \"computational fluid\"  OR  \"computational model*\"  OR  \"computational method*\"  OR  \"computational framework\"  OR  \"computational approach\"  OR  \"computational experiment\"  OR  \"computational mechanic*\"  OR  \"computational technique\"  OR  \"computational study\"  OR  \"computational analysis\"  OR  \"computational science\"  OR  \"computational electro*\"  OR  \"computational material\"  OR  \"computational biomech*\"  OR  \"computational physics\"  OR  \"computational research\"  OR  \"computational engineering\"  OR  \"finite element\"  OR  \"finite difference\"  OR  \"finite volume\"  OR  \"boundary element method\"  OR  \"discrete element method\"  OR  \"meshfree method\"  OR  \"mesh free method\"  OR  \"meshless method\"  OR  \"particle hydrodynamics\"  OR  \"dissipative particle dynamics\"  OR  \"particle method\") OR abstract:(simulation  OR  \"numerical method\"  OR  \"numerical model\"  OR  \"navier stokes\"  OR  \"system dynamics\"  OR  \"numerical experiment\"  OR  fem  OR  turbulence  OR  \"numerical analysis\"  OR  \"multiagent\"  OR  \"multi-agent\"  OR  \"surrogate\"  OR  pde  OR  \"partial differential equation\"  OR  \"computational fluid\"  OR  \"computational model*\"  OR  \"computational method*\"  OR  \"computational framework\"  OR  \"computational approach\"  OR  \"computational experiment\"  OR  \"computational mechanic*\"  OR  \"computational technique\"  OR  \"computational study\"  OR  \"computational analysis\"  OR  \"computational science\"  OR  \"computational electro*\"  OR  \"computational material\"  OR  \"computational biomech*\"  OR  \"computational physics\"  OR  \"computational research\"  OR  \"computational engineering\"  OR  \"finite element\"  OR  \"finite difference\"  OR  \"finite volume\"  OR  \"boundary element method\"  OR  \"discrete element method\"  OR  \"meshfree method\"  OR  \"mesh free method\"  OR  \"meshless method\"  OR  \"particle hydrodynamics\"  OR  \"dissipative particle dynamics\"  OR  \"particle method\"))) OR (title:(\"neural differential\" OR \"neural ordinary\" OR \"neural ODE\" OR \"data-driven model\" OR \"physics-informed\" OR \"physics-constrained\" OR \"physics-embedded\" OR \"physics-inspired\" OR \"physics-aware\" OR \"physics-enhanced\" OR \"hidden physics\" OR \"differentiable physics\" OR \"scientific machine learning\" OR \"physics machine learning\" ) OR abstract:(\"neural differential\" OR \"neural ordinary\" OR \"neural ODE\" OR \"data-driven model\" OR \"physics-informed\" OR \"physics-constrained\" OR \"physics-embedded\" OR \"physics-inspired\" OR \"physics-aware\" OR \"physics-enhanced\" OR \"hidden physics\" OR \"differentiable physics\" OR \"scientific machine learning\" OR \"physics machine learning\")) \n",
    "# '''\n",
    "\n",
    "full_search_terms = ['ann', 'bert', 'gan']  # force a full word match for these terms\n",
    "# full_search_terms = []\n",
    "# duplicate_terms = ['']\n",
    "\n",
    "# Remove these strings from the search term\n",
    "strings_to_remove = ['(',')','TITLE-ABS-KEY', 'TITLE', 'TITLE-ABS', 'KEY']  # Scopus items\n",
    "strings_to_remove += ['(',')','title:', 'abstract:', 'keyword:']    # Lens items\n",
    "\n",
    "for string_to_remove in strings_to_remove:\n",
    "    biblio_search_term = biblio_search_term.replace(string_to_remove, '')\n",
    "\n",
    "biblio_search_term = re.split(r'\\b(OR|AND)\\b', biblio_search_term)\n",
    "biblio_search_term = [item.replace('OR', '').replace('AND', '').replace('\"', '').replace('*', '').strip() \n",
    "                        for item in biblio_search_term \n",
    "                        if item.replace('OR', '').replace('AND', '').replace('\"', '').replace('*', '').strip()]\n",
    "\n",
    "# Replace '-' with whitespace in the biblio_search_terms\n",
    "biblio_search_term = [search_term.replace('-', ' ') for search_term in biblio_search_term]\n",
    "\n",
    "# Remove any duplicates from the biblio_search_terms\n",
    "count_terms = len(biblio_search_term)\n",
    "biblio_search_term = list(set(biblio_search_term))\n",
    "print(f'Removed {count_terms - len(biblio_search_term)} duplicate search terms')\n",
    "\n",
    "# Remove the full_search_terms from the search terms list so that later\n",
    "# the titles and abstracts only match the complete words from the \n",
    "# full_search_terms list\n",
    "filter_set = set(full_search_terms)\n",
    "target_set = set(biblio_search_term)\n",
    "biblio_search_term = list(target_set - filter_set)\n",
    "\n",
    "print(f'biblio_search_term: {biblio_search_term}')\n",
    "\n",
    "# Stem the search terms since in the original search they might have been stemmed (e.g. Lens has that option)\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "biblio_search_term_stemmed = [' '.join([stemmer.stem(word) for word in word_tokenize(sentence)]) for sentence in biblio_search_term]\n",
    "# print(biblio_search_term)\n",
    "\n",
    "# Find the strings in search_terms and full_search_terms that are in a given sentence and create a new list with them\n",
    "def filter_strings_by_sentence(row, column):\n",
    "\n",
    "    if(row.name % 100 == 0): # print row index\n",
    "        print(f'{row.name}', end = '\\r')\n",
    "\n",
    "    # 'title' or 'abstract'\n",
    "    value = row[column]\n",
    "    \n",
    "    if not pd.isna(value):\n",
    "        sentence = value.lower()\n",
    "        sentence = sentence.replace('-', ' ')\n",
    "        sentence = ' '.join([stemmer.stem(word) for word in word_tokenize(sentence)])\n",
    "\n",
    "        matches = [string for string in full_search_terms if re.search(r'(?i)\\b' + string +r'\\b', sentence)]\n",
    "        matches += [string for string in biblio_search_term_stemmed if re.search(r'(?i)\\b' + string, sentence)]\n",
    "    else:\n",
    "        matches = []\n",
    "    \n",
    "    return matches\n",
    "\n",
    "# biblio_df = biblio_df.head(4000).copy()\n",
    "# biblio_df = biblio_df.iloc[:97].copy()\n",
    "# display(biblio_df)\n",
    "\n",
    "# Create lists of search terms as they appear in the title, abstract, and keywords\n",
    "if logger.getEffectiveLevel() == logging.INFO: print(f'Extracting search terms from titles...')\n",
    "# biblio_df['search_title'] = biblio_df['title'].apply(lambda x: filter_strings_by_sentence(biblio_search_term, full_search_terms, x))\n",
    "biblio_df['search_title'] = biblio_df.apply(lambda row: filter_strings_by_sentence(row, 'title'), axis = 1)\n",
    "\n",
    "if logger.getEffectiveLevel() == logging.INFO: print(f'\\nExtracting search terms from abstracts...')\n",
    "# biblio_df['search_abs'] = biblio_df['abstract'].apply(lambda x: filter_strings_by_sentence(biblio_search_term, full_search_terms, x))\n",
    "biblio_df['search_abs'] = biblio_df.apply(lambda row: filter_strings_by_sentence(row, 'abstract'), axis = 1)\n",
    "\n",
    "# TODO Add the keyword matches so that we don't remove publications (see below) that have no title and abstract match but that have a keyword match\n",
    "# if logger.getEffectiveLevel() == logging.INFO: print(f'\\nExtracting search terms from keywords...')\n",
    "# biblio_df['search_kws'] = biblio_df.apply(lambda row: filter_strings_by_sentence(row, 'kws'), axis = 1)\n",
    "\n",
    "# Number of publications before applying the search term filter to titles and abstracts. You can apply a \n",
    "# subset of the original search terms if you want to remove pulications that only contain search terms not\n",
    "# in the subset. Sometimes this is useful when some of the search terms turn out to be adding many \n",
    "# publications that are not relevant.\n",
    "n_pubs = len(biblio_df)\n",
    "\n",
    "# Replace the stemmed search terms in columns 'search_title' and 'search_abstract' with the original search terms\n",
    "mapping = dict(zip(biblio_search_term_stemmed, biblio_search_term))   # create a dictionary that maps values in A to their corresponding values in B\n",
    "\n",
    "def replace_values(lst):    # replace values in a list using the mapping dictionary\n",
    "    return [mapping.get(x, x) for x in lst]\n",
    "\n",
    "biblio_df[['search_title', 'search_abs']] = biblio_df[['search_title', 'search_abs']].applymap(replace_values)\n",
    "\n",
    "# FIXME This is a little dodgy since it might remove publciations that were matched by the Scopus or Lens search engine but that for some reason aren't matched here\n",
    "# Remove publications where search_title and search_abs are empty lists. This happens when you remove search terms from the \n",
    "# biblio_search_term string, for instance terms that turn out to generate a lot of irrelevant publications.\n",
    "print(f\"Removing {len(biblio_df[~biblio_df[['search_title', 'search_abs']].apply(lambda x: any(x.apply(bool)), axis=1)])} \\\n",
    "      publications where search_title and search_abs are empty...\")\n",
    "biblio_df = biblio_df[biblio_df[['search_title', 'search_abs']].apply(lambda x: any(x.apply(bool)), axis=1)]\n",
    "\n",
    "n_pubs_filtered = len(biblio_df)\n",
    "\n",
    "if logger.getEffectiveLevel() == logging.INFO: print(f'Retained {n_pubs_filtered} of {n_pubs} publications.')\n",
    "\n",
    "# Sort the table\n",
    "if logger.getEffectiveLevel() == logging.INFO: print(f'Sorting table...')\n",
    "biblio_df = biblio_df.sort_values(by = ['year', 'title'], ascending = [False, True]).reset_index()\n",
    "biblio_df.drop(['index'], axis = 1, inplace = True)\n",
    "\n",
    "if logger.getEffectiveLevel() == logging.INFO: display(biblio_df.head())\n",
    "\n",
    "# Write dataframe to CSV\n",
    "if write_csv:\n",
    "    if logger.getEffectiveLevel() == logging.INFO: print(f'Saving file {file_csv_out}.csv ...')\n",
    "    biblio_df.to_csv(root_dir + results_dir + file_csv_out + '.csv', index = False)\n",
    "\n",
    "print(f'DONE!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "pFjXgcBXRSjJ"
   },
   "source": [
    "### Generate keyword stats tables\n",
    "Create tables with counts for search terms, bibliographic keywords, BERT keywords, fields of study, and BERT topics. What counts are generated depends on the variables provided by the biblio_df table.\n",
    "\n",
    "#### TODO\n",
    "- Add BERTopic count functionality\n",
    "- Add the abstract keyBERT processing\n",
    "- Add the probability cutoff functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "# WARNING: You need to rerun the cells above to reload the biblio_df dataset if you change this filter\n",
    "filter_st = \"\"  # if you want to do the keyword count on the subset of publications defined by filter_st\n",
    "\n",
    "keybert_title_count = False     # set this to True if you want to compute title keywords with keyBERT\n",
    "keybert_abstract_count = False  # idem for abstract keywords\n",
    "n_keybert_kws = 10\n",
    "keybert_cutoff = 0.4    # probability value below which the keywords are excluded from the count\n",
    "keybert_use_mmr = False  # use the Maximal Marginal Relevance keyword algorithm\n",
    "mmr_diversity = 0.5\n",
    "\n",
    "read_csv = False\n",
    "biblio_file_csv = ''\n",
    "\n",
    "write_csv = False\n",
    "prefix = ''\n",
    "suffix = ''\n",
    "\n",
    "write_st_filtered_csv = False\n",
    "biblio_st_filtered_file_csv = ''\n",
    "\n",
    "biblio_st_count_csv = prefix + '_st_count_' + project + suffix\n",
    "biblio_kw_count_csv = prefix + '_kw_count_' + project + suffix\n",
    "biblio_fos_count_csv = prefix + '_fos_count_' + project + suffix\n",
    "biblio_bertopic_count_csv = prefix + '_bt_count_' + project + suffix\n",
    "biblio_keybert_count_csv = prefix + '_kb_count_' + project + suffix\n",
    "\n",
    "timestamping = False\n",
    "\n",
    "logger.setLevel(logging.INFO)\n",
    "# --------------------------------\n",
    "\n",
    "# For large datasets, we read the initial dataframes from file\n",
    "if read_csv:\n",
    "    biblio_df = pd.read_csv(root_dir + results_dir + biblio_file_csv + '.csv')\n",
    "\n",
    "    # Convert the string representation of the search term lists to Python lists\n",
    "    biblio_df['search_title'] = biblio_df['search_title'].apply(ast.literal_eval)\n",
    "    biblio_df['search_abs'] = biblio_df['search_abs'].apply(ast.literal_eval)\n",
    "\n",
    "n_all_pubs = len(biblio_df)\n",
    "\n",
    "\n",
    "'''\n",
    "    0. Apply the search term filter\n",
    "'''\n",
    "\n",
    "# Convert filter_st into a word list\n",
    "words = re.split(r'\\(|\\)|\\b(?:and|or)\\b', filter_st)\n",
    "words = [x.strip() for x in words if x.strip()]\n",
    "\n",
    "# Filter publications by filter_st\n",
    "def evaluate_expression(lst):\n",
    "\n",
    "    # If the first keyword is 'only', then only include publications where the\n",
    "    # search_title and search_abs only contain the terms following 'only'\n",
    "    if (words[0] == 'only') or (words[0] == 'not only'):\n",
    "\n",
    "        # Find the substring between the parentheses\n",
    "        only_terms = re.search(r'\\((.*?)\\)', filter_st).group(1)\n",
    "\n",
    "        # Split the substring on commas and strip whitespace\n",
    "        only_terms = [x.strip() for x in only_terms.split(',')]\n",
    "\n",
    "        if set(lst).issubset(set(only_terms)):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    values = {word: word in lst for word in words}\n",
    "\n",
    "    filter_st_eval = filter_st\n",
    "\n",
    "    for key in values:\n",
    "        filter_st_eval = filter_st_eval.replace(key, str(values[key]))\n",
    "    \n",
    "    return eval(filter_st_eval)\n",
    "\n",
    "# biblio_tmp_df = biblio_df.head(10).copy()\n",
    "# biblio_tmp_df = biblio_df.copy()\n",
    "\n",
    "# display(biblio_tmp_df)\n",
    "\n",
    "if filter_st != '':\n",
    "    if words[0] == 'only':\n",
    "        biblio_df = biblio_df[biblio_df[['search_title', 'search_abs']].applymap(evaluate_expression).all(axis=1)]\n",
    "    elif words[0] == 'not only':\n",
    "        biblio_df = biblio_df[~biblio_df[['search_title', 'search_abs']].applymap(evaluate_expression).all(axis=1)]\n",
    "    else:\n",
    "        biblio_df = biblio_df[biblio_df[['search_title', 'search_abs']].applymap(evaluate_expression).any(axis=1)]\n",
    "\n",
    "print(f'Retained {len(biblio_df)} of {n_all_pubs} publications after applying filter_st')\n",
    "\n",
    "# display(biblio_df)\n",
    "\n",
    "\n",
    "'''\n",
    "    1. Search term count\n",
    "'''\n",
    "\n",
    "# Create a count table for the search terms originally used to generate the bibliographic dataset\n",
    "st_count_df = biblio_df[['search_title', 'search_abs']].copy()\n",
    "st_count_df['search_terms'] = st_count_df.apply(lambda row: ','\n",
    "                              .join(set([item for sublist in row[['search_title', 'search_abs']]\n",
    "                                 .tolist() for item in sublist])), axis=1)\n",
    "st_count_df.drop(['search_title', 'search_abs'], axis = 1, inplace = True)\n",
    "st_count_df = st_count_df.apply(lambda x: x.str.split(',')).explode('search_terms').reset_index()\n",
    "st_count_df.drop(['index'], axis = 1, inplace = True)\n",
    "st_count_df = st_count_df['search_terms'].value_counts().reset_index().rename(columns = {'search_terms': 'count', 'index': 'search_term'})\n",
    "\n",
    "if logger.getEffectiveLevel() == logging.INFO:\n",
    "    print(f'Number of publication search terms: {len(st_count_df)}')\n",
    "\n",
    "\n",
    "'''\n",
    "    2. Keywords count\n",
    "'''\n",
    "\n",
    "# Create keywords table\n",
    "kws_df = biblio_df[['kws']].copy()\n",
    "\n",
    "# Create a single keyword list by exploding the table\n",
    "kws_df = kws_df.apply(lambda x: x.str.split(',')).explode('kws').reset_index()\n",
    "kws_df.drop(['index'], axis = 1, inplace = True)\n",
    "\n",
    "# Remove NaN entries\n",
    "kws_df.dropna(subset = ['kws'], inplace = True)\n",
    "kws_df = kws_df[kws_df['kws'] != 'nan']\n",
    "\n",
    "# Create a count table for the keywords\n",
    "kw_count_df = pd.DataFrame(kws_df['kws'].value_counts()).reset_index()\n",
    "kw_count_df.columns = ['kw', 'count']\n",
    "kw_count_df = kw_count_df.sort_values(by = ['count', 'kw'], ascending = [False, True]).reset_index()\n",
    "kw_count_df.drop(['index'], axis = 1, inplace = True)\n",
    "\n",
    "if logger.getEffectiveLevel() == logging.INFO:\n",
    "    print(f'Number of unique publication keywords: {len(kw_count_df)}')\n",
    "    print(f\"{biblio_df['kws'].count()} publications of a total of {n_all_pubs} have keywords\")\n",
    "\n",
    "kw_stem_count_df = pd.DataFrame()\n",
    "\n",
    "'''\n",
    "# Create stemmed keywords\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "kws_df['kws_stemmed'] = kws_df['kws'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
    "\n",
    "# Create a count table for the stemmed Scopus keywords\n",
    "kw_stem_count_df = pd.DataFrame(kws_df['kws_stemmed'].value_counts()).reset_index()\n",
    "kw_stem_count_df.columns = ['kw_stem', 'count']\n",
    "kw_stem_count_df = kw_stem_count_df.sort_values(by = ['count', 'kw_stem'], ascending = [False, True]).reset_index()\n",
    "kw_stem_count_df.drop(['index'], axis = 1, inplace = True)\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "    3. Fields of study count\n",
    "'''\n",
    "\n",
    "fos_df = pd.DataFrame()\n",
    "fos_count_df = pd.DataFrame()\n",
    "\n",
    "if 'fos' in biblio_df.columns:     # if biblio_df has a fields of study column\n",
    "\n",
    "    # Create fields of study table\n",
    "    fos_df = biblio_df[['fos']].copy()\n",
    "\n",
    "    # Create a single keyword list by exploding the table\n",
    "    fos_df = fos_df.apply(lambda x: x.str.split(';')).explode('fos').reset_index()\n",
    "    fos_df.drop(['index'], axis = 1, inplace = True)\n",
    "\n",
    "    # Remove NaN entries\n",
    "    fos_df.dropna(subset = ['fos'], inplace = True)\n",
    "    fos_df = fos_df[fos_df['fos'] != 'nan']\n",
    "\n",
    "    # Create a count table for the Lens fields of study\n",
    "    fos_count_df = pd.DataFrame(fos_df['fos'].value_counts()).reset_index()\n",
    "    fos_count_df.columns = ['fos', 'count']\n",
    "    fos_count_df = fos_count_df.sort_values(by = ['count', 'fos'], ascending = [False, True]).reset_index()\n",
    "    fos_count_df.drop(['index'], axis = 1, inplace = True)\n",
    "\n",
    "    if logger.getEffectiveLevel() == logging.INFO:\n",
    "        print(f'Number of unique fields of study: {len(fos_count_df)}')\n",
    "        print(f\"{biblio_df['fos'].count()} publications of a total of {n_all_pubs} have fields of study\")\n",
    "\n",
    "'''\n",
    "# Create a table with the count of counts of Lens keywords\n",
    "kw_count_count_df = pd.DataFrame(kw_count_df['count'].value_counts()).reset_index().rename(columns = {'count': 'count_count', 'index': 'count'})\n",
    "total_counts = kw_count_count_df['count_count'].sum()\n",
    "kw_count_count_df['count_count_pct'] = kw_count_count_df['count_count'] / total_counts\n",
    "kw_count_count_df = kw_count_count_df.sort_values(by = ['count_count', 'count'], ascending = [False, True]).reset_index()\n",
    "kw_count_count_df.drop(['index'], axis = 1, inplace = True)\n",
    "\n",
    "if logger.getEffectiveLevel() == logging.INFO:\n",
    "    display(kw_count_count_df.head())\n",
    "    print(f'count = 1 are the keywords that appear once in the Scopus keywords etc.\\n')\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "    4. BERTopic count\n",
    "'''\n",
    "\n",
    "bertopic_df = pd.DataFrame()\n",
    "\n",
    "if 'tp_name' in biblio_df.columns:  # biblio_df has BERTopic information\n",
    "    pass\n",
    "\n",
    "'''\n",
    "    5. keyBERT count\n",
    "'''\n",
    "\n",
    "keybert_count_df = pd.DataFrame()\n",
    "\n",
    "if keybert_title_count or keybert_abstract_count:   # generate title and abstract keywords using keyBERT\n",
    "\n",
    "    kw_model = KeyBERT(model = 'all-MiniLM-L6-v2')\n",
    "\n",
    "    def generate_keybert_keywords(row):\n",
    "        tuples = []\n",
    "        new_columns = {}\n",
    "\n",
    "        if(row.name % 10 == 0):\n",
    "            print(f'{row.name}', end = '\\r')\n",
    "        \n",
    "        # Generate keywords for the title and/or the abstract\n",
    "        if keybert_title_count:\n",
    "            if not keybert_use_mmr:\n",
    "                tuples = kw_model.extract_keywords(row['title'], keyphrase_ngram_range = (1, 2), \n",
    "                                                stop_words = 'english', top_n = n_keybert_kws)\n",
    "            else :\n",
    "                tuples = kw_model.extract_keywords(row['title'], keyphrase_ngram_range = (1, 2), \n",
    "                                                stop_words = 'english', top_n = n_keybert_kws,\n",
    "                                                use_mmr = True, diversity = mmr_diversity)\n",
    "        \n",
    "            # Create a dictionary to store the new columns\n",
    "\n",
    "            for i, (a, b) in enumerate(tuples):\n",
    "                new_columns[f'kbt_{i+1}'] = a\n",
    "                new_columns[f'kbt_p_{i+1}'] = b\n",
    "            # Return the new columns as a Series\n",
    "\n",
    "        if keybert_abstract_count:\n",
    "            if not keybert_use_mmr:\n",
    "                tuples = kw_model.extract_keywords(row['abstract'], keyphrase_ngram_range = (1, 2), \n",
    "                                                stop_words = 'english', top_n = n_keybert_kws)\n",
    "            else :\n",
    "                tuples = kw_model.extract_keywords(row['abstract'], keyphrase_ngram_range = (1, 2), \n",
    "                                                stop_words = 'english', top_n = n_keybert_kws,\n",
    "                                                use_mmr = True, diversity = mmr_diversity)\n",
    "\n",
    "            # Create a dictionary to store the new columns\n",
    "\n",
    "            for i, (a, b) in enumerate(tuples):\n",
    "                new_columns[f'kba_{i+1}'] = a\n",
    "                new_columns[f'kba_p_{i+1}'] = b\n",
    "            # Return the new columns as a Series\n",
    "\n",
    "        return pd.Series(new_columns)\n",
    "\n",
    "    # Create new columns in biblio_df with the keybert keyword information\n",
    "    print(f'Generating keyBERT keywords...')\n",
    "\n",
    "    new_columns = biblio_df.apply(generate_keybert_keywords, axis=1)\n",
    "    biblio_df = pd.concat([biblio_df, new_columns], axis=1)\n",
    "\n",
    "    # Melt the keybert keywords in biblio_df to create a single column with all the values\n",
    "    keybert_count_df = biblio_df.loc[:, [col for col in biblio_df.columns if col.startswith('kbt_') and not col.startswith('kbt_p')]]\n",
    "    keybert_count_df = pd.melt(keybert_count_df)\n",
    "    keybert_count_df = keybert_count_df.drop(columns=['variable'])\n",
    "    counts = keybert_count_df.value_counts().reset_index()\n",
    "    keybert_count_df = pd.DataFrame({'string': counts.iloc[:, 0], 'count': counts.iloc[:, 1]})\n",
    "\n",
    "    if logger.getEffectiveLevel() == logging.INFO:\n",
    "        print(f'Number of unique keybert keywords: {len(keybert_count_df)}')\n",
    "        display(keybert_count_df.head(20))\n",
    "\n",
    "# Create timestamp\n",
    "if timestamping:\n",
    "    timestamp = '_' + datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "else:\n",
    "    timestamp = ''\n",
    "\n",
    "# Write results to CSV\n",
    "if write_st_filtered_csv:\n",
    "    print(f'Saving file {biblio_st_filtered_file_csv + timestamp}.csv ...')\n",
    "    biblio_df.to_csv(root_dir + results_dir + biblio_st_filtered_file_csv + timestamp + '.csv', index = False)\n",
    "\n",
    "if write_csv:\n",
    "    print(f'Saving file {biblio_st_count_csv + timestamp}.csv ...')\n",
    "    st_count_df.to_csv(root_dir + results_dir + biblio_st_count_csv + timestamp + '.csv', index = False)\n",
    "\n",
    "    print(f'Saving file {biblio_kw_count_csv + timestamp}.csv ...')\n",
    "    kw_count_df.to_csv(root_dir + results_dir + biblio_kw_count_csv + timestamp + '.csv', index = False)\n",
    "\n",
    "    if len(fos_count_df):\n",
    "        print(f'Saving file {biblio_fos_count_csv + timestamp}.csv ...')\n",
    "        fos_count_df.to_csv(root_dir + results_dir + biblio_fos_count_csv + timestamp + '.csv', index = False)\n",
    "\n",
    "    if len(keybert_count_df):\n",
    "        print(f'Saving file {biblio_keybert_count_csv + timestamp}.csv ...')\n",
    "        keybert_count_df.to_csv(root_dir + results_dir + biblio_keybert_count_csv + timestamp + '.csv', index = False)\n",
    "\n",
    "print(f'DONE!')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "L9JKjBSNlDhg",
    "tags": []
   },
   "source": [
    "### Filtering Lens keywords & fields of study and associated keywords & fields of study\n",
    "For a Lens bibliographic dataset with a given number of articles, there can be up to 10 times as many keywords. This script provides boolean filtering on the original or stemmed Scopus keywords. It produces two tables: (1) the keywords that match the boolean filter 'filter' (set in the Parameters below) and (2) the other keywords, filtered by 'filter_assoc' of the entries that contain the keywords that match the boolean filter 'filter'.\n",
    "\n",
    "You can use any boolean expression with 'and' and 'or' keywords and with any number of parentheses. Examples\n",
    "* cardi\n",
    "* heart and cardi\n",
    "* (neural network or deep learning) and convoluted\n",
    "\n",
    "The notebook output is a truncated (see Parameters) HTML table that should be easier to inspect than the full keyword list. You can also write the table to an HTML file, which when opened in the browser gives you an additional way to search by using the browser search feature. This then highlights matches directly in the HTML.\n",
    "\n",
    "**Parameters**\n",
    "* filter: a boolean filter term applied to the Scopus keywords of the full dataframe.\n",
    "* filter_assoc: a boolean filter that creates a table with the keywords that co-occur with the keywords matched by the filter.\n",
    "* num_cols: number of columns in the output HTML table.\n",
    "* max_rows: the output table will be truncated at max_rows rows.\n",
    "* stemmed_kws: whether the filters are applied to the stemmed keywords list.\n",
    "* write_html: set to True to generate the HTML file of the keywords.\n",
    "* file_html_out: the output HTML file name.\n",
    "\n",
    "#### TODO\n",
    "- Add a topic filter here, using the bertopic numbers. As part of the HTML printouts, display all the topics in an HTML table and also display the filtered topics, as a check.\n",
    "- Add a keybert filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b97mUyn_lUll",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "filter = \"surrogate model\"\n",
    "filter_assoc = \"\"\n",
    "filter_fos = \"\"\n",
    "\n",
    "num_cols = 5\n",
    "max_rows = 50\n",
    "stemmed_kws = False\n",
    "\n",
    "read_csv = False\n",
    "lens_kw_count_csv = 'lens_kw_count_ml_in_medicine_all'\n",
    "lens_st_count_csv = 'lens_st_count_ml_in_medicine_all'\n",
    "lens_fos_count_csv = 'lens_fos_count_ml_in_medicine_all'\n",
    "\n",
    "write_html = False\n",
    "file_html_out = 'filtered_kws'\n",
    "\n",
    "# ------------------------------------------\n",
    "\n",
    "# For large datasets, we read the initial dataframes from file\n",
    "if read_csv:\n",
    "    kw_count_df = pd.read_csv(root_dir + results_dir + lens_kw_count_csv + '.csv')\n",
    "    st_count_df = pd.read_csv(root_dir + results_dir + lens_st_count_csv + '.csv')\n",
    "    fos_count_df = pd.read_csv(root_dir + results_dir + lens_fos_count_csv + '.csv')\n",
    "\n",
    "\n",
    "'''\n",
    "    Count of search terms (no filter is applied here)\n",
    "'''\n",
    "\n",
    "st_count = st_count_df.apply(lambda row: str(row['search_term']) + ' (' + str(row['count']) + ')', axis=1).tolist()\n",
    "num_rows = len(st_count) // num_cols + (len(st_count) % num_cols > 0)\n",
    "num_rows_p = num_rows\n",
    "\n",
    "# Create an HTML string to display the list of strings in a table\n",
    "st_html_str = '<table style=\"width:100%;\">'\n",
    "for j in range(num_cols):\n",
    "    st_html_str += '<td style=\"vertical-align:top;\">'\n",
    "    for i in range(num_rows):\n",
    "        idx = j * num_rows + i\n",
    "        if idx < len(st_count) and st_count[idx]:\n",
    "            st_html_str += '{}<br>'.format(st_count[idx])\n",
    "    st_html_str += '</td>'\n",
    "st_html_str += '</table>'\n",
    "\n",
    "print(f'\\nSEARCH TERMS (FULL DATASET)')\n",
    "print(\"------------------------------------\")\n",
    "print(f'(displaying {num_rows} rows of {num_rows_p})')\n",
    "print(f'Number of Scopus search terms: {len(st_count)}')\n",
    "\n",
    "# Display the HTML table\n",
    "display(HTML(st_html_str))\n",
    "\n",
    "\n",
    "'''\n",
    "    Count of keywords matched by filter\n",
    "'''\n",
    "\n",
    "# List of keywords with their counts, as strings\n",
    "if stemmed_kws:\n",
    "    kws_count = kw_stem_count_df.apply(lambda row: str(row['kw_stem']) + ' (' + str(row['count']) + ')', axis=1).tolist()\n",
    "else:\n",
    "    kws_count = kw_count_df.apply(lambda row: str(row['kw']) + ' (' + str(row['count']) + ')', axis=1).tolist()\n",
    "\n",
    "# Filter the keywords using the filter provided above\n",
    "def evaluate_expression(text, expression):\n",
    "    words = re.split(r'\\(|\\)|\\b(?:and|or)\\b', expression)\n",
    "    words = [x.strip() for x in words if x.strip()]\n",
    "    values = {word: word in text for word in words}\n",
    "\n",
    "    for key in values:\n",
    "        expression = expression.replace(key, str(values[key]))\n",
    "\n",
    "    return eval(expression)\n",
    "\n",
    "if filter != \"\":\n",
    "    kws_count = [x for x in kws_count if evaluate_expression(x, filter)]\n",
    "\n",
    "# Calculate the number of rows in the table\n",
    "num_rows = len(kws_count) // num_cols + (len(kws_count) % num_cols > 0)\n",
    "num_rows_p = num_rows\n",
    "\n",
    "if num_rows > max_rows:\n",
    "    num_rows = max_rows\n",
    "\n",
    "# Create an HTML string to display the list of strings in a table\n",
    "kws_html_str = '<table style=\"width:100%;\">'\n",
    "for j in range(num_cols):\n",
    "    kws_html_str += '<td style=\"vertical-align:top;\">'\n",
    "    for i in range(num_rows):\n",
    "        idx = j * num_rows + i\n",
    "        if idx < len(kws_count) and kws_count[idx]:\n",
    "            kws_html_str += '{}<br>'.format(kws_count[idx])\n",
    "    kws_html_str += '</td>'\n",
    "kws_html_str += '</table>'\n",
    "\n",
    "if stemmed_kws:\n",
    "    print(f'\\nFILTERED STEMMED KEYWORDS')\n",
    "    print(f'(displaying {num_rows} rows of {num_rows_p})')\n",
    "    print(\"--------------------------\")\n",
    "    print(f'Number of unique stemmed Lens keywords: {len(kw_stem_count_df)}')\n",
    "else:\n",
    "    print(f'\\nFILTERED KEYWORDS')\n",
    "    print(f'(displaying {num_rows} rows of {num_rows_p})')\n",
    "    print(\"--------------------------\")\n",
    "    print(f'Number of unique Lens keywords: {len(kw_count_df)}')\n",
    "    \n",
    "print(f'Number of keyword matches for filter: {len(kws_count)}')\n",
    "print(f'\\nFilter term: {filter}\\n')\n",
    "\n",
    "# Display the HTML table\n",
    "display(HTML(kws_html_str))\n",
    "\n",
    "\n",
    "'''\n",
    "    Associated keywords count after applying the filter 'filter_assoc'. When the \n",
    "    filter 'filter' is applied to the keywords in the dataset, an associated set of keywords\n",
    "    is generated that consists of all keywords in those articles that were matched by the filter\n",
    "    'filter'. Subsequently, the filter 'filter_assoc' is applied to home in on a particular\n",
    "    subset of the typically very large associated keyword set.\n",
    "'''\n",
    "\n",
    "# Create table with Lens keywords\n",
    "kws_assoc_count_df = biblio_df[['kws']].copy()\n",
    "\n",
    "# Remove NaN entries\n",
    "kws_assoc_count_df.dropna(subset = ['kws'], inplace = True)\n",
    "kws_assoc_count_df = kws_assoc_count_df[kws_assoc_count_df['kws'].str.lower() != 'nan']\n",
    "\n",
    "# Apply the filter\n",
    "if filter != \"\":\n",
    "    kws_assoc_count_df = kws_assoc_count_df[kws_assoc_count_df['kws'].apply(evaluate_expression, expression = filter)]\n",
    "\n",
    "#display(kws_assoc_count_df)\n",
    "\n",
    "# Create a single keyword list by exploding the table\n",
    "kws_assoc_count_df = kws_assoc_count_df.apply(lambda x: x.str.split(',')).explode('kws').reset_index()\n",
    "kws_assoc_count_df.drop(['index'], axis = 1, inplace = True)\n",
    "\n",
    "#display(kws_assoc_count_df)\n",
    "\n",
    "# Create a count table for the keywords\n",
    "kw_assoc_count_df = pd.DataFrame(kws_assoc_count_df['kws'].value_counts()).reset_index()\n",
    "kw_assoc_count_df.columns = ['kw', 'count']\n",
    "kw_assoc_count_df = kw_assoc_count_df.sort_values(by = ['count', 'kw'], ascending = [False, True]).reset_index()\n",
    "kw_assoc_count_df.drop(['index'], axis = 1, inplace = True)\n",
    "\n",
    "kw_assoc_stem_count_df = pd.DataFrame()\n",
    "\n",
    "if stemmed_kws:\n",
    "\n",
    "    # Create stemmed keywords\n",
    "    kws_assoc_count_df['kws_stemmed'] = kws_assoc_count_df['kws'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
    "\n",
    "    # Create a count table for the stemmed keywords\n",
    "    kw_assoc_stem_count_df = pd.DataFrame(kws_assoc_count_df['kws_stemmed'].value_counts()).reset_index()\n",
    "    kw_assoc_stem_count_df.columns = ['kw_stem', 'count']\n",
    "    kw_assoc_stem_count_df = kw_assoc_stem_count_df.sort_values(by = ['count', 'kw_stem'], ascending = [False, True]).reset_index()\n",
    "    kw_assoc_stem_count_df.drop(['index'], axis = 1, inplace = True)\n",
    "\n",
    "#display(kws_assoc_count_df)\n",
    "\n",
    "# List of keywords with their count, as strings\n",
    "if (stemmed_kws == True) and (len(kw_assoc_stem_count_df) > 0):\n",
    "    kws_assoc_count = kw_assoc_stem_count_df.apply(lambda row: row['kw_stem'] + ' (' + str(row['count']) + ')', axis=1).tolist()\n",
    "elif (stemmed_kws == False) and (len(kw_assoc_count_df) > 0):\n",
    "    kws_assoc_count = kw_assoc_count_df.apply(lambda row: row['kw'] + ' (' + str(row['count']) + ')', axis=1).tolist()\n",
    "else:\n",
    "    kws_assoc_count = []\n",
    "\n",
    "if filter_assoc != \"\":\n",
    "    kws_assoc_count = [x for x in kws_assoc_count if evaluate_expression(x, filter_assoc)]\n",
    "\n",
    "# Calculate the number of rows in the table\n",
    "num_rows = len(kws_assoc_count) // num_cols + (len(kws_assoc_count) % num_cols > 0)\n",
    "num_rows_p = num_rows\n",
    "\n",
    "if num_rows > 0:\n",
    "\n",
    "    if num_rows > max_rows:\n",
    "        num_rows = max_rows\n",
    "\n",
    "  # Create an HTML string to display the list of strings in a table\n",
    "    ka_html_str = '<table style=\"width:100%;\">'\n",
    "    for j in range(num_cols):\n",
    "        ka_html_str += '<td style=\"vertical-align:top;\">'\n",
    "        for i in range(num_rows):\n",
    "            idx = j * num_rows + i\n",
    "            if idx < len(kws_assoc_count) and kws_assoc_count[idx]:\n",
    "                ka_html_str += '{}<br>'.format(kws_assoc_count[idx])\n",
    "        ka_html_str += '</td>'\n",
    "    ka_html_str += '</table>'\n",
    "else:\n",
    "    ka_html_str = ''\n",
    "\n",
    "if stemmed_kws:\n",
    "    print(f'\\nFILTERED STEMMED ASSOCIATED KEYWORDS')\n",
    "    print(f'(displaying {num_rows} rows of {num_rows_p})')\n",
    "    print(\"------------------------------\")\n",
    "    print(f'Number of kw_assoc_stem_count terms: {len(kw_assoc_stem_count_df)}')\n",
    "else:\n",
    "    print(f'\\nFILTERED ASSOCIATED KEYWORDS')\n",
    "    print(f'(displaying {num_rows} rows of {num_rows_p})')\n",
    "    print(\"---------------------------\")\n",
    "    print(f'Number of kw_assoc_count terms: {len(kw_assoc_count_df)}')\n",
    "\n",
    "print(f'Number of keyword matches for filter_assoc: {len(kws_assoc_count)}')\n",
    "print(f'\\nFilter term: {filter_assoc}\\n')\n",
    "\n",
    "# Display the HTML table\n",
    "display(HTML(ka_html_str))\n",
    "\n",
    "\n",
    "'''\n",
    "    Count of fields of study matched by filter\n",
    "'''\n",
    "\n",
    "if len(fos_count_df):\n",
    "\n",
    "    # List of FOS with their counts, as strings\n",
    "    fos_count = fos_count_df.apply(lambda row: str(row['fos']) + ' (' + str(row['count']) + ')', axis=1).tolist()\n",
    "\n",
    "    # Filter the FOS using the filter provided above\n",
    "    def evaluate_expression(text, expression):\n",
    "        words = re.split(r'\\(|\\)|\\b(?:and|or)\\b', expression)\n",
    "        words = [x.strip() for x in words if x.strip()]\n",
    "        values = {word: word in text for word in words}\n",
    "\n",
    "        for key in values:\n",
    "            expression = expression.replace(key, str(values[key]))\n",
    "\n",
    "        return eval(expression)\n",
    "\n",
    "    if filter_fos != \"\":\n",
    "        fos_count = [x for x in fos_count if evaluate_expression(x, filter_fos)]\n",
    "\n",
    "    # Calculate the number of rows in the table\n",
    "    num_rows = len(fos_count) // num_cols + (len(fos_count) % num_cols > 0)\n",
    "    num_rows_p = num_rows\n",
    "\n",
    "    if num_rows > max_rows:\n",
    "        num_rows = max_rows\n",
    "\n",
    "    # Create an HTML string to display the list of strings in a table\n",
    "    fos_html_str = '<table style=\"width:100%;\">'\n",
    "    for j in range(num_cols):\n",
    "        fos_html_str += '<td style=\"vertical-align:top;\">'\n",
    "        for i in range(num_rows):\n",
    "            idx = j * num_rows + i\n",
    "            if idx < len(fos_count) and fos_count[idx]:\n",
    "                fos_html_str += '{}<br>'.format(fos_count[idx])\n",
    "        fos_html_str += '</td>'\n",
    "    fos_html_str += '</table>'\n",
    "\n",
    "    print(f'\\nFILTERED FIELDS OF STUDY')\n",
    "    print(f'(displaying {num_rows} rows of {num_rows_p})')\n",
    "    print(\"--------------------------\")\n",
    "    print(f'Number of unique Lens fields of study: {len(fos_count_df)}')\n",
    "    print(f'Number of fields of study matches for filter: {len(fos_count)}')\n",
    "    print(f'\\nFilter term: {filter_fos}\\n')\n",
    "\n",
    "    # Display the HTML table\n",
    "    display(HTML(fos_html_str))\n",
    "\n",
    "\n",
    "'''\n",
    "    Write HTML\n",
    "'''\n",
    "\n",
    "if write_html:\n",
    "    with open(root_dir + results_dir + file_html_out + '.html', 'w') as f:\n",
    "        f.write(str(HTML('<h3>Search terms (full dataset)</h3>').data))\n",
    "        f.write(str(HTML(st_html_str).data))\n",
    "        f.write(str(HTML('<h3>Filtered bibliographic keywords</h3>').data))\n",
    "        f.write(str(HTML(kws_html_str).data))\n",
    "        f.write(str(HTML('<h3>Filtered associated bibliographic keywords</h3>').data))\n",
    "        f.write(str(HTML(ka_html_str).data))\n",
    "        if len(fos_count_df):\n",
    "            f.write(str(HTML('<h3>Filtered fields of study</h3>').data))\n",
    "            f.write(str(HTML(ka_html_str).data))\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "NzPKCVyjihDQ",
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "### Highlighted search terms and keyword filter terms and save to HTML and/or Excel\n",
    "When skimming through larger numbers of titles and abstracts from a bibliograophic search result, it is possible to speed up the task by highlighting the search terms directly in the title and abstracts. This script highlights in bold red the search terms used in the original Scopus search and in bold blue the filter terms provided in the Parameter section below. The dataframe can be saved to an HTML file. The next cell builds the Excel file from scratch with the same highlighhts.\n",
    "\n",
    "**Parameters**\n",
    "* filter: a boolean filter string with the same specs than the boolean filters in the keyword searches, with one difference. You can specifiy where you want to search by prepending T, A, and/or K (stanfing for title, abstract, and keyword search respectively) to any of the search strings. If you provide a search term with none of the three letters prepended, it will search in all three (that is, in title, abstract, and keyword).\n",
    "* write_html: set to True to generate the HTML file of the highlights.\n",
    "* file_html_out: the output HTML file name.\n",
    "\n",
    "#### TODO\n",
    "- Add a bertopic filter here, using the topic numbers. As part of the HTML printouts, display all the topics in an HTML table and also display the filtered topics, as a check.\n",
    "- Add a keybert filter (low priority)\n",
    "- Add the possibility of colouring additional keywords that are not used to filter the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-KPCCTE5gqGg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading the input file scopus_lens_ml_sim_engineering_all_st_plus_abstract_topics.csv ...\n",
      "Filtering the dataframe...\n",
      "73500\n",
      "2684 filter matches of a total of 73557 publications\n",
      "Highlighting the filter keywords in the titles...\n",
      "2600\n",
      "Highlighting the filter keywords in the abstracts...\n",
      "2600\n",
      "Highlighting the filter keywords in the keywords...\n",
      "2600\n",
      "Highlighting the original search terms in the titles...\n",
      "2600\n",
      "Highlighting the original search terms in the abstracts...\n",
      "2600\n",
      "Highlighting the original search terms in the keywords...\n",
      "Saving results...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>kws</th>\n",
       "      <th>fos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>A <span style=\"color: blue; font-weight: bold\">CNN</span> based <span style=\"color: blue; font-weight: bold\">surrogate</span> model of isogeometric analysis in nonlocal flexoelectric problems</td>\n",
       "      <td>We proposed a convolutional <span style=\"color: blue; font-weight: bold\">neural network</span> <span style=\"color: blue; font-weight: bold\">CNN</span> -based <span style=\"color: blue; font-weight: bold\">surrogate</span> model to predict the nonlocal response for flexoelectric structures with complex topologies. The input i.e. the binary images for the <span style=\"color: blue; font-weight: bold\">CNN</span> is obtained by converting geometries into pixels while the output comes from <span style=\"color: blue; font-weight: bold\">simulations</span> of an isogeometric IGA flexoelectric model which in turn exploits the higher-order continuity of the underlying non-uniform rational B-splines NURBS basis functions to fast computing of flexoelectric parameters e.g. electric gradient mechanical displacement strain and strain gradient. To generate the dataset of porous flexoelectric cantilevers we developed a NURBS trimming technique based on the IGA model. As for <span style=\"color: blue; font-weight: bold\">CNN</span> construction the key factors were optimized based on the IGA dataset including activation functions dropout layers and optimizers. Then the cross-validation was conducted to test the <span style=\"color: blue; font-weight: bold\">CNNgeneralization</span> ability. Last but not least the potential of the <span style=\"color: blue; font-weight: bold\">CNN</span> performance has been explored under different model output sizes and the corresponding possible optimal model layout is proposed. The can be instructive for studies on <span style=\"color: blue; font-weight: bold\">deep learning</span> of other nonlocal mech-physical <span style=\"color: blue; font-weight: bold\">simulations</span>. 2022 The Author.</td>\n",
       "      <td>trimming,network-based,isogeometric analysis,convolutional <span style=\"color: blue; font-weight: bold\">neural networks</span>,trimming techniques,topology,<span style=\"color: blue; font-weight: bold\">deep learning</span>,rational functions,nurbs trimming technique,convolution,binary images,nonlocal flexoelectricity,flexoelectric,flexoelectricity,convolutional <span style=\"color: blue; font-weight: bold\">neural network</span>,non-uniform rational b-splines,non-uniform rational b-spline trimming technique,interpolation,nonlocal</td>\n",
       "      <td>Isogeometric analysis; Convolutional neural network; Computer science; Surrogate model; Generalization; Basis function; Topology (electrical circuits); Algorithm; Applied mathematics; Mathematics; Mathematical optimization; Artificial intelligence; Mathematical analysis; Structural engineering; Finite element method; Combinatorics; Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>A <span style=\"color: blue; font-weight: bold\">PINN</span> <span style=\"color: blue; font-weight: bold\">surrogate</span> modeling methodology for steady state integrated thermofluid systems modeling</td>\n",
       "      <td><span style=\"color: blue; font-weight: bold\">Physics-informed</span> <span style=\"color: blue; font-weight: bold\">neural networks</span> <span style=\"color: blue; font-weight: bold\">PINNs</span> were developed to overcome the limitations associated with the acquisition of large training data sets that are commonly encountered when using purely data-driven <span style=\"color: blue; font-weight: bold\">machine learning</span> methods. This paper proposes a <span style=\"color: blue; font-weight: bold\">PINN</span> <span style=\"color: blue; font-weight: bold\">surrogate</span> modeling methodology for steady-state integrated thermofluid systems modeling based on the mass energy and momentum balance equations combined with the relevant component characteristics and fluid property relationships. The methodology is applied to two thermofluid systems that encapsulate the important phenomena typically encountered namely a heat exchanger network with two different fluid streams and components linked in series and parallel and ii a recuperated closed Brayton cycle with various turbomachines and heat exchangers. The generated with the <span style=\"color: blue; font-weight: bold\">PINN</span> models were compared to benchmark solutions generated via conventional physics-based thermofluid process models. The largest average relative errors are 0.17 and 0.93 for the heat exchanger network and Brayton cycle respectively. It was shown that the use of a hybrid Adam-TNC optimizer requires between 180 and 690 fewer iterations during the training process thus providing a significant computational advantage over a pure Adam optimization approach. The resulting <span style=\"color: blue; font-weight: bold\">PINN</span> models can make predictions 75 to 88 times faster than their respective conventional process models. This highlights the potential for <span style=\"color: blue; font-weight: bold\">PINN</span> <span style=\"color: blue; font-weight: bold\">surrogate</span> models as a valuable engineering tool in component and system design and optimization as well as in real-time <span style=\"color: blue; font-weight: bold\">simulation</span> for <span style=\"color: blue; font-weight: bold\">anomaly detection</span> diagnosis and forecasting.</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>A bayesian based inspection monitoring data fusion approach for historical buildings and its post earthquake application to a monumental masonry palace</td>\n",
       "      <td>Many countries exposed to high levels of seismic risk including Italy are facing a huge challenge in promptly quantifying post-earthquake damages to their built historical heritage. In this context structural health monitoring plays a fundamental role allowing to continuously track changes in selected damage-sensitive features. However monitoring data interpretation is often not univocal and may be affected by large uncertainty provoking false positives and false negatives. Hence this research proposes a novel approach for post-earthquake structural condition assessment by exploiting the aggregation of different sources of information notably steering from both monitoring and visual inspection campaigns in order to take risk-informed decisions. More in depth an automatic tool is proposed to detect and locate structural damages in monumental structures with the aid of a data fusion approach including vibration-based system identification static and dynamic measurements <span style=\"color: blue; font-weight: bold\">finite element</span> FE and <span style=\"color: blue; font-weight: bold\">surrogate</span> modeling Bayesian-based model updating and visual inspections. In a preliminary phase potential damage-sensitive regions in the structure are identified through FE-based <span style=\"color: blue; font-weight: bold\">numerical analysis</span> and engineering judgment. Then the solution of the inverse problem aimed at deriving the Bayesian posterior statistics of the uncertain parameters is entrusted to a computational-effective <span style=\"color: blue; font-weight: bold\">surrogate</span> model SMFinally the Bayesian-based updated parameters are adjusted considering the different allowable sources of information to achieve a final assessment. The effectiveness of the proposed approach is demonstrated by using the recorded data acquired in the Consoli Palace an historical building located in Umbria central Italy which has been continuously monitored since 2017 using dynamic static and environmental sensors and which has been hit by low-intensity earthquakes in 2021. 2022 The Authorunder exclusive licence to Springer Nature B.V.</td>\n",
       "      <td>bayesian,risk assessment,continuous monitoring,<span style=\"color: blue; font-weight: bold\">bayesian networks</span>,historic masonry construction,inverse problems,uncertainty analysis,historic masonry,data interpretation,earthquake damages,historical buildings,earthquakes,data fusion,<span style=\"color: blue; font-weight: bold\">finite element</span> method,masonry materials,masonry construction,damage detection,damage classification,masonry,umbria,bayesian model updating,health monitoring,building,monitoring,post-earthquake damage classification,<span style=\"color: blue; font-weight: bold\">surrogate</span> modeling,structural health monitoring,italy</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>A bayesian optimization algorithm for the optimization of mooring system design using time domain analysis</td>\n",
       "      <td>Dynamic analysis can consider the complex behavior of mooring systems. However the relatively long analysis time of the dynamic analysis makes it difficult to use in the design of mooring systems. To tackle this we present a Bayesian optimization algorithm BOA which is well known as fast convergence using a small number of data points. The BOA evaluates design candidates using a probability-based function which is updated during the optimization process as more data points are achieved. In a case study we applied the BOA to improve an initial mooring system that had been designed by human experts. The BOA was also compared with a <span style=\"color: blue; font-weight: bold\">genetic algorithm</span> GA that used a pre-trained <span style=\"color: blue; font-weight: bold\">surrogate</span> model for fast evaluation. The optimal designs that were determined by both the BOA and GA have a 50 lower maximum tension than the initial design. However the computation time of the GA needed 20 times more than that of the BOA because of the training time of the <span style=\"color: blue; font-weight: bold\">surrogate</span> model.</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>A beam on elastic foundation method for predicting deflection of braced excavations considering uncertainties</td>\n",
       "      <td>Predicting wall deflection is important to provide a critical reference to evaluate the current construction conditions and prevent potential damage risks of adjacent facilities during excavations. This paper presents a combination of a beam on elastic foundation model BEFM and the Bayesian framework to realize effective probabilistic predictions of wall deflection at various depths in braced excavations. First a <span style=\"color: blue; font-weight: bold\">finite element</span> solving algorithm to calculate wall deflection for the BEFM is developed and incorporated into the Bayesian framework. Next the most suitable distribution pattern for soil resistance and an appropriate set of uncertain parameters in the BEFM are determined through the application of the Bayesian model selection technique. Meanwhile the uncertain parameters are updated. A prediction is then made using the optimal model and corresponding posterior probability distributions of the updated parameters at each stage. The parameter updating and prediction process are repeated as additional field observations become available during construction. The performance of the proposed method is examined using a field case study. The show that this method provides a satisfactory approach to predict both the magnitudes and profile of deflection when considering uncertainties. Additionally comparisons with a Bayesian updating framework using a <span style=\"color: blue; font-weight: bold\">surrogate</span> model i.e. the KJHH model indicate higher updating efficiency of the developed method. 2022 John Wiley Sons Ltd.</td>\n",
       "      <td>wall deflection,uncertain parameters,<span style=\"color: blue; font-weight: bold\">bayesian networks</span>,deflection (structures),beam on elastic foundation,elastic foundation model,beam on elastic foundation method,bayesian frameworks,uncertainty,forecasting,uncertainty analysis,wall,probability,bayesian analysis,braced excavations,<span style=\"color: blue; font-weight: bold\">finite element</span> method,probability distributions,bayesian probabilistic method,excavation,bayesian probabilistic methods,current construction,wall deflections,prediction,beams on elastic foundation</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>A comparative study of learning techniques for the compressible aerodynamics over a transonic RAE2822 airfoil</td>\n",
       "      <td>In this study the modeling of the compressible pressure field on the RAE 2822 airfoil using <span style=\"color: blue; font-weight: bold\">deep learning</span> DL is investigated. The is to generate at low cost the complete Mach envelope from a given aerodynamic database. A dataset with more than 2.000 RANS <span style=\"color: blue; font-weight: bold\">simulations</span> at various angles of attack and Mach numbers has been created and validated to train fast and accurate <span style=\"color: blue; font-weight: bold\">surrogate</span> models model of the pressure field. The procedure is inspired by the well-known analytical Prandtl Glauert transformation which maps solutions from one Mach number to another but however fails in the transonic regime because of the strong non-linearities due to shock waves. Thus instead of a classical direct generation of the pressure fields DL is used here to perform a geometric transformation from an incompressible pressure field towards a compressible solution. The key question addressed in this study is how accurate and sample efficient are DL techniques compared with classical <span style=\"color: blue; font-weight: bold\">surrogate</span> modeling tools To answer this question two types of DL are investigated <span style=\"color: blue; font-weight: bold\">CNN</span> and <span style=\"color: blue; font-weight: bold\">GCNN</span> and compared with Proper Orthogonal Decomposition POD coupled to a Gaussian Process Regression GPRThese methods are trained on various input resolutions and database sizes. show that DL models are able to accurately predict the intensity and location of the shock over a wide range of angles of attack and Mach numbers up to the transonic regime. Predictions of the lift coefficient reveal a relative error lower thanwith respect to the high-fidelity data. Because of their inherent nonlinear nature <span style=\"color: blue; font-weight: bold\">CNN</span> and <span style=\"color: blue; font-weight: bold\">GCNN</span> provide more accurate even for small training datasets whereas POD encounters difficulties to reconstruct properly shock waves. Since <span style=\"color: blue; font-weight: bold\">CNN</span> requires inputs and outputs in a pixel-like format it suffers from interpolation errors that can only be mitigated with fine resolutions making the training more difficult. By performing convolutions directly on unstructured data <span style=\"color: blue; font-weight: bold\">GCNN</span> eliminates this interpolation error and provides the best accuracy and sample efficiency. This study shows that advanced DL techniques such as <span style=\"color: blue; font-weight: bold\">CNN</span> and <span style=\"color: blue; font-weight: bold\">GCNN</span> are capable of predicting complex flows and outperform classical tools on such tasks. 2022 Elsevier Ltd</td>\n",
       "      <td>rae2822,learning techniques,shock waves,shock-waves,transonic regime,forecasting,errors,proper orthogonal,<span style=\"color: blue; font-weight: bold\">deep learning</span>,incompressible flow,mach number,orthogonal decomposition,cfd,<span style=\"color: blue; font-weight: bold\">computational fluid</span> dynamics,airfoils,interpolation error,pressure-field,angle of attack,principal component analysis,interpolation,<span style=\"color: blue; font-weight: bold\">surrogate</span> modeling,transonic aerodynamics</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>A computational strategy for determining the optimal scaled wind speed in icing wind tunnel experiments</td>\n",
       "      <td>A new computational strategy for determining the optimal scaled wind speed in icing wind tunnel experiments. Icing numerical computation is involved in icing scaling analysis. A high-efficient <span style=\"color: blue; font-weight: bold\">surrogate</span> model and Differential Evolution <span style=\"color: blue; font-weight: bold\">Genetic Algorithm</span> are used in the method. The new method shows improvement in finding reasonable scaling icing parameters. The icing wind tunnel experiment is one of the most important methods to investigate icing problems. Due to the blockage or capability limitations of icing wind tunnels the geometric size of the testing model is usually needed to be sub-scaled and the corresponding icing conditions are required to be converted. However the icing phenomenon is affected by various parameters and how to determine the optimal subscale wind speed remains inconclusive. To solve this problem a new computational strategy named Improved Ruff Icing Scaling Method IRISM is proposed. In IRISM the Ruff icing scaling theory is firstly applied to calculate the basic sub-scale temperature and the related cloudy parameters etc. and then the icing numerical computation is utilized to evaluate the influence of sub-scale icing conditions on the similarity of the reference and subscale ice shapes. Finally the optimal scaled wind speed is calculated by Differential Evolution <span style=\"color: blue; font-weight: bold\">Genetic Algorithm</span> based on a high-efficient <span style=\"color: blue; font-weight: bold\">surrogate</span> model in which the ice shape consistency especially the ice horn structure is set as the optimizationThe IRISM is comprehensively analyzed by icing numerical computations as well as icing wind tunnel experiments. The show that the IRISM is capable of automatically providing the optimal scaled wind speed with the acceptable icing scaling error. As an important supplement to the current icing scaling methods the IRISM can find the reasonable experimental parameters of scaled icing wind tunnel tests which provides a new way for icing scaling analysis.</td>\n",
       "      <td>nan</td>\n",
       "      <td>Wind tunnel; Meteorology; Environmental science; Wind speed; Computational fluid dynamics; Icing; Marine engineering; Computer science; Mechanics; Physics; Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>A computationally efficient high fidelity multi physics design optimization of traction motors for drive cycle loss minimization</td>\n",
       "      <td>Continuous improvement in performance of interior permanent magnet IPM machines is critical for electric vehicle traction applications. However due to the cross-coupling and saturation effects a significant amount of time-consuming <span style=\"color: blue; font-weight: bold\">finite element</span> analysis FEA <span style=\"color: blue; font-weight: bold\">simulations</span> are required to accurately estimate machine performance. Moreover iterative design optimization will take significantly longer. In this article an improved rapid performance estimation technique utilizing <span style=\"color: blue; font-weight: bold\">surrogate</span> models is developed and coupled with a design optimization algorithm. The proposed framework has significantly less computational cost than alternative <span style=\"color: blue; font-weight: bold\">surrogate</span>-based approaches and efficiently employs drive cycle loss minimization for a multi-physics multi- traction motor design optimization. <span style=\"color: blue; font-weight: bold\">Simulation</span> and experimental suggest the proposed optimization framework yields optimal designs more efficiently than existing methods while maintaining accuracy. 1972-2012 IEEE.</td>\n",
       "      <td>traction motors,<span style=\"color: blue; font-weight: bold\">computational modelling</span>,<span style=\"color: blue; font-weight: bold\">predictive models</span>,design optimization,drive cycle,multi-physic optimization,drive cycles,interior permanent magnet,iterative methods,optimisations,multi-physics,electric traction,couplings,electric losses,<span style=\"color: blue; font-weight: bold\">finite element</span> method,multi-physics optimization,<span style=\"color: blue; font-weight: bold\">surrogate</span> model,permanent magnets,<span style=\"color: blue; font-weight: bold\">surrogate</span> modeling</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>A <span style=\"color: red; font-weight: bold\">data driven model</span> of the yield and strain hardening response of commercially pure titanium in uniaxial stress</td>\n",
       "      <td>This study presents a technique to develop data-driven constitutive models for the elastic-plastic response of materials and applies this technique to the case of commercially pure titanium. The complex yield and strain hardening characteristics of this solid are captured for random non-monotonic uniaxial loading without relying on specific theoretical descriptions. The <span style=\"color: blue; font-weight: bold\">surrogate</span> model is obtained by supervised <span style=\"color: blue; font-weight: bold\">machine learning</span> relying on feed-forward <span style=\"color: blue; font-weight: bold\">neural networks</span> trained with data obtained from random loading of titanium specimens in uniaxial stress. Uniaxial tests are conducted in strain control applying random histories of axial strain in the range 0.04 0.04 to prevent the occurrence of significant damage. The corresponding stress versus strain histories are subdivided into a finite number of increments and <span style=\"color: blue; font-weight: bold\">machine learning</span> is applied to predict the change in stress in each increment. A suitable architecture of the <span style=\"color: blue; font-weight: bold\">data-driven model</span> key to obtaining accurate predictions is presented. The predictions of the <span style=\"color: blue; font-weight: bold\">surrogate</span> model are validated by comparing to experiments not used in the training process and compared to those of an established theoretical model. An excellent agreement is obtained between the measurements and the predictions of the data-driven <span style=\"color: blue; font-weight: bold\">surrogate</span> model. 2023 The Authors</td>\n",
       "      <td>hardening response,uniaxial stress,elastic-plastic response,machine-learning,<span style=\"color: blue; font-weight: bold\">machine learning</span>,titanium compounds,learning systems,elastoplasticity,monotonics,plasticity,cyclic loading,tensile stress,feedforward <span style=\"color: blue; font-weight: bold\">neural networks</span>,commercially pure titanium,forecasting,supervised learning,compressive stress,<span style=\"color: blue; font-weight: bold\">surrogate</span> model,data driven,stress analysis,strain hardening,<span style=\"color: blue; font-weight: bold\">data-driven model</span>,<span style=\"color: blue; font-weight: bold\">surrogate</span> modeling</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>A data driven tip flow loss prediction method for a transonic fan under boundary layer ingesting inflow distortion</td>\n",
       "      <td>In a boundary layer ingesting BLI propulsion system the fan blades need to operate continuously under large-scale inflow distortion. The distortion will lead to serious aerodynamic losses in the fan degrading the fan performance and the overall aerodynamic benefits of the aircraft. Therefore in the preliminary design of a BLI propulsion system it is necessary to evaluate the influence of the fuselage boundary layer under different flight conditions on the fan aerodynamic performance. However a gap exists in the current <span style=\"color: blue; font-weight: bold\">computational methods</span> for BLI fan performance evaluations. The full-<span style=\"color: blue; font-weight: bold\">annulus</span> unsteady Reynolds-averaged Navier-Stokes URANS <span style=\"color: blue; font-weight: bold\">simulations</span> can provide reliable predictions but are computationally expensive for design iterations. The low-order <span style=\"color: blue; font-weight: bold\">computational methods</span> are cost-efficient but rely on the loss models for accurate prediction. The conventional empirical or physics-based loss models show notable limitations under complex distortion-induced off-design working conditions in a BLI fan especially in the rotor tip region compromising the reliability of the low-order <span style=\"color: blue; font-weight: bold\">computational methods</span>. To balance the accuracy and cost of loss prediction the paper proposes a data-driven tip flow loss prediction framework for a BLI fan. It employs a <span style=\"color: blue; font-weight: bold\">neural network</span> to build a <span style=\"color: blue; font-weight: bold\">surrogate</span> model to predict the tip flow loss at complex non-uniform aerodynamic conditions. Physical understandings of the flow features in the BLI fan are integrated into the datadriven modeling process to further reduce the computational cost and improve the methodapplicability. The data-driven prediction method shows good accuracy in predicting the overall values and radial distributions of fan rotor tip flow loss under various BLI inflow distortion conditions. Not only does it have higher accuracy than the conventional physics-based loss models but also needs much less computational time than the <span style=\"color: blue; font-weight: bold\">fullannulus</span> time-accurate <span style=\"color: blue; font-weight: bold\">simulations</span>. The present work has demonstrated a significant potential of data-driven approaches in complex aerodynamic loss modeling and will contribute to future BLI fan design. Copyright 2022 by ASME.</td>\n",
       "      <td>bli fan,<span style=\"color: blue; font-weight: bold\">neural network</span>,tip flow,forecasting,aerodynamics,loss prediction method,boundary layer ingesting fan,loss model,boundary layers,flow loss,loss prediction,<span style=\"color: blue; font-weight: bold\">navier stokes</span> equations,neural-networks,data driven,fans,prediction methods,<span style=\"color: blue; font-weight: bold\">computational methods</span>,tip flow loss</td>\n",
       "      <td>Aerodynamics; Distortion (music); Inflow; Computational fluid dynamics; Boundary layer; Computer science; Fuselage; Engineering; Aerospace engineering; Simulation; Marine engineering; Structural engineering; Mechanics; Physics; Electronic engineering; Amplifier; CMOS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>A deep encoder decoder for <span style=\"color: blue; font-weight: bold\">surrogate</span> modelling of liquid moulding of composites</td>\n",
       "      <td>The paper proposes a <span style=\"color: blue; font-weight: bold\">surrogate</span> model for liquid moulding of structural composites. A methodology is presented to simulate the dual-phase Darcyflow in a heterogeneous porous medium. The approach is an encoder decoder that receives as input a matrix of permeabilities and produces two scalar fields that represent the pressure and front flow. This model is trained with synthetic data generated with a computer fluid dynamics simulator. In this context the lack of robustness of models trained with the popular L2 and L1 losses is highlighted and several enhancements to these baseline approaches are introduced. First the study provides a piece-wise power-logarithmic loss that improves training in the presence of the bimodal distribution of error residuals produced by the dual-phase flow predictions. A non-uniform sampling strategy for the selection of time training snapshots is also included which contributes to improve the prediction accuracy. The estimation of the front flow field is further refined with a multi-task training strategy. The of these improvements in the baseline models reduce the relative error of the pressure and front flow fields by more than 50 performing these <span style=\"color: blue; font-weight: bold\">simulations</span> in a record time of 50 ms. The <span style=\"color: blue; font-weight: bold\">surrogate</span> model is further evaluated as a digital twin to predict in a real experiment the location and spatial extent of race-tracking <span style=\"color: blue; font-weight: bold\">channels</span> and regions with dissimilar degrees of permeability. 2023 The Author</td>\n",
       "      <td>heterogeneous porous media,composite materials,encoder-decoder,decoding,liquid mouldings,molding,dual phase,darcy's flow,forecasting,<span style=\"color: blue; font-weight: bold\">deep learning</span>,signal encoding,dual phasis,convolution,structural composites,liquid moulding,flow fields,porous materials,composites material,<span style=\"color: blue; font-weight: bold\">surrogate</span> modeling,<span style=\"color: blue; font-weight: bold\">surrogate</span> models</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>A deep generative framework for data driven <span style=\"color: blue; font-weight: bold\">surrogate</span> modeling and visualization of parameterized nonlinear dynamical systems</td>\n",
       "      <td>Nonlinear dynamical systems in applications such as design and control typically depend on a set of variable parameters that represent system geometry boundary conditions material properties etc. Such a parameterized dynamical system requires a parameterized model e.g. a parameterized differential equation to describe. On the one hand to discover the wide variety of the parameter-dependent dynamical behaviors repeated <span style=\"color: blue; font-weight: bold\">simulations</span> with the parameterized model are often required over a large range of parameter values leading to significant computational burdens especially when the system is complex strongly nonlinear and or high-dimensional and the high-fidelity model is inefficient to simulate. Thus seeking <span style=\"color: blue; font-weight: bold\">surrogate</span> models that mimic the behaviors of high-fidelity parameterized models while being efficient to simulate is critically needed. On the other hand the governing equations of the parameterized nonlinear dynamical system e.g. an aerodynamic system with a physical model full-scale or scaled in the laboratory for optimization or design tasks may be unknown or partially unknown due to insufficient physics knowledge leading to an inverse problem where we need to identify the models from measurement data only. Accordingly this work presents a novel deep generative framework for data-driven <span style=\"color: blue; font-weight: bold\">surrogate</span> modeling identification of parameterized nonlinear dynamical systems from data only. Specifically the presented framework learns the direct mapping from <span style=\"color: blue; font-weight: bold\">simulation</span> parameters to visualization images of dynamical systems by leveraging deep generative convolutional <span style=\"color: blue; font-weight: bold\">neural networks</span> yielding two advantages the <span style=\"color: blue; font-weight: bold\">surrogate</span> <span style=\"color: blue; font-weight: bold\">simulation</span> is efficient because the calculation of transient dynamics over time is circumvented ii the <span style=\"color: blue; font-weight: bold\">surrogate</span> output retains characterizing ability and flexibility as the visualization image is customizable and supports any visualization scheme for revealing and representing high-level dynamics feature e.g. Poincar mapWe study and demonstrate the framework on Lorenz system forced pendulum system and forced Duffing system. We present and discuss the prediction performance of the obtained <span style=\"color: blue; font-weight: bold\">surrogate</span> models. It is observed that the obtained model has promising performance on capturing the sensitive parameter dependence of the nonlinear dynamical behaviors even when the bifurcation occurs. We also discuss in detail the limitation of this work and potential future work. 2023 The Authorunder exclusive licence to Springer Nature B.V.</td>\n",
       "      <td>nonlinear dynamical systems,parameterized model,molecular dynamics,parameterized nonlinear dynamical system,dynamical systems,deep generative learning,parameterized,inverse problems,nonlinear equations,deep image synthesis,dynamics,<span style=\"color: blue; font-weight: bold\">deep learning</span>,boundary conditions,data visualization,visualization images,images synthesis,<span style=\"color: blue; font-weight: bold\">surrogate</span> model,data driven,parameterization,visualization,<span style=\"color: blue; font-weight: bold\">surrogate</span> modeling,design and control</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>A <span style=\"color: blue; font-weight: bold\">deep learning</span> approach for exploring the design space for the decarbonization of the canadian electricity system</td>\n",
       "      <td>Conventional energy system models have limitations in evaluating complex choices for transitioning to low-carbon energy systems and preventing catastrophic climate change. To address this challenge we propose a model that allows for the exploration of a broader design space. We develop a supervised <span style=\"color: blue; font-weight: bold\">machine learning</span> <span style=\"color: blue; font-weight: bold\">surrogate</span> of a capacity expansion model based on residual <span style=\"color: blue; font-weight: bold\">neural networks</span> that accurately approximates the modeloutputs while reducing the computation cost by five orders of magnitude. This increased efficiency enables the evaluation of the sensitivity of the outputs to the inputs providing valuable insights into system development factors for the Canadian electricity system between 2030 and 2050. To facilitate the interpretation and communication of a large number of <span style=\"color: blue; font-weight: bold\">surrogate</span> model we propose an easy-to-interpret method using an unsupervised <span style=\"color: blue; font-weight: bold\">machine learning</span> technique. Our analysis identified key factors and quantified their relationships showing that the carbon tax and wind energy capital cost are the most impactful factors on emissions in most provinces and aretotimes more impactful than other factors on the development of wind and natural gas generations nationally. Our model generates insights that deepen our understanding of the most impactful decarbonization policy interventions.</td>\n",
       "      <td>nan</td>\n",
       "      <td>Electricity; Computer science; Energy system; Carbon tax; Space (punctuation); Key (lock); Artificial intelligence; Wind power; Climate change; Energy (signal processing); Environmental economics; Machine learning; Economics; Engineering; Mathematics; Ecology; Statistics; Computer security; Electrical engineering; Biology; Operating system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>A <span style=\"color: blue; font-weight: bold\">deep learning</span> based <span style=\"color: blue; font-weight: bold\">surrogate</span> model for thermal signature prediction in laser metal deposition</td>\n",
       "      <td>Laser metal deposition LMD is an additive manufacturing method for metal parts by using focused thermal energy to fuse materials as they are deposited. During LMD transient thermal signatures such as the in-situ thermal images of melt pool contain rich information about process performance. Early prediction of such transient thermal signatures provides opportunities for process monitoring and defect prevention. While physics-based models of LMD have been conventionally used for thermal signature prediction they have limitations and are computationally expensive for real-time prediction. A scalable efficient data-science-based model is therefore needed. This paper develops a deep-learning-based <span style=\"color: blue; font-weight: bold\">surrogate</span> model called LMD-<span style=\"color: blue; font-weight: bold\">cGAN</span> to predict and emulate the transient thermal signatures in LMD. The model generates images for the thermal dynamics of melt pool conditionally on the deposition layer. It enables early prediction of future-layer thermal signatures for an in-process part based on its early-layer thermal signatures. To respect the physics in LMD a physics-guided image selection PGIS mechanism is integrated with LMD-<span style=\"color: blue; font-weight: bold\">cGAN</span> to calibrate the predictions against physical benchmarks of transient melt pool for the process. The effectiveness and efficiency of the proposed method are demonstrated in a case study on the LMD of Ti-4Al-6V thin-walled structures. Note to PractitionersWith online sensing many LMD applications have real-time process data that convey valuable information about the process status and part quality. The proposed method leverages these data for thermal signature prediction. LMD-<span style=\"color: blue; font-weight: bold\">cGAN</span> is a deep-learning-based <span style=\"color: blue; font-weight: bold\">surrogate</span> model that learns the population profile of real thermal signatures and generates thermal signatures from there. The proposed PGIS mechanism in LMD-<span style=\"color: blue; font-weight: bold\">cGAN</span> ensures the physical validity of these predictions by benchmarking them against physical insights about the process. LMD-<span style=\"color: blue; font-weight: bold\">cGAN</span> can be applied to predict thermal signatures in future layers based on early-layer thermal signatures of an in-process part an implicit assumption here is that the in-process part to be predicted for is the same typeLMD-<span style=\"color: blue; font-weight: bold\">cGAN</span> can also be applied to emulate thermal signatures in specific layers. To generate thermal signatures for generic non-defect parts the training data should be selected with cautionthe part where the data were collected should have no obvious defects so the thermal signatures generated by LMD-<span style=\"color: blue; font-weight: bold\">cGAN</span> show the regular thermal dynamics. Compared with pure physical models the proposed method incorporates process uncertainties captured from the early-layer data hence on-the-fly emulation of the melt pool while characterizing the inherent relationship between the LMD process and thermal signatures. 2004-2012 IEEE.</td>\n",
       "      <td>physics-guided image generation,laser metal deposition,<span style=\"color: blue; font-weight: bold\">computational modelling</span>,aluminum alloys,<span style=\"color: blue; font-weight: bold\">predictive models</span>,lakes,generative adversarial net,transient analysis,forecasting,ternary alloys,<span style=\"color: blue; font-weight: bold\">deep learning</span>,deposition,process monitoring,guided images,<span style=\"color: blue; font-weight: bold\">surrogate</span> model,titanium alloys,image generations,thermal signatures,physic-guided image generation.,thin walled structures,image processing,metals,melt pool,<span style=\"color: blue; font-weight: bold\">surrogate</span> modeling,thermal images</td>\n",
       "      <td>Transient (computer programming); Artificial intelligence; Thermal; Process (computing); Computer science; Signature (topology); Scalability; Materials science; Machine learning; Layer (electronics); Deep learning; Fuse (electrical); Deposition (geology); Nanotechnology; Engineering; Physics; Database; Mathematics; Paleontology; Geometry; Sediment; Meteorology; Electrical engineering; Biology; Operating system</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>A deep <span style=\"color: blue; font-weight: bold\">neural network</span> multi fidelity <span style=\"color: blue; font-weight: bold\">surrogate</span> model approach for bayesian model updating in SHM</td>\n",
       "      <td>This paper presents a methodology to move toward reliable real-time structural health monitoring SHMThe proposed procedure relies upon <span style=\"color: blue; font-weight: bold\">surrogate</span> modeling based on a multi-fidelity MF deep <span style=\"color: blue; font-weight: bold\">neural network</span> DNN conceived to map damage and operational parameters onto sensor recordings. Within a stochastic framework the MF-DNN is adopted by a Markov chain Monte Carlo sampling procedure to update the probability distribution of the structural state conditioned on noisy observations. The MF-DNN enables to locate and possibly quantify the presence of damage and its multi-fidelity configuration effectively blends datasets featuring different fidelities without any prior assumption. The training datasets are generated with physics-based models of the monitored structure high fidelity HF and low fidelity LF models are considered to simulate the structural response under varying operational conditions respectively in the presence or absence of a structural damage. The MF-DNN is a composition of a fully-connected LF-DNN which mimics sensor recordings in the undamaged condition and of a <span style=\"color: blue; font-weight: bold\">long short-term memory</span> HF-DNN which is exploited to enrich the LF approximation for the considered damaged scenarios. By framing the model updating strategy as an incremental or residual modeling problem the MF-DNN is reported to provide numerous advantages over single-fidelity based models for SHM purposes. 2023 The Authorunder exclusive license to Springer Nature Switzerland AG.</td>\n",
       "      <td>monte carlo methods,multi-fidelity method,<span style=\"color: blue; font-weight: bold\">bayesian networks</span>,multi-fidelity methods,real-time damage identification,reduced order modelling,real- time,deep <span style=\"color: blue; font-weight: bold\">neural networks</span>,reduced-order model,stochastic systems,damage identification,<span style=\"color: blue; font-weight: bold\">deep learning</span>,markov chain monte carlo,probability distributions,multi fidelities,damage detection,markov chain monte-carlo,reduced-order modeling,markov processes,bayesian model updating,fidelity methods,structural health monitoring</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>A fast data driven optimization method of multi area combined economic emission dispatch</td>\n",
       "      <td>To provide more feasible schemes for power system optimization management and operation the interconnection of power grids in different areas is inevitable. Naturally the multi-area combined economic emission dispatch MACEED problem becomes a more important decision problem. However as the dimensions of MACEED problems increase existing studies may not obtain feasible scheduling decisions in a suitable time. On this MACEED problems are transferred into computational expensive problems. In order to solve high-dimensional large-scale MACEED problems a data-driven <span style=\"color: blue; font-weight: bold\">surrogate</span>-assisted approach is proposed. First a feature engineering-based <span style=\"color: blue; font-weight: bold\">support vector regression</span> <span style=\"color: blue; font-weight: bold\">surrogate</span> model is proposed to replace the traditional functions in high-dimensional MACEED problems. Second knowledge distillation is applied as a freezing and fine-tuning mechanism for the improved <span style=\"color: blue; font-weight: bold\">support vector regression</span> <span style=\"color: blue; font-weight: bold\">surrogate</span> models which significantly reduces the time required to build <span style=\"color: blue; font-weight: bold\">surrogate</span> models. Third an improved non-dominated sorting <span style=\"color: blue; font-weight: bold\">genetic algorithm</span>-III is proposed to obtain feasible solutions to high-dimensional MACEED problem. The proposed algorithm enhances the convergence and diversity of optimal solutions. The effectiveness of the proposed data-driven approach is demonstrated through <span style=\"color: blue; font-weight: bold\">simulations</span> of a four-area 40-unit test system under different constraints. 2023 Elsevier Ltd</td>\n",
       "      <td>electric power system interconnection,emission dispatch problem,multi areas,large-scale power system,large-scale power systems,<span style=\"color: blue; font-weight: bold\">genetic algorithms</span>,emission inventory,convergence,distillation,regression analysis,multi-area combined economic emission dispatch,<span style=\"color: blue; font-weight: bold\">surrogate</span> model,electric load dispatching,<span style=\"color: blue; font-weight: bold\">genetic algorithm</span>,data driven,freezing,optimization,combined economic emission dispatch,higher-dimensional,<span style=\"color: blue; font-weight: bold\">surrogate</span> modeling,knowledge distillation,high-dimensional</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>A framework for <span style=\"color: red; font-weight: bold\">physics informed</span> <span style=\"color: blue; font-weight: bold\">deep learning</span> over freeform domains</td>\n",
       "      <td><span style=\"color: blue; font-weight: bold\">Deep learning</span> is a popular approach for approximating the solutions to <span style=\"color: blue; font-weight: bold\">partial differential equations</span> <span style=\"color: blue; font-weight: bold\">PDEs</span> over different material parameters and boundary conditions. However no work has yet been reported on learning <span style=\"color: blue; font-weight: bold\">PDE</span> solutions over changing shapes of the underlying domain. We present a framework to train <span style=\"color: blue; font-weight: bold\">neural networks</span> NN and <span style=\"color: blue; font-weight: bold\">physics-informed</span> <span style=\"color: blue; font-weight: bold\">neural networks</span> <span style=\"color: blue; font-weight: bold\">PINNs</span> to learn the solutions to <span style=\"color: blue; font-weight: bold\">PDEs</span> defined over varying freeform domains. This is made possible through our adoption of a parametric non-uniform rational B-Spline NURBS representation of the underlying physical shape. Distinct physical domains are mapped to a common parametric space via NURBS parameterization. In our approach we formulate NNs and <span style=\"color: blue; font-weight: bold\">PINNs</span> that learn the solutions to <span style=\"color: blue; font-weight: bold\">PDEs</span> as a function of the shape of the domain itself through shape parameters. Under this formulation the loss function is based on an unchanging parametric domain that maps to a variable physical domain. Residual computation in <span style=\"color: blue; font-weight: bold\">PINNs</span> is made possible through the Jacobian of the mapping. Numerical show that our networks can be trained to predict the solutions to a <span style=\"color: blue; font-weight: bold\">PDE</span> defined over an entire set of shapes. We focus on the linear elasticity <span style=\"color: blue; font-weight: bold\">PDE</span> and show how we can build a <span style=\"color: blue; font-weight: bold\">surrogate</span> model that is able to predict displacements and stresses over a variety of freeform domains. To assess the efficacy of all networks in this work data efficiency network accuracy and the capacity of networks to extrapolate are considered and compared between NNs and <span style=\"color: blue; font-weight: bold\">PINNs</span>. The comparison includes cases where little training data is available. <span style=\"color: blue; font-weight: bold\">Transfer learning</span> and applications to shape optimization are analyzed as well. A selection of the used codes and datasets is provided at https github.com fmezzadri shape parameterized.git. 2023 Elsevier Ltd</td>\n",
       "      <td>materials parameters,<span style=\"color: blue; font-weight: bold\">physics-informed</span> <span style=\"color: blue; font-weight: bold\">deep learning</span>,nurbs,shape optimization,freeforms,equation solution,neural-networks,<span style=\"color: blue; font-weight: bold\">deep learning</span>,boundary conditions,computer-aided design,computer aided design,physic-informed <span style=\"color: blue; font-weight: bold\">deep learning</span>,physical domain,material boundaries,<span style=\"color: blue; font-weight: bold\">neural network</span>,<span style=\"color: blue; font-weight: bold\">partial differential equations</span>,non-uniform rational b-splines,interpolation,learn+</td>\n",
       "      <td>Data science; Mathematics education; Computer science; Physics; Artificial intelligence; Psychology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>A <span style=\"color: blue; font-weight: bold\">gan</span> based <span style=\"color: blue; font-weight: bold\">surrogate</span> model for instantaneous urban wind flow prediction</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>Surrogate model; Flow (mathematics); Environmental science; Econometrics; Business; Meteorology; Computer science; Geography; Economics; Mechanics; Physics; Machine learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>A global local attention network for uncertainty analysis of ground penetrating radar modeling</td>\n",
       "      <td>A global local attention-based feature reconstruction GLAFR <span style=\"color: blue; font-weight: bold\">surrogate</span> model is proposed for uncertainty analysis UA in ground penetrating radar GPR <span style=\"color: blue; font-weight: bold\">simulation</span>. The uncertain inputs are converted to electric fields by the <span style=\"color: blue; font-weight: bold\">surrogate</span> model instead of the full-wave <span style=\"color: blue; font-weight: bold\">simulation</span> and the uncertainty of output is quantified effectively. In the model the global feature scaling GFS and the local feature reconstruction LFR are employed to obtain the long-term and short-term relationships of features. In addition a new loss function is proposed to accelerate the convergence of the model for training data with a wider range of input disturbances. The validity of the <span style=\"color: blue; font-weight: bold\">surrogate</span> model is verified by the UA result from the Monte Carlo method MCMCompared with existing <span style=\"color: blue; font-weight: bold\">deep learning</span> methods the proposed approach can efficiently get higher quality predictions. Meanwhile the Sobol indices evaluated by GLAFR are in agreement with those of MCM which requires running the full-wave <span style=\"color: blue; font-weight: bold\">simulation</span> one thousand times to converge. 2023 Elsevier Ltd</td>\n",
       "      <td>full-wave <span style=\"color: blue; font-weight: bold\">simulations</span>,monte carlo methods,montecarlo methods,attention mechanisms,global-local,multi-scales,uncertainty analysis,feature reconstruction,ground penetrating radar,radar modeling,electric fields,ground penetrating radar systems,geological surveys,attention mechanism,geophysical prospecting,<span style=\"color: blue; font-weight: bold\">deep learning</span>,<span style=\"color: blue; font-weight: bold\">surrogate</span> modeling,multi-scale</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>A <span style=\"color: blue; font-weight: bold\">machine learning</span> approach for calibrating ABL profiles in large eddy <span style=\"color: blue; font-weight: bold\">simulations</span></td>\n",
       "      <td>Atmospheric boundary layer ABL inhomogeneity is a common problem encountered in computational wind engineering CWE where inflow profiles experience unintended flow adaptation while travelling from the inlet boundary to the location of interest inside the domain. This causes the region of interest to experience flow conditions different from those intended by the modeller thereby introducing error in the <span style=\"color: blue; font-weight: bold\">simulation</span>. Solutions to ABL inhomogeneity have been proposed for RANS models but this issue remains problematic in scale-resolving <span style=\"color: blue; font-weight: bold\">simulations</span>. In this study a <span style=\"color: blue; font-weight: bold\">machine learning</span> ML approach is proposed for calibrating ABL profiles to achieve target flow properties in large-eddy <span style=\"color: blue; font-weight: bold\">simulations</span> LESThe proposed method is demonstrated for ABL flow over a suburban terrain based on AS 1170.2 leading to a considerable reduction of inhomogeneity error from 49.6 to 4.6Sensitivity studies are also presented to investigate the influence of numerical parameters on profile calibration. While the proposed approach does not resolve underlying theoretical limitations resulting in ABL inhomogeneity it provides a practical solution for achieving target ABL profiles which can help improve confidence in the reliability of LES for wind engineering applications. 2022 Elsevier Ltd</td>\n",
       "      <td>machine-learning,<span style=\"color: blue; font-weight: bold\">machine learning</span>,inlet profiles,large-eddy <span style=\"color: blue; font-weight: bold\">simulation</span> (les),synthetic <span style=\"color: blue; font-weight: bold\">turbulence</span> generator,artificial <span style=\"color: blue; font-weight: bold\">neural network</span>,<span style=\"color: blue; font-weight: bold\">neural networks</span>,large eddy <span style=\"color: blue; font-weight: bold\">simulation</span>,atmospheric boundary layer,atmospheric boundary layer (abl),artificial <span style=\"color: blue; font-weight: bold\">neural network</span> (<span style=\"color: blue; font-weight: bold\">ann</span>),inlet profile,openfoam,large-eddy <span style=\"color: blue; font-weight: bold\">simulation</span>,<span style=\"color: blue; font-weight: bold\">computational fluid</span> dynamics (cfd),<span style=\"color: blue; font-weight: bold\">machine learning</span> (ml),<span style=\"color: blue; font-weight: bold\">surrogate</span> model,<span style=\"color: blue; font-weight: bold\">turbulence</span> generators,atmospheric movements,large-eddy <span style=\"color: blue; font-weight: bold\">simulations</span>,<span style=\"color: blue; font-weight: bold\">computational fluid</span> dynamics,image segmentation,<span style=\"color: blue; font-weight: bold\">surrogate</span> modeling,calibration,<span style=\"color: blue; font-weight: bold\">computational fluid</span> dynamic</td>\n",
       "      <td>Computer science; Large eddy simulation; Eddy current; Physics; Engineering; Mechanics; Electrical engineering; Turbulence</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving file scopus_lens_ml_sim_engineering_all_highlights_plus_abstract_topics.xlsx ...\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "# filter_words = \"Tmachine learning and (cardi or heart or vent)\"  # Filter: T = Title, A = Abstract, K = keywords; TA = occurs in title or keywords etc.; no T,A,K == TAK\n",
    "\n",
    "# Datasets\n",
    "# - ML or PINN\n",
    "# filter_words = '''\n",
    "# \"neural network\" OR \"reinforcement learning\" OR \"machine learning\" OR \"deep learning\" OR \"transformer model\" OR \"BERT\" OR \"GPT\" OR \"adversarial network\" OR \"gan\" OR \"natural language processing\" OR \"word embedding\" OR \"document embedding\" OR \"sentence embedding\" OR \"transfer learning\" OR \"ensemble learning\" OR \"learning algorithm\" OR \"genetic algorithm\" OR \"evolutionary algorithm\" OR \"support vector machine\" OR \"decision tree\" OR \"bayesian network\" OR \"q-learning\" OR \"long short-term memory\" OR \"classification model\" OR \"classification algorithm\" OR \"ann\" OR \"clustering algorithm\" OR \"feature extraction\" OR \"anomaly detection\" OR \"inference engine\" OR \"k nearest neighbour\" OR \"cluster analysis\" OR \"linear regression\" OR \"hidden markov\" OR perceptron OR \"random forest\" OR \"support vector regression\" OR cnn OR rnn OR \"predictive model\" OR \"logistic regression\" OR \"statistical learning\" OR lstm OR TITLE-ABS-KEY( \"neural differential\" OR \"neural ordinary\" OR \"neural ODE\" OR \"data-driven model\" OR \"physics-informed\" OR \"physics-constrained\" OR \"physics-embedded\" OR \"physics-inspired\" OR \"physics-aware\" OR \"physics-enhanced\" OR \"hidden physics\" OR \"differentiable physics\" OR \"scientific machine learning\" OR \"physics machine learning\")\n",
    "# '''\n",
    "\n",
    "# Datasets\n",
    "# - ML in Medicine\n",
    "# filter_words = '''\n",
    "# (title:(\"neural network\" OR \"reinforcement learning\" OR \"machine learning\" OR \"deep learning\" OR \"transformer model\" OR \"BERT\" OR \"GPT\" OR \"adversarial network\" OR \"gan\" OR \"natural language processing\" OR \"word embedding\" OR \"document embedding\" OR \"sentence embedding\" OR \"transfer learning\" OR \"ensemble learning\" OR \"learning algorithm\" OR \"genetic algorithm\" OR \"evolutionary algorithm\" OR \"support vector machine\" OR \"decision tree\" OR \"q-learning\" OR \"long short-term memory\" OR \"classification model\" OR \"classification algorithm\" OR \"ann\" OR \"clustering algorithm\" OR \"feature extraction\" OR \"anomaly detection\" OR \"inference engine\" OR \"k nearest neighbour\" OR \"cluster analysis\" OR \"hidden markov\" OR perceptron OR \"random forest\" OR \"support vector regression\" OR cnn OR rnn OR \"predictive model\" OR \"statistical learning\" OR lstm) OR title:(\"neural differential\" OR \"neural ordinary\" OR \"neural ODE\" OR \"data-driven model\" OR \"physics-informed\" OR \"physics-constrained\" OR \"physics-embedded\" OR \"physics-inspired\" OR \"physics-aware\" OR \"physics-enhanced\" OR \"hidden physics\" OR \"differentiable physics\" OR \"scientific machine learning\" OR \"physics machine learning\" ))\n",
    "# '''\n",
    "\n",
    "# SIM + HEART => for highlighting only\n",
    "# filter_words = '''\n",
    "# simulation  OR  \"numerical method\"  OR  \"numerical model\"  OR  \"navier stokes\"  OR  \"system dynamics\"  OR  \"numerical experiment\"  OR  fem  OR  turbulence  OR  \"numerical analysis\"  OR  \"multiagent\"  OR  \"multi-agent\"  OR  \"surrogate\"  OR  pde  OR  \"partial differential equation\"  OR  \"computational fluid\"  OR  \"computational model*\"  OR  \"computational method*\"  OR  \"computational framework\"  OR  \"computational approach\"  OR  \"computational experiment\"  OR  \"computational mechanic*\"  OR  \"computational technique\"  OR  \"computational study\"  OR  \"computational analysis\"  OR  \"computational science\"  OR  \"computational electro*\"  OR  \"computational material\"  OR  \"computational biomech*\"  OR  \"computational physics\"  OR  \"computational research\"  OR  \"computational engineering\"  OR  \"finite element\"  OR  \"finite difference\"  OR  \"finite volume\"  OR  \"boundary element method\"  OR  \"discrete element method\"  OR  \"meshfree method\"  OR  \"mesh free method\"  OR  \"meshless method\"  OR  \"particle hydrodynamics\"  OR  \"dissipative particle dynamics\"  OR  \"particle method\" OR *cardi*  OR  heart  OR  *ventric*  OR  coronary  OR  *atria*  OR  atrium\n",
    "# '''\n",
    "\n",
    "# ML + SIM (Engineering, Medicine)\n",
    "highlight_words = '''\n",
    "    \"neural network\" OR \"reinforcement learning\" OR \"machine learning\" OR \"deep learning\" OR \"transformer model\" OR \"BERT\" OR \"GPT\" OR \"adversarial network\" OR \"gan\" OR \"natural language processing\" OR \"word embedding\" OR \"document embedding\" OR \"sentence embedding\" OR \"transfer learning\" OR \"ensemble learning\" OR \"learning algorithm\" OR \"genetic algorithm\" OR \"evolutionary algorithm\" OR \"support vector machine\" OR \"decision tree\" OR \"bayesian network\" OR \"q-learning\" OR \"long short-term memory\" OR \"classification model\" OR \"classification algorithm\" OR \"ann\" OR \"clustering algorithm\" OR \"feature extraction\" OR \"anomaly detection\" OR \"inference engine\" OR \"k nearest neighbour\" OR \"cluster analysis\" OR \"linear regression\" OR \"hidden markov\" OR perceptron OR \"random forest\" OR \"support vector regression\" OR cnn OR rnn OR \"predictive model\" OR \"logistic regression\" OR \"statistical learning\" OR lstm OR \"neural differential\" OR \"neural ordinary\" OR \"neural ODE\" OR \"data-driven model\" OR \"physics-informed\" OR \"physics-constrained\" OR \"physics-embedded\" OR \"physics-inspired\" OR \"physics-aware\" OR \"physics-enhanced\" OR \"hidden physics\" OR \"differentiable physics\" OR \"scientific machine learning\" OR \"physics machine learning\" OR simulation  OR  \"numerical method\"  OR  \"numerical model\"  OR  \"navier stokes\"  OR  \"system dynamics\"  OR  \"numerical experiment\"  OR  fem  OR  turbulence  OR  \"numerical analysis\"  OR  \"multiagent\"  OR  \"multi-agent\"  OR  \"surrogate\"  OR  pde  OR  \"partial differential equation\"  OR  \"computational fluid\"  OR  \"computational model*\"  OR  \"computational method*\"  OR  \"computational framework\"  OR  \"computational approach\"  OR  \"computational experiment\"  OR  \"computational mechanic*\"  OR  \"computational technique\"  OR  \"computational study\"  OR  \"computational analysis\"  OR  \"computational science\"  OR  \"computational electro*\"  OR  \"computational material\"  OR  \"computational biomech*\"  OR  \"computational physics\"  OR  \"computational research\"  OR  \"computational engineering\"  OR  \"finite element\"  OR  \"finite difference\"  OR  \"finite volume\"  OR  \"boundary element method\"  OR  \"discrete element method\"  OR  \"meshfree method\"  OR  \"mesh free method\"  OR  \"meshless method\"  OR  \"particle hydrodynamics\"  OR  \"dissipative particle dynamics\"  OR  \"particle method\" OR pinn\n",
    "'''\n",
    "\n",
    "# ML + SIM (Heart)\n",
    "# highlight_words = '''\n",
    "#     \"neural network\" OR \"reinforcement learning\" OR \"machine learning\" OR \"deep learning\" OR \"transformer model\" OR \"BERT\" OR \"GPT\" OR \"adversarial network\" OR \"gan\" OR \"natural language processing\" OR \"word embedding\" OR \"document embedding\" OR \"sentence embedding\" OR \"transfer learning\" OR \"ensemble learning\" OR \"learning algorithm\" OR \"genetic algorithm\" OR \"evolutionary algorithm\" OR \"support vector machine\" OR \"decision tree\" OR \"bayesian network\" OR \"q-learning\" OR \"long short-term memory\" OR \"classification model\" OR \"classification algorithm\" OR \"ann\" OR \"clustering algorithm\" OR \"feature extraction\" OR \"anomaly detection\" OR \"inference engine\" OR \"k nearest neighbour\" OR \"cluster analysis\" OR \"linear regression\" OR \"hidden markov\" OR perceptron OR \"random forest\" OR \"support vector regression\" OR cnn OR rnn OR \"predictive model\" OR \"logistic regression\" OR \"statistical learning\" OR lstm OR \"neural differential\" OR \"neural ordinary\" OR \"neural ODE\" OR \"data-driven model\" OR \"physics-informed\" OR \"physics-constrained\" OR \"physics-embedded\" OR \"physics-inspired\" OR \"physics-aware\" OR \"physics-enhanced\" OR \"hidden physics\" OR \"differentiable physics\" OR \"scientific machine learning\" OR \"physics machine learning\" OR simulation  OR  \"numerical method\"  OR  \"numerical model\"  OR  \"navier stokes\"  OR  \"system dynamics\"  OR  \"numerical experiment\"  OR  fem  OR  turbulence  OR  \"numerical analysis\"  OR  \"multiagent\"  OR  \"multi-agent\"  OR  \"surrogate\"  OR  pde  OR  \"partial differential equation\"  OR  \"computational fluid\"  OR  \"computational model*\"  OR  \"computational method*\"  OR  \"computational framework\"  OR  \"computational approach\"  OR  \"computational experiment\"  OR  \"computational mechanic*\"  OR  \"computational technique\"  OR  \"computational study\"  OR  \"computational analysis\"  OR  \"computational science\"  OR  \"computational electro*\"  OR  \"computational material\"  OR  \"computational biomech*\"  OR  \"computational physics\"  OR  \"computational research\"  OR  \"computational engineering\"  OR  \"finite element\"  OR  \"finite difference\"  OR  \"finite volume\"  OR  \"boundary element method\"  OR  \"discrete element method\"  OR  \"meshfree method\"  OR  \"mesh free method\"  OR  \"meshless method\"  OR  \"particle hydrodynamics\"  OR  \"dissipative particle dynamics\"  OR  \"particle method\" OR pinn OR *cardi*  OR  heart  OR  *ventric*  OR  coronary  OR  *atria*  OR  atrium\n",
    "# '''\n",
    "\n",
    "# filter_words = '''\n",
    "#     \"physics-informed\" OR \"physics-constrained\" OR \"physics-embedded\" OR \"physics-inspired\" OR \"physics-aware\" OR \"physics-enhanced\" OR \"hidden physics\" OR \"differentiable physics\" OR \"scientific machine learning\" OR \"physics machine learning\" OR \"inverse model\" OR pinn\n",
    "# '''\n",
    "\n",
    "filter_words = \"surrogate model\"\n",
    "\n",
    "# filter_acronyms = ['PINN', 'GAN', 'ANN', 'BERT']\n",
    "filter_acronyms = []\n",
    "\n",
    "filter_fos = \"\"\n",
    "\n",
    "highlights_only = False      # skip the filtering; useful if the dataset is very large and already filtered\n",
    "highlight_partial = True    # highlight partial matches too\n",
    "\n",
    "display_html = True\n",
    "max_html_rows = 20\n",
    "\n",
    "data_dir = '/results/'\n",
    "results_dir = '/results/'\n",
    "\n",
    "data_src = 'csv_load'  # data sources: 'csv_load', 'biblio_df', 'csv_loaded'\n",
    "read_biblio_csv = 'scopus_lens_ml_sim_engineering_all_st_plus_abstract_topics'\n",
    "\n",
    "write_biblio_highlights_file = 'scopus_lens_ml_sim_engineering_all_highlights_plus_abstract_topics'\n",
    "\n",
    "write_csv = False  # just the filtered publications without highlights; use e.g. for merging with other datasets\n",
    "write_html = False\n",
    "write_xlsx = True\n",
    "\n",
    "# --------------------------------------------\n",
    "\n",
    "copy_of_biblio_highlights_df = pd.DataFrame()\n",
    "\n",
    "if data_src == 'csv_load':  # for large or datasets further processed elsewhere, we read the bibliographic dataset from file\n",
    "\n",
    "    print(f'\\nReading the input file {read_biblio_csv}.csv ...')\n",
    "\n",
    "    biblio_highlights_df = pd.read_csv(root_dir + results_dir + read_biblio_csv + '.csv') #.head(100)\n",
    "\n",
    "    # Convert the search_title and search_abs into lists\n",
    "    biblio_highlights_df['search_title'] = biblio_highlights_df['search_title'] \\\n",
    "                                           .apply(lambda x: [i.strip().strip(\"'\") \\\n",
    "                                            for i in x.strip('[]').split(',')])\n",
    "    biblio_highlights_df['search_abs'] = biblio_highlights_df['search_abs'] \\\n",
    "                                           .apply(lambda x: [i.strip().strip(\"'\") \\\n",
    "                                            for i in x.strip('[]').split(',')])\n",
    "    copy_of_biblio_highlights_df = biblio_highlights_df.copy()\n",
    "\n",
    "elif data_src == 'biblio_df':   # use the biblio_df dataset from the previous cells\n",
    "\n",
    "    biblio_highlights_df = biblio_df.copy()\n",
    "\n",
    "elif data_src == 'csv_loaded': # for large datasets, use biblio_highlights_df if the CSV file was previously loaded\n",
    "\n",
    "    biblio_highlights_df = copy_of_biblio_highlights_df.copy()\n",
    "else:\n",
    "    raise Exception(f'The variable data_src cannot have the value \"{data_src}\"')\n",
    "\n",
    "n_pubs = len(biblio_highlights_df)\n",
    "\n",
    "def highlight_selected_text(text, search_terms, color, i_row):\n",
    "    text = str(text)\n",
    "    \n",
    "    if(i_row % 100 == 0): # print row index\n",
    "        print(f'{i_row}', end = '\\r')\n",
    "\n",
    "    if len(search_terms) == 0:\n",
    "        return text\n",
    "    \n",
    "    if (len(search_terms) == 1) and (search_terms[0] == \"\"):\n",
    "        return text\n",
    "\n",
    "    for k in search_terms:\n",
    "        if highlight_partial:\n",
    "            pattern = r\"\\b\\w*{}+\\w*\\b\".format(k)\n",
    "            text = re.sub(pattern, lambda match: f'<span style=\"color: {color}; font-weight: bold\">{match.group()}</span>', text, flags = re.IGNORECASE)\n",
    "        else:\n",
    "            text = re.sub(r\"(?i)\\b\"+k+r\"[\\w-]*\", lambda match: f'<span style=\"color: {color}; font-weight: bold\">{match.group()}</span>', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def explode_filter_kws(kw):\n",
    "    match = re.match(r'^([A-Z]{0,3})([^A-Z]+)$', kw)\n",
    "    kw_stripped = kw.lstrip('TAK')\n",
    "    kw_expl = kw\n",
    "\n",
    "    if match:\n",
    "        if len(match.group(1)) == 0:\n",
    "            kw_expl = '(T' + kw_stripped + ' or A' + kw_stripped + ' or K' + kw_stripped + ')'\n",
    "        elif len(match.group(1)) == 1:\n",
    "            kw_expl = kw\n",
    "        elif len(match.group(1)) == 2:\n",
    "            kw_expl = '(' + match.group(1)[0] + kw_stripped + ' or ' + match.group(1)[1] + kw_stripped + ')'\n",
    "        elif len(match.group(1)) == 3:\n",
    "            kw_expl = '(' + match.group(1)[0] + kw_stripped + ' or ' + match.group(1)[1] + kw_stripped + ' or ' + match.group(1)[2] + kw_stripped + ')'\n",
    "\n",
    "    return kw_expl\n",
    "\n",
    "# Expand the search terms in the boolean filter string that are not prepended with T/A/K\n",
    "def expand_search_terms(my_filter):\n",
    "    new_filter_as_list = []\n",
    "    my_filter = my_filter.replace('\"', '').replace(\"'\", '').replace(\"*\", '')\n",
    "    filter_as_list = re.split(r'(\\(|\\)|\\b(?:and|or|AND|OR)\\b)', my_filter)   # extract all the words used in the filter\n",
    "    filter_as_list = [x.strip() for x in filter_as_list if x.strip()]\n",
    "\n",
    "    i = 0\n",
    "    comp_str = \"\"\n",
    "\n",
    "    while i < len(filter_as_list):\n",
    "\n",
    "        if filter_as_list[i] not in ['or', 'and', 'OR', 'AND', '(', ')']:\n",
    "            if comp_str == \"\":\n",
    "                comp_str = filter_as_list[i]\n",
    "            else:\n",
    "                comp_str += ' ' + filter_as_list[i]\n",
    "            i += 1\n",
    "            \n",
    "            if i == len(filter_as_list):\n",
    "                new_filter_as_list.append(comp_str)\n",
    "\n",
    "        elif comp_str == \"\":\n",
    "            new_filter_as_list.append(filter_as_list[i])\n",
    "            i += 1\n",
    "        else:\n",
    "            new_filter_as_list.append(comp_str)\n",
    "            new_filter_as_list.append(filter_as_list[i])\n",
    "            comp_str = \"\"\n",
    "            i += 1\n",
    "\n",
    "    return(new_filter_as_list)\n",
    "\n",
    "# Strings to remove from filter_words and highlight_words (this allows direct copying of search terms from Scopus and Lens)\n",
    "strings_to_remove = ['TITLE-ABS-KEY', 'TITLE', 'TITLE-ABS', 'KEY','title:', 'abstract:', 'keyword:']\n",
    "\n",
    "# biblio_highlights_df = biblio_highlights_df.head(100)\n",
    "# display(biblio_highlights_df)\n",
    "\n",
    "# Filter the titles, abstracts, and/or keywords with filter_words\n",
    "if not highlights_only:\n",
    "\n",
    "    for string_to_remove in strings_to_remove:\n",
    "        filter_words = filter_words.replace(string_to_remove, '')\n",
    "\n",
    "    # Remove quotes from search term\n",
    "    filter_words = re.sub(r'[\\\"\\']', '', filter_words)\n",
    "\n",
    "    # Parse the filter and expand unlabeled (T,A,K) terms to include all labels T, K, A\n",
    "    filter_exp_list = expand_search_terms(filter_words)\n",
    "    filter_exp = ' '.join([explode_filter_kws(x) if x not in ['(', ')', 'and', 'or', 'AND', 'OR'] else x.lower() for x in filter_exp_list])\n",
    "\n",
    "    # Filter the dataframe using the filter provided above\n",
    "    def evaluate_expression(row, filter):\n",
    "\n",
    "        if(row.name % 100 == 0): # print row index\n",
    "            print(f'{row.name}', end = '\\r')\n",
    "\n",
    "        # Apply the Field of Study filter first (single keyword only at this moment)\n",
    "        if filter_fos != \"\":\n",
    "\n",
    "            fos_list = str(row['fos']).split(';')\n",
    "\n",
    "            if not any(filter_fos.lower() in s.lower().strip() for s in fos_list):\n",
    "                return False\n",
    "            \n",
    "            if filter == \"\":\n",
    "                return True\n",
    "\n",
    "        title = str(row['title'])\n",
    "        abs = str(row['abstract'])\n",
    "        kws = str(row['kws'])\n",
    "\n",
    "        words_all = expand_search_terms(filter)\n",
    "        words = [x for x in words_all if x not in ['(', ')', 'and', 'or']]  # remove the parantheses, 'and' and 'or'\n",
    "\n",
    "        # Prepend TAK to filter keywords that do not start with T, A, or K\n",
    "        words = [\"TAK\" + string if string[0] not in ['T', 'A', 'K'] else string for string in words]\n",
    "        \n",
    "        words_T, words_A, words_K = [], [], []\n",
    "        values_T, values_A, values_K = {}, {}, {}\n",
    "\n",
    "        try:\n",
    "            words_T = [string.lstrip('TAK') for string in words if string[:3].count('T') > 0]\n",
    "            # values_T = {'T' + word: word.lower() in title.lower() for word in words_T}\n",
    "            values_T = {'T' + word: word.upper() in title if word.upper() in filter_acronyms else word.lower() in title.lower() for word in words_T}\n",
    "\n",
    "            words_A = [string.lstrip('TAK') for string in words if string[:3].count('A') > 0]\n",
    "            # values_A = {'A' + word: word.lower() in abs.lower() for word in words_A}\n",
    "            values_A = {'A' + word: word.upper() in abs if word.upper() in filter_acronyms else word.lower() in abs.lower() for word in words_A}\n",
    "\n",
    "            words_K = [string.lstrip('TAK') for string in words if string[:3].count('K') > 0]\n",
    "            # values_K = {'K' + word: word.lower() in kws.lower() for word in words_K}\n",
    "            values_K = {'K' + word: word.upper() in kws if word.upper() in filter_acronyms else word.lower() in kws.lower() for word in words_K}\n",
    "        except AttributeError as e:\n",
    "            print(f\"Error: {e}. Row has values {title}\\n{abs}\\n{kws}\")\n",
    "        \n",
    "        words_all = list(set(words_T + words_A + words_K))\n",
    "\n",
    "        values = values_T.copy()\n",
    "        values.update(values_A)\n",
    "        values.update(values_K)\n",
    "\n",
    "        for key in values:\n",
    "            filter = filter.replace(key, str(values[key]))\n",
    "\n",
    "        return eval(filter)\n",
    "\n",
    "    print(f'Filtering the dataframe...')\n",
    "\n",
    "    # Apply the keyword filter\n",
    "    if filter_exp != \"\" or filter_fos != \"\":\n",
    "        biblio_highlights_df = biblio_highlights_df[biblio_highlights_df.apply(evaluate_expression, filter = filter_exp, axis = 1)].copy()\n",
    "\n",
    "    print(f\"\\n{len(biblio_highlights_df)} filter matches of a total of {n_pubs} publications\")\n",
    "\n",
    "# Remove quotes, parantheses, stars, boolean operators, and special search operators from highlight_words\n",
    "for string_to_remove in strings_to_remove:\n",
    "    highlight_words = highlight_words.replace(string_to_remove, '')\n",
    "\n",
    "highlight_words = re.sub(r'[\\\"\\']', '', highlight_words)\n",
    "highlight_words = highlight_words.replace('\"', '').replace(\"'\", '').replace(\"*\", '').lower()\n",
    "highlights_as_list = re.split(r'(\\(|\\)|\\b(?:and|or)\\b)', highlight_words)   # extract all the words used in the filter\n",
    "highlights_as_list = [x.strip() for x in highlights_as_list if x.strip()]   # remove leading and trailing whitespaces\n",
    "\n",
    "# All words in highlight_words\n",
    "# words_filter_all = expand_search_terms(highlight_words)  # extract all the words used in the filter\n",
    "# words_filter_all = [x.lstrip('TAK') for x in words_filter_all if x not in ['(', ')', 'and', 'or']]  # remove the parantheses, 'and' and 'or'\n",
    "# words_filter_all = list(set(words_filter_all))\n",
    "highlights_as_list = [x.strip() for x in highlights_as_list if x not in ['(', ')', 'and', 'or']]  # remove the parantheses, 'and' and 'or'\n",
    "highlights_as_list = list(set(highlights_as_list))\n",
    "\n",
    "# Words in the search term that are not in the filter (to avoid nested <span> tags; the Excel creator function below doesn't like it)\n",
    "words_st = set(x for lst in biblio_highlights_df['search_title'] + biblio_highlights_df['search_abs'] for x in lst)\n",
    "words_st = {x.lower() for x in words_st}\n",
    "words_diff_st = list(words_st - set(highlights_as_list))\n",
    "words_diff_st = [x for x in words_diff_st if x]     # remove empty list items\n",
    "\n",
    "# biblio_highlights_df = biblio_highlights_df.head(10)\n",
    "\n",
    "# Highlight the keywords used in the search terms\n",
    "if not biblio_highlights_df.empty:\n",
    "    biblio_no_highlights_df = pd.DataFrame()\n",
    "\n",
    "    if write_csv:\n",
    "        biblio_no_highlights_df = biblio_highlights_df.copy()   # filtered dataset without the highlight markup\n",
    "\n",
    "    # Highlight the filter keywords\n",
    "    print(f'Highlighting the filter keywords in the titles...')\n",
    "    biblio_highlights_df['title'] = biblio_highlights_df.apply(lambda x: highlight_selected_text(x['title'], highlights_as_list, 'blue', biblio_highlights_df.index.get_loc(x.name)), axis = 1)\n",
    "    print(f'\\nHighlighting the filter keywords in the abstracts...')\n",
    "    biblio_highlights_df['abstract'] = biblio_highlights_df.apply(lambda x: highlight_selected_text(x['abstract'], highlights_as_list, 'blue', biblio_highlights_df.index.get_loc(x.name)), axis = 1)\n",
    "    print(f'\\nHighlighting the filter keywords in the keywords...')\n",
    "    biblio_highlights_df['kws'] = biblio_highlights_df.apply(lambda x: highlight_selected_text(x['kws'], highlights_as_list, 'blue', biblio_highlights_df.index.get_loc(x.name)), axis = 1)\n",
    "    \n",
    "    # Highlight the original search terms\n",
    "    print(f'\\nHighlighting the original search terms in the titles...')\n",
    "    biblio_highlights_df['title'] = biblio_highlights_df.apply(lambda x: highlight_selected_text(x['title'], words_diff_st, 'red', biblio_highlights_df.index.get_loc(x.name)), axis = 1)\n",
    "    print(f'\\nHighlighting the original search terms in the abstracts...')\n",
    "    biblio_highlights_df['abstract'] = biblio_highlights_df.apply(lambda x: highlight_selected_text(x['abstract'], words_diff_st, 'red', biblio_highlights_df.index.get_loc(x.name)), axis = 1)\n",
    "    print(f'\\nHighlighting the original search terms in the keywords...')\n",
    "    biblio_highlights_df['kws'] = biblio_highlights_df.apply(lambda x: highlight_selected_text(x['kws'], words_diff_st, 'red', biblio_highlights_df.index.get_loc(x.name)), axis = 1)\n",
    "\n",
    "    print(f'Saving results...')\n",
    "    \n",
    "    if display_html:\n",
    "        display(HTML(biblio_highlights_df[['title', 'abstract', 'kws', 'fos']].iloc[:max_html_rows].to_html(escape = False)))\n",
    "\n",
    "    if write_csv:\n",
    "        print(f'Saving file {write_biblio_highlights_file}.csv ...')\n",
    "        biblio_no_highlights_df.to_csv(root_dir + results_dir + write_biblio_highlights_file + '.csv', index = False)\n",
    "    \n",
    "    if write_html:\n",
    "        print(f'Saving file {write_biblio_highlights_file}.html ...')\n",
    "        with open(root_dir + results_dir + write_biblio_highlights_file + '.html', 'w') as f:\n",
    "            f.write(str(HTML(biblio_highlights_df[['id', 'title', 'abstract', 'kws', 'fos', 'year', 'source']].to_html(escape = False)).data))\n",
    "\n",
    "    if write_xlsx:\n",
    "\n",
    "        # Define the rich text formattings\n",
    "        bold_red = InlineFont(b = True, color = '00FF0000')\n",
    "        bold_blue = InlineFont(b = True, color = '000000FF')\n",
    "\n",
    "        # Create a new workbook\n",
    "        wb = Workbook()\n",
    "\n",
    "        # Create a new sheet with the name 'TAK Highlights'\n",
    "        ws = wb.create_sheet('TAK Highlights')\n",
    "\n",
    "        if wb[\"Sheet\"]:\n",
    "            wb.remove(wb[\"Sheet\"])\n",
    "\n",
    "        # Make a copy of titles_highlights_df\n",
    "        tak_excel_df = biblio_highlights_df.copy()\n",
    "\n",
    "        # Rearrange columns\n",
    "        if not 'tp_num' in tak_excel_df.columns:\n",
    "            new_order = ['title', 'abstract', 'year', 'cited', 'source', 'kws', \n",
    "                        'search_label', 'authors', 'search_title', 'search_abs',\n",
    "                        'lit_review']\n",
    "        else:\n",
    "            new_order = ['title', 'abstract', 'year', 'cited', 'tp_num', 'tp_name', \n",
    "                         'top_n_words', 'prob', 'representative', 'source', 'kws', \n",
    "                         'search_label', 'authors', 'search_title', 'search_abs',\n",
    "                         'lit_review']\n",
    "            tak_excel_df = tak_excel_df.sort_values(by = ['tp_num', 'prob'], ascending = [True, False])\n",
    "\n",
    "        tak_excel_df = tak_excel_df.reindex(columns = new_order)\n",
    "\n",
    "        # Excel column headers\n",
    "        for j in range(len(tak_excel_df.columns)):\n",
    "            ws.cell(row = 1, column = j + 1, value = str(tak_excel_df.columns[j]))\n",
    "\n",
    "        # Apply findall() to split a string at '<span.../span>'\n",
    "        def split_string_at_span(string):\n",
    "            lst = re.findall(r\"(.*?)(<span.*?/span>|$)\", string)\n",
    "            lst = [elem for tup in lst for elem in tup]\n",
    "            lst = [x for x in lst if x.strip()]\n",
    "            return lst\n",
    "\n",
    "        def replace_span_with_textblock(lst):\n",
    "\n",
    "            is_prev_kw = False  # need to add a space between two consecutive keywords\n",
    "\n",
    "            # TODO replace with a list comprehension: new_list = [x if x != 'banana' else 'orange' for x in my_list]\n",
    "            for i in range(len(lst)):\n",
    "\n",
    "                if '<span style=\"color: red; font-weight: bold\">' in lst[i]:\n",
    "                    text = (' ' if is_prev_kw else '') + lst[i].split('>')[1].split('<')[0]\n",
    "                    text_block = TextBlock(bold_red, text)\n",
    "                    lst[i] = text_block\n",
    "                    is_prev_kw = True\n",
    "                elif '<span style=\"color: blue; font-weight: bold\">' in lst[i]:\n",
    "                    text = (' ' if is_prev_kw else '') + lst[i].split('>')[1].split('<')[0]\n",
    "                    text_block = TextBlock(bold_blue, text)\n",
    "                    lst[i] = text_block\n",
    "                    is_prev_kw = True\n",
    "                else:\n",
    "                    is_prev_kw = False\n",
    "\n",
    "            return lst\n",
    "\n",
    "        # print(tak_excel_df.iloc[0,1])\n",
    "\n",
    "        # Create a list of each cell content by splitting at '<span...>some text</span>' using findall()\n",
    "        tak_excel_df['title'] = tak_excel_df['title'].apply(split_string_at_span)\n",
    "        tak_excel_df['abstract'] = tak_excel_df['abstract'].apply(split_string_at_span)\n",
    "        tak_excel_df['kws'] = tak_excel_df['kws'].apply(split_string_at_span)\n",
    "\n",
    "        # Replace all '<span...>some text</span>' with the results of calling TextBlock(bold_red, 'some text')\n",
    "        tak_excel_df['title'] = tak_excel_df['title'].apply(replace_span_with_textblock)\n",
    "        tak_excel_df['abstract'] = tak_excel_df['abstract'].apply(replace_span_with_textblock)\n",
    "        tak_excel_df['kws'] = tak_excel_df['kws'].apply(replace_span_with_textblock)\n",
    "\n",
    "        highlight_cols = ['title', 'abstract', 'kws']\n",
    "        num_value_cols = ['tp_num', 'prob']\n",
    "\n",
    "        # Loop through rows and columns of the dataframe\n",
    "        for i in range(len(tak_excel_df)):\n",
    "            for col in tak_excel_df.columns:\n",
    "                j = tak_excel_df.columns.get_loc(col)\n",
    "\n",
    "                if col in highlight_cols:\n",
    "                    rs = CellRichText(tak_excel_df.iloc[i, j])\n",
    "                elif col in num_value_cols:\n",
    "                    # rs = str(tak_excel_df.iloc[i, j])\n",
    "                    rs = tak_excel_df.iloc[i, j]\n",
    "                else:\n",
    "                    rs = str(tak_excel_df.iloc[i, j])\n",
    "\n",
    "                ws.cell(row = i + 2, column = j + 1, value = rs)\n",
    "\n",
    "        if not 'tp_num' in tak_excel_df.columns:\n",
    "            ws.column_dimensions['A'].width = 40\n",
    "            ws.column_dimensions['B'].width = 80\n",
    "            ws.column_dimensions['C'].width = 6\n",
    "            ws.column_dimensions['D'].width = 6\n",
    "            ws.column_dimensions['E'].width = 30\n",
    "            ws.column_dimensions['F'].width = 40\n",
    "            ws.column_dimensions['G'].width = 25\n",
    "            ws.column_dimensions['H'].width = 25\n",
    "            ws.column_dimensions['I'].width = 25\n",
    "            ws.column_dimensions['J'].width = 25\n",
    "            ws.column_dimensions['K'].width = 6\n",
    "            wrap_col = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n",
    "        else:\n",
    "            ws.column_dimensions['A'].width = 40\n",
    "            ws.column_dimensions['B'].width = 80\n",
    "            ws.column_dimensions['C'].width = 6\n",
    "            ws.column_dimensions['D'].width = 6\n",
    "            ws.column_dimensions['E'].width = 8\n",
    "            ws.column_dimensions['F'].width = 35\n",
    "            ws.column_dimensions['G'].width = 30\n",
    "            ws.column_dimensions['H'].width = 8\n",
    "            ws.column_dimensions['I'].width = 6\n",
    "            ws.column_dimensions['J'].width = 30\n",
    "            ws.column_dimensions['K'].width = 35\n",
    "            ws.column_dimensions['L'].width = 25\n",
    "            ws.column_dimensions['M'].width = 25\n",
    "            ws.column_dimensions['N'].width = 25\n",
    "            ws.column_dimensions['O'].width = 25\n",
    "            ws.column_dimensions['P'].width = 6\n",
    "            wrap_col = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'J', 'K', 'L', 'M', 'N', 'O']\n",
    "\n",
    "        for col in wrap_col:\n",
    "            for cell in ws[col]:\n",
    "                cell.alignment = Alignment(wrap_text = True)\n",
    "\n",
    "        ws.sheet_view.zoomScale = 140\n",
    "        ws.freeze_panes = 'A2'\n",
    "\n",
    "        print(f'Saving file {write_biblio_highlights_file}.xlsx ...')\n",
    "        wb.save(root_dir + results_dir + write_biblio_highlights_file + '.xlsx')\n",
    "\n",
    "else:\n",
    "    print(f\"No matches found for boolean search: {filter_words}\")\n",
    "\n",
    "print(f'DONE!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cTvCVIoX3OT"
   },
   "source": [
    "### Researcher, departments, and countries\n",
    "\n",
    "The script extracts the author names and affiliations and creates tables with:\n",
    "* Number of articles per country.\n",
    "* Number of articles per institution/department.\n",
    "* Number of articles per author.\n",
    "\n",
    "The tables can be saved in a sheet of an Excel workbook.\n",
    "\n",
    "**Parameters**\n",
    "* write_xlsx: set to True to generate the Excel file of the highlights.\n",
    "* file_xlsx_out: the output Excel file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEV2RBImMygp"
   },
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "use_highlights_df = False\n",
    "write_excel = True\n",
    "file_xlsx_out = 'country_dept_author_counts'\n",
    "logger.setLevel(logging.INFO)\n",
    "# --------------------------------------------\n",
    "\n",
    "# For the initial table, either copy the full table or the highlights table\n",
    "if use_highlights_df:\n",
    "  ctry_affil_auth_df = titles_highlights_clean_df[['author', 'author_id', 'affiliation', 'title', 'year', 'source', 'cited']].copy() #.head(20)\n",
    "else:\n",
    "  ctry_affil_auth_df = biblio_df[['author', 'author_id', 'affiliation', 'title', 'year', 'source', 'cited']].copy() #.head(5)\n",
    "\n",
    "# Find duplicate titles\n",
    "dup = ctry_affil_auth_df.duplicated(subset=['title'], keep = False)\n",
    "df_dup = ctry_affil_auth_df[dup]\n",
    "\n",
    "'''\n",
    "print(\"DUPLICATE TITLES\")\n",
    "print(\"----------------\")\n",
    "print(df_dup['title'].value_counts())\n",
    "'''\n",
    "\n",
    "# Remove duplicate titles\n",
    "ctry_affil_auth_df = ctry_affil_auth_df.drop_duplicates(subset=['title'], keep = 'first')\n",
    "\n",
    "ctry_affil_auth_df['author'] = ctry_affil_auth_df['author'].apply(lambda x: x.split(','))\n",
    "ctry_affil_auth_df['author_id'] = ctry_affil_auth_df['author_id'].apply(str)\n",
    "ctry_affil_auth_df['author_id'] = ctry_affil_auth_df['author_id'].apply(lambda x: x.rstrip(';'))\n",
    "ctry_affil_auth_df['author_id'] = ctry_affil_auth_df['author_id'].apply(lambda x: x.split(';'))\n",
    "ctry_affil_auth_df['affiliation'] = ctry_affil_auth_df['affiliation'].apply(str)\n",
    "ctry_affil_auth_df['affiliation'] = ctry_affil_auth_df['affiliation'].apply(lambda x: x.split(';'))\n",
    "\n",
    "# Count the delimiters for author and author_id and remove those rows where they don't conincide\n",
    "ctry_affil_auth_df['length'] = [len(x) for x in ctry_affil_auth_df['author']]\n",
    "ctry_affil_auth_df['length2'] = [len(x) for x in ctry_affil_auth_df['author_id']]\n",
    "ctry_affil_auth_df['length3'] = [len(x) for x in ctry_affil_auth_df['affiliation']]\n",
    "ctry_affil_auth_df = ctry_affil_auth_df[ctry_affil_auth_df['length'] == ctry_affil_auth_df['length2']]\n",
    "ctry_affil_auth_df = ctry_affil_auth_df[ctry_affil_auth_df['length'] == ctry_affil_auth_df['length3']]\n",
    "\n",
    "ctry_affil_auth_df.drop(['length', 'length2', 'length3'], axis=1, inplace=True)\n",
    "\n",
    "# Explode the columns with author and affiliation information\n",
    "ctry_affil_auth_df = ctry_affil_auth_df.set_index(['title', 'year', 'source', 'cited']).apply(lambda x: x.explode()).reset_index()\n",
    "\n",
    "# Separate the author from the affiliation and the country\n",
    "ctry_affil_auth_df['affiliation'] = ctry_affil_auth_df['affiliation'].apply(lambda x: x.split(','))\n",
    "ctry_affil_auth_df['affil_auth'] = ctry_affil_auth_df['affiliation'].apply(lambda x: ''.join(x[0:2]))\n",
    "ctry_affil_auth_df['affil_dept'] = ctry_affil_auth_df['affiliation'].apply(lambda x: ','.join(x[2:-1]))\n",
    "ctry_affil_auth_df['affil_country'] = ctry_affil_auth_df['affiliation'].apply(lambda x: x[-1].strip())\n",
    "\n",
    "ctry_affil_auth_df = ctry_affil_auth_df.sort_values(by = ['affil_country', 'affil_dept', 'affil_auth'], ascending = [True, True, True], na_position = 'first').reset_index()\n",
    "ctry_affil_auth_df.drop(['index'], axis = 1, inplace = True)\n",
    "\n",
    "# Mark rows where affil_country is not a country\n",
    "\n",
    "def country_name(country):\n",
    "\n",
    "  try:\n",
    "    country_name = pycountry.countries.search_fuzzy(country)[0].name\n",
    "  except LookupError:\n",
    "    country_name = \"no_country\"\n",
    " \n",
    "  return country_name\n",
    "\n",
    "\n",
    "def is_country(country):\n",
    "\n",
    "  if pycountry.countries.get(name = country) == None:\n",
    "    return False\n",
    " \n",
    "  return True\n",
    "\n",
    "ctry_affil_auth_df['is_country'] = ctry_affil_auth_df['affil_country'].apply(is_country)\n",
    "not_countries_df = ctry_affil_auth_df[['affil_country']][~ctry_affil_auth_df['is_country']].drop_duplicates(subset = ['affil_country']).copy()\n",
    "not_countries_df['fuzzy_search'] = not_countries_df['affil_country'].apply(country_name)\n",
    "ctry_affil_auth_df = ctry_affil_auth_df.merge(not_countries_df, left_on = 'affil_country', right_on = 'affil_country', how = 'left')\n",
    "ctry_affil_auth_df['affil_country'] = ctry_affil_auth_df['fuzzy_search'].fillna(ctry_affil_auth_df['affil_country'])\n",
    "ctry_affil_auth_df.drop(columns = ['is_country', 'fuzzy_search'], inplace = True)\n",
    "\n",
    "#display(ctry_affil_auth_df)\n",
    "#display(not_countries_df)\n",
    "\n",
    "'''\n",
    "print(f\"\\nHEAD COUNTRY-AFFILIATION-AUTHOR DF\")\n",
    "print(\"-------------------------------------\")\n",
    "display(ctry_affil_auth_df[['title', 'year', 'source', 'cited', 'affil_auth', 'affil_dept', 'affil_country']].head())\n",
    "\n",
    "# Entries where author names and those extracted from the affiliations don't coincide\n",
    "print(f\"\\nAUTHOR AND AFFIL AUTHOR DO NOT COINCIDE\")\n",
    "print(\"------------------------------------------\")\n",
    "display(ctry_affil_auth_df[ctry_affil_auth_df['author'] != ctry_affil_auth_df['affil_auth']])\n",
    "'''\n",
    "\n",
    "ctry_affil_auth_df.drop(['affiliation', 'author', 'author_id'], axis=1, inplace = True)\n",
    "#display(ctry_affil_auth_df)\n",
    "\n",
    "# Compute the number of articles per country\n",
    "country_freq_df = ctry_affil_auth_df.groupby('affil_country')['title'].nunique().reset_index(name = 'count').copy()\n",
    "country_freq_df = country_freq_df.sort_values(by = ['count', 'affil_country'], ascending = [False, True]).reset_index()\n",
    "country_freq_df.drop(['index'], axis = 1, inplace = True)\n",
    "\n",
    "#display(country_freq_df)\n",
    "\n",
    "# Compute the number of articles per institution\n",
    "dept_freq_df = ctry_affil_auth_df.groupby(['affil_dept', 'affil_country'])['title'].nunique().reset_index(name = 'count').copy()\n",
    "dept_freq_df = dept_freq_df.sort_values(by = ['count', 'affil_dept'], ascending = [False, True]).reset_index()\n",
    "dept_freq_df.drop(['index'], axis = 1, inplace = True)\n",
    "\n",
    "display(dept_freq_df)\n",
    "\n",
    "# Compute the number of articles per author\n",
    "auth_freq_df = ctry_affil_auth_df.groupby(['affil_auth', 'affil_country']).size().reset_index(name = 'freq').copy()\n",
    "auth_freq_df = auth_freq_df.sort_values(by = ['freq', 'affil_country', 'affil_auth'], ascending = [False, True, True]).reset_index()\n",
    "auth_freq_df.drop(['index'], axis = 1, inplace = True)\n",
    "\n",
    "\n",
    "#display(auth_freq_df)\n",
    "\n",
    "# Write file to Excel\n",
    "if write_excel == True:\n",
    "\n",
    "  # Create a new workbook\n",
    "  wb = Workbook()\n",
    "\n",
    "  # Create a new sheet with the name 'TAK Highlights'\n",
    "  ws = wb.create_sheet('Counts')\n",
    "\n",
    "  if wb[\"Sheet\"]:\n",
    "    wb.remove(wb[\"Sheet\"])\n",
    "\n",
    "  # Create the headings in the sheet\n",
    "  ws.cell(row = 1, column = 1, value=\"Country\")\n",
    "  ws.cell(row = 1, column = 2, value=\"Count\")\n",
    "  ws.cell(row = 1, column = 4, value=\"Institution\")\n",
    "  ws.cell(row = 1, column = 5, value=\"Country\")\n",
    "  ws.cell(row = 1, column = 6, value=\"Count\")\n",
    "  ws.cell(row = 1, column = 8, value=\"Author\")\n",
    "  ws.cell(row = 1, column = 9, value=\"Country\")\n",
    "  ws.cell(row = 1, column = 10, value=\"Count\")\n",
    "  \n",
    "  # Write the count results to the cells\n",
    "  for i in range(len(country_freq_df)):\n",
    "    for j in range(len(country_freq_df.columns)):\n",
    "      ws.cell(row = i + 2, column = j + 1, value = country_freq_df.iloc[i, j])\n",
    "\n",
    "  for i in range(len(dept_freq_df)):\n",
    "    for j in range(len(dept_freq_df.columns)):\n",
    "      ws.cell(row = i + 2, column = j + 1 + 3, value = dept_freq_df.iloc[i, j])\n",
    "\n",
    "  for i in range(len(auth_freq_df)):\n",
    "    for j in range(len(auth_freq_df.columns)):\n",
    "      ws.cell(row = i + 2, column = j + 1 + 7, value = auth_freq_df.iloc[i, j])\n",
    "\n",
    "  ws.column_dimensions['A'].width = 20\n",
    "  ws.column_dimensions['B'].width = 5\n",
    "  ws.column_dimensions['D'].width = 70\n",
    "  ws.column_dimensions['E'].width = 20\n",
    "  ws.column_dimensions['F'].width = 5\n",
    "  ws.column_dimensions['H'].width = 20\n",
    "  ws.column_dimensions['I'].width = 20\n",
    "  ws.column_dimensions['J'].width = 5\n",
    "\n",
    "  ws.sheet_view.zoomScale = 140\n",
    "\n",
    "if write_xlsx:\n",
    "  wb.save(root_dir + results_dir + file_xlsx_out + '.xlsx')\n",
    "\n",
    "#ctry_affil_auth_df.to_excel('/drive/My Drive/Colab Notebooks/Scopus files/ELEM Hire ML in CE/scopus_ml_ce_PINN_out.xlsx', index=False)\n",
    "\n",
    "# TODO\n",
    "# - When authors are affiliated with multiple institutions, these are not split out in the results. Here is an\n",
    "#   example: \"Raghunath, S., Geisinger, Danville, PA, United States, Tempus Labs Inc., Chicago, IL, United States;\"\n",
    "#   It seems the only way to split this is to split the string on country name. I can do this using an array with all\n",
    "#   country names.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6ZcIZeYd0XQ"
   },
   "source": [
    "## Sandbox\n",
    "From this point onward, the cells are for playing around with code."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### keyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "doc = \"\"\"\n",
    "         Since the creation of stock markets there have been attempts to predict their movements and new prediction methodologies have been devised According to a recent study when the Russell industry index starts to rise stocks belonging to the corresponding industry in other countries also rise accordingly Based on this empirical result this study seeks to predict the start date of industry uptrends using the Russell industry index The proposed model in this study predicts future stock prices using a denoising autoencoder DAE long short term memory LSTM model and predicts the existence and timing of future change points in stock prices through Pettitttest The of the empirical analysis confirmed that this proposed model can find the change points in stock prices within days prior to the start date of actual uptrends in selected industries This study contributes to predicting a change point through a combination of statistical and deep learning models and the methodology developed in this study could be applied to various financial time series data for various purposes.\n",
    "      \"\"\"\n",
    "kw_model = KeyBERT(model = 'all-MiniLM-L6-v2')\n",
    "\n",
    "print(f'{doc}')\n",
    "print(f'Baseline')\n",
    "\n",
    "keywords = kw_model.extract_keywords(doc, keyphrase_ngram_range = (1, 2), \n",
    "                                     stop_words = 'english', top_n = 10)\n",
    "\n",
    "display(keywords)\n",
    "\n",
    "print(f'Max Sum Distance')\n",
    "\n",
    "keywords = kw_model.extract_keywords(doc, keyphrase_ngram_range = (1, 2), \n",
    "                                     stop_words = 'english', use_maxsum = True, \n",
    "                                     nr_candidates = 20, top_n = 10)\n",
    "\n",
    "display(keywords)\n",
    "\n",
    "print(f'Maximal Marginal Relevance')\n",
    "\n",
    "keywords = kw_model.extract_keywords(doc, keyphrase_ngram_range = (1, 2), \n",
    "                                     stop_words = 'english', use_mmr = True, \n",
    "                                     diversity = 0.5, top_n = 10)\n",
    "\n",
    "display(keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktdbNEIJ8xwz"
   },
   "source": [
    "### LDA topic modelling\n",
    "This is a first implementation of an LDA topic modelling following the [video tutorial](https://https://www.youtube.com/watch?v=TKjjlp5_r7o) by William Mattingly. It uses the Scopus keywords to create the topic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYJJD-6JD5m5"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ElqXVo-WSDQ2"
   },
   "outputs": [],
   "source": [
    "# Lemmatise the keywords\n",
    "def lemmatization(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "    nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "    texts_out = []\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        new_text = []\n",
    "        for token in doc:\n",
    "            if token.pos_ in allowed_postags:\n",
    "                new_text.append(token.lemma_)\n",
    "        final = \" \".join(new_text)\n",
    "        texts_out.append(final)\n",
    "    return (texts_out)\n",
    "\n",
    "# Do some pre-processing on the keywords\n",
    "def gen_words(texts):\n",
    "    final = []\n",
    "    for text in texts:\n",
    "        new = gensim.utils.simple_preprocess(text, deacc=True)\n",
    "        final.append(new)\n",
    "    return (final)\n",
    "\n",
    "# Create a bag-of-word representation of the keywords\n",
    "def keywords_to_bow(words):\n",
    "  corpus = []\n",
    "  for text in data_words:\n",
    "      new = id2word.doc2bow(text)\n",
    "      corpus.append(new)\n",
    "  return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5SC7dSYDS81"
   },
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "filter_lda = \"cardi or heart\"  # uses the keywords of the full dataset or the filtered keywords\n",
    "# ------------------------\n",
    "\n",
    "if filter_lda != \"\":\n",
    "  '''\n",
    "  lda_kws_df['keep'] = titles_df['kws'].apply(evaluate_expression, expression = filter_lda)\n",
    "  lda_kws_df = lda_kws_df[lda_kws_df['keep'] == True]\n",
    "  print(f'Filtered dataset has {len(lda_kws_df)} entries out of the {len(titles_df)} in the original dataset')\n",
    "  '''\n",
    "  filter_lda_exp_list = expand_search_terms(filter_lda)\n",
    "  filter_lda_exp = ' '.join([explode_filter_kws(x) if x not in ['(', ')', 'and', 'or'] else x for x in filter_exp_list])  # build the exploded string\n",
    "\n",
    "  if filter_lda_exp != \"\":\n",
    "    lda_kws_df = titles_df[titles_df.apply(evaluate_expression, filter = filter_lda_exp, axis = 1)].copy()\n",
    "    #print(f'Scopus keywords of the dataframe filtered by the boolean expression: {filter_lda}')\n",
    "    #display(lda_kws_df)\n",
    "else:\n",
    "  lda_kws_df = titles_df[['kws']].copy()\n",
    "\n",
    "# Combine the multi-word keywords with underscores '_' and convert to a single string for each entry\n",
    "lda_kws_df['kws'] = lda_kws_df['kws'].str.split(',')\n",
    "lda_kws_df['kws'] = lda_kws_df['kws'].apply(lambda x: [word.strip().replace(' ', '_') for word in x])\n",
    "lda_kws_df['kws'] = lda_kws_df['kws'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Create a list of the keyword strings\n",
    "data = lda_kws_df['kws'].to_list()\n",
    "\n",
    "''' Remove this comment to run the topic model with William Mattingly's test file\n",
    "# Get test json file\n",
    "def load_data(file):\n",
    "    with open (file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f) \n",
    "    return (data)\n",
    "\n",
    "data = load_data('/content/drive/My Drive/Colab Notebooks/Biblio Analysis/Input_test/ushmm_dn.json')[\"texts\"]\n",
    "data = data[0:4]\n",
    "'''\n",
    "# Lemmatise the keywords\n",
    "lemmatized_texts = lemmatization(data)\n",
    "\n",
    "# High-count keywords that don't contribute much to the topics\n",
    "exclude = ['article', 'adult', 'human', 'male', 'non', 'middle_age', 'female', 'age', \n",
    "           'high', 'single', 'child', 'aged', 'animal']\n",
    "lemmatized_texts = [\" \".join([word for word in sentence.split() if word not in exclude]) for sentence in lemmatized_texts]\n",
    "\n",
    "# Do some pre-processing on the keywords\n",
    "data_words = gen_words(lemmatized_texts)\n",
    "\n",
    "# Generate the wordID-to-word dictionary\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "\n",
    "# Create a bag-of-word representation of the keywords\n",
    "corpus = keywords_to_bow(data_words)\n",
    "\n",
    "# TODO\n",
    "# - add the search terms to the keyword list; create two corpora: one with and one without the search terms\n",
    "# - run the lemmatiser on the individual words of the multi-word keywords and remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OV_FHtZEEVOW"
   },
   "outputs": [],
   "source": [
    "# Compute LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=4,\n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xjCwLrwQEWqB"
   },
   "outputs": [],
   "source": [
    "# Visualise the topics model\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word, mds=\"mmds\", R=30)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qHPym2AgN6c"
   },
   "source": [
    "### DEPRECATED Associated term clustering\n",
    "I replaced this with the LDA topic model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bg9il25NER4r"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "df = titles_df[['kws']].copy()\n",
    "\n",
    "df['kws'] = df['kws'].str.split(',')\n",
    "df['kws'] = df['kws'].apply(lambda x: [word.strip().replace(' ', '_') for word in x])\n",
    "df['kws'] = df['kws'].apply(lambda x: [word.strip('_') for word in x])\n",
    "print(df)\n",
    "\n",
    "# Convert the 'kws' column to a list of strings\n",
    "docs = [' '.join(kws) for kws in df['kws']]\n",
    "\n",
    "# Use CountVectorizer to create a co-occurrence matrix\n",
    "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')\n",
    "X = vectorizer.fit_transform(docs)\n",
    "co_occurrence_matrix = (X.T * X)\n",
    "\n",
    "# Cluster the keywords based on their co-occurrence patterns\n",
    "num_clusters = 10\n",
    "kmeans = KMeans(n_clusters = num_clusters, n_init = 10)\n",
    "kmeans.fit(co_occurrence_matrix)\n",
    "\n",
    "# Get the cluster labels and print the top keywords in each cluster\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    cluster_keywords = [vectorizer.get_feature_names_out()[idx] for idx, label in enumerate(cluster_labels) if label == i]\n",
    "    print(f'Cluster {i+1}: {\" | \".join(cluster_keywords[:100])}')\n",
    "\n",
    "# TODO\n",
    "# - Show the most frequent cluster keywords first. Currently they are in alphabetical order.\n",
    "# - Remove low count keywords directly from df before doing any processing.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
